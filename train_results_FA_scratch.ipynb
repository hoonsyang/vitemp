{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# custom\n",
    "from model_layer import Vgg16_all_layer, Vgg19_all_layer, Res152_all_layer, Dense169_all_layer\n",
    "from generator import GeneratorResnet\n",
    "from dct import *\n",
    "# from utils import *\n",
    "from loader_checkpoint import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser0 = argparse.ArgumentParser(description='Transferable Perturbation via Frequency Manipulation')\n",
    "parser0.add_argument('--epochs', type=int, default=0, help='Model checkpoint epoch number')\n",
    "parser0.add_argument('--eps', type=int, default=10, help='Perturbation budget (0~255)')\n",
    "parser0.add_argument('--model_type', type=str, default='vgg16', help='Victim model: vgg16, vgg19, res152, dense169')\n",
    "parser0.add_argument('--RN', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Random Normalization module in training phase')\n",
    "parser0.add_argument('--DA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Domain-agnostic Attention module in training phase')\n",
    "parser0.add_argument('--FA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Frequency Augmentation module in training phase')\n",
    "args0 = parser0.parse_args(args=[])\n",
    "print(args0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DA=False, FA=True, RN=False, batch_size=16, epochs=1, eps=10, lr=0.0002, model_type='vgg16', rho=0.5, sigma=16.0, train_dir='../dataset/imagenet/train')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Transferable Perturbation via Frequency Manipulation')\n",
    "parser.add_argument('--train_dir', default='../dataset/imagenet/train', help='Path for imagenet training data')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='Batch size')\n",
    "parser.add_argument('--epochs', type=int, default=1, help='Number of training epochs')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='Initial learning rate')\n",
    "parser.add_argument('--eps', type=int, default=10, help='Perturbation budget (0~255)')\n",
    "parser.add_argument('--model_type', type=str, default='vgg16', help='Victim model: vgg16, vgg19, res152, dense169')\n",
    "parser.add_argument('--RN', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Random Normalization module in training phase')\n",
    "parser.add_argument('--DA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Domain-agnostic Attention module in training phase')\n",
    "parser.add_argument('--FA', type=lambda x: (str(x).lower() == 'true'), default=True, help='If true, activating the Frequency Augmentation module in training phase')\n",
    "parser.add_argument(\"--rho\", type=float, default=0.5, help=\"Tuning factor\")\n",
    "parser.add_argument(\"--sigma\", type=float, default=16.0, help=\"Std of random noise\")\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_seed(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# setup_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the victim classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16(\n",
       "  (vgg): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (model): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.model_type == 'vgg16':\n",
    "    model = Vgg16_all_layer.Vgg16()\n",
    "    layer_idx = 16 # Maxpooling.3\n",
    "elif args.model_type == 'vgg19':\n",
    "    model = Vgg19_all_layer.Vgg19()\n",
    "    layer_idx = 18 # Maxpooling.3\n",
    "elif args.model_type == 'res152':\n",
    "    model = Res152_all_layer.Resnet152()\n",
    "    layer_idx = 5 # Conv3_8\n",
    "elif args.model_type == 'dense169':\n",
    "    model = Dense169_all_layer.Dense169()\n",
    "    layer_idx = 6 # Denseblock.2\n",
    "else:\n",
    "    raise Exception('Check the model_type')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generative attack model/optimizer/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1281167\n"
     ]
    }
   ],
   "source": [
    "### Model, Optimizer\n",
    "# netG = GeneratorResnet()\n",
    "netG = load_gan(args0, 'imagenet')\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "\n",
    "netG = nn.DataParallel(netG, device_ids=[0,1,2,3]) # multi-GPU\n",
    "netG = netG.to(device)\n",
    "\n",
    "if args.RN and args.DA:\n",
    "    save_checkpoint_suffix = 'BIA+RN+DA'\n",
    "elif args.RN:\n",
    "    save_checkpoint_suffix = 'BIA+RN'\n",
    "elif args.DA:\n",
    "    save_checkpoint_suffix = 'BIA+DA'\n",
    "elif args.FA:\n",
    "    save_checkpoint_suffix = 'BIA+FA'\n",
    "else:\n",
    "    save_checkpoint_suffix = 'BIA'\n",
    "\n",
    "# Data, Transform\n",
    "scale_size = 256\n",
    "img_size = 224\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(scale_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dir = args.train_dir\n",
    "train_set = datasets.ImageFolder(train_dir, data_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_size = len(train_set)\n",
    "print('Training data size:', train_size)\n",
    "\n",
    "def default_normalize(t):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - 0.485) / 0.229\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - 0.456) / 0.224\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - 0.406) / 0.225\n",
    "    return t\n",
    "\n",
    "def normalize(t, mean, std):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - mean) / std\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - mean) / std\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - mean) / std\n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t loss: 0.00000\n",
      "Epoch: 0 \t Batch: 100 \t loss: 0.81441\n",
      "Epoch: 0 \t Batch: 200 \t loss: 0.80399\n",
      "Epoch: 0 \t Batch: 300 \t loss: 0.79863\n",
      "Epoch: 0 \t Batch: 400 \t loss: 0.76896\n",
      "Epoch: 0 \t Batch: 500 \t loss: 0.75162\n",
      "Epoch: 0 \t Batch: 600 \t loss: 0.74135\n",
      "Epoch: 0 \t Batch: 700 \t loss: 0.72688\n",
      "Epoch: 0 \t Batch: 800 \t loss: 0.71881\n",
      "Epoch: 0 \t Batch: 900 \t loss: 0.71023\n",
      "Epoch: 0 \t Batch: 1000 \t loss: 0.70422\n",
      "Epoch: 0 \t Batch: 1100 \t loss: 0.69773\n",
      "Epoch: 0 \t Batch: 1200 \t loss: 0.69294\n",
      "Epoch: 0 \t Batch: 1300 \t loss: 0.68436\n",
      "Epoch: 0 \t Batch: 1400 \t loss: 0.67694\n",
      "Epoch: 0 \t Batch: 1500 \t loss: 0.67371\n",
      "Epoch: 0 \t Batch: 1600 \t loss: 0.66614\n",
      "Epoch: 0 \t Batch: 1700 \t loss: 0.66126\n",
      "Epoch: 0 \t Batch: 1800 \t loss: 0.65916\n",
      "Epoch: 0 \t Batch: 1900 \t loss: 0.65226\n",
      "Epoch: 0 \t Batch: 2000 \t loss: 0.64787\n",
      "Epoch: 0 \t Batch: 2100 \t loss: 0.64269\n",
      "Epoch: 0 \t Batch: 2200 \t loss: 0.63974\n",
      "Epoch: 0 \t Batch: 2300 \t loss: 0.63818\n",
      "Epoch: 0 \t Batch: 2400 \t loss: 0.63095\n",
      "Epoch: 0 \t Batch: 2500 \t loss: 0.63087\n",
      "Epoch: 0 \t Batch: 2600 \t loss: 0.63023\n",
      "Epoch: 0 \t Batch: 2700 \t loss: 0.62535\n",
      "Epoch: 0 \t Batch: 2800 \t loss: 0.62723\n",
      "Epoch: 0 \t Batch: 2900 \t loss: 0.62430\n",
      "Epoch: 0 \t Batch: 3000 \t loss: 0.62367\n",
      "Epoch: 0 \t Batch: 3100 \t loss: 0.62062\n",
      "Epoch: 0 \t Batch: 3200 \t loss: 0.62052\n",
      "Epoch: 0 \t Batch: 3300 \t loss: 0.61442\n",
      "Epoch: 0 \t Batch: 3400 \t loss: 0.61312\n",
      "Epoch: 0 \t Batch: 3500 \t loss: 0.61499\n",
      "Epoch: 0 \t Batch: 3600 \t loss: 0.61033\n",
      "Epoch: 0 \t Batch: 3700 \t loss: 0.61332\n",
      "Epoch: 0 \t Batch: 3800 \t loss: 0.60932\n",
      "Epoch: 0 \t Batch: 3900 \t loss: 0.60733\n",
      "Epoch: 0 \t Batch: 4000 \t loss: 0.60892\n",
      "Epoch: 0 \t Batch: 4100 \t loss: 0.60879\n",
      "Epoch: 0 \t Batch: 4200 \t loss: 0.60233\n",
      "Epoch: 0 \t Batch: 4300 \t loss: 0.59769\n",
      "Epoch: 0 \t Batch: 4400 \t loss: 0.60005\n",
      "Epoch: 0 \t Batch: 4500 \t loss: 0.60134\n",
      "Epoch: 0 \t Batch: 4600 \t loss: 0.59962\n",
      "Epoch: 0 \t Batch: 4700 \t loss: 0.59630\n",
      "Epoch: 0 \t Batch: 4800 \t loss: 0.59484\n",
      "Epoch: 0 \t Batch: 4900 \t loss: 0.59694\n",
      "Epoch: 0 \t Batch: 5000 \t loss: 0.58800\n",
      "Epoch: 0 \t Batch: 5100 \t loss: 0.59206\n",
      "Epoch: 0 \t Batch: 5200 \t loss: 0.59210\n",
      "Epoch: 0 \t Batch: 5300 \t loss: 0.58966\n",
      "Epoch: 0 \t Batch: 5400 \t loss: 0.58943\n",
      "Epoch: 0 \t Batch: 5500 \t loss: 0.58735\n",
      "Epoch: 0 \t Batch: 5600 \t loss: 0.58680\n",
      "Epoch: 0 \t Batch: 5700 \t loss: 0.58673\n",
      "Epoch: 0 \t Batch: 5800 \t loss: 0.58354\n",
      "Epoch: 0 \t Batch: 5900 \t loss: 0.58599\n",
      "Epoch: 0 \t Batch: 6000 \t loss: 0.58495\n",
      "Epoch: 0 \t Batch: 6100 \t loss: 0.58602\n",
      "Epoch: 0 \t Batch: 6200 \t loss: 0.57743\n",
      "Epoch: 0 \t Batch: 6300 \t loss: 0.58315\n",
      "Epoch: 0 \t Batch: 6400 \t loss: 0.58087\n",
      "Epoch: 0 \t Batch: 6500 \t loss: 0.57874\n",
      "Epoch: 0 \t Batch: 6600 \t loss: 0.58463\n",
      "Epoch: 0 \t Batch: 6700 \t loss: 0.57793\n",
      "Epoch: 0 \t Batch: 6800 \t loss: 0.57813\n",
      "Epoch: 0 \t Batch: 6900 \t loss: 0.57934\n",
      "Epoch: 0 \t Batch: 7000 \t loss: 0.57656\n",
      "Epoch: 0 \t Batch: 7100 \t loss: 0.57346\n",
      "Epoch: 0 \t Batch: 7200 \t loss: 0.57583\n",
      "Epoch: 0 \t Batch: 7300 \t loss: 0.57166\n",
      "Epoch: 0 \t Batch: 7400 \t loss: 0.57351\n",
      "Epoch: 0 \t Batch: 7500 \t loss: 0.57259\n",
      "Epoch: 0 \t Batch: 7600 \t loss: 0.57208\n",
      "Epoch: 0 \t Batch: 7700 \t loss: 0.56828\n",
      "Epoch: 0 \t Batch: 7800 \t loss: 0.57105\n",
      "Epoch: 0 \t Batch: 7900 \t loss: 0.56773\n",
      "Epoch: 0 \t Batch: 8000 \t loss: 0.57155\n",
      "Epoch: 0 \t Batch: 8100 \t loss: 0.57134\n",
      "Epoch: 0 \t Batch: 8200 \t loss: 0.56723\n",
      "Epoch: 0 \t Batch: 8300 \t loss: 0.57341\n",
      "Epoch: 0 \t Batch: 8400 \t loss: 0.56196\n",
      "Epoch: 0 \t Batch: 8500 \t loss: 0.56011\n",
      "Epoch: 0 \t Batch: 8600 \t loss: 0.56991\n",
      "Epoch: 0 \t Batch: 8700 \t loss: 0.56366\n",
      "Epoch: 0 \t Batch: 8800 \t loss: 0.55938\n",
      "Epoch: 0 \t Batch: 8900 \t loss: 0.56121\n",
      "Epoch: 0 \t Batch: 9000 \t loss: 0.56340\n",
      "Epoch: 0 \t Batch: 9100 \t loss: 0.56419\n",
      "Epoch: 0 \t Batch: 9200 \t loss: 0.56172\n",
      "Epoch: 0 \t Batch: 9300 \t loss: 0.56145\n",
      "Epoch: 0 \t Batch: 9400 \t loss: 0.56199\n",
      "Epoch: 0 \t Batch: 9500 \t loss: 0.55829\n",
      "Epoch: 0 \t Batch: 9600 \t loss: 0.55717\n",
      "Epoch: 0 \t Batch: 9700 \t loss: 0.55740\n",
      "Epoch: 0 \t Batch: 9800 \t loss: 0.55627\n",
      "Epoch: 0 \t Batch: 9900 \t loss: 0.55418\n",
      "Epoch: 0 \t Batch: 10000 \t loss: 0.55411\n",
      "Epoch: 0 \t Batch: 10100 \t loss: 0.55193\n",
      "Epoch: 0 \t Batch: 10200 \t loss: 0.55165\n",
      "Epoch: 0 \t Batch: 10300 \t loss: 0.55303\n",
      "Epoch: 0 \t Batch: 10400 \t loss: 0.55317\n",
      "Epoch: 0 \t Batch: 10500 \t loss: 0.55028\n",
      "Epoch: 0 \t Batch: 10600 \t loss: 0.54786\n",
      "Epoch: 0 \t Batch: 10700 \t loss: 0.55162\n",
      "Epoch: 0 \t Batch: 10800 \t loss: 0.55070\n",
      "Epoch: 0 \t Batch: 10900 \t loss: 0.55020\n",
      "Epoch: 0 \t Batch: 11000 \t loss: 0.54493\n",
      "Epoch: 0 \t Batch: 11100 \t loss: 0.54071\n",
      "Epoch: 0 \t Batch: 11200 \t loss: 0.53648\n",
      "Epoch: 0 \t Batch: 11300 \t loss: 0.53629\n",
      "Epoch: 0 \t Batch: 11400 \t loss: 0.52885\n",
      "Epoch: 0 \t Batch: 11500 \t loss: 0.52388\n",
      "Epoch: 0 \t Batch: 11600 \t loss: 0.52201\n",
      "Epoch: 0 \t Batch: 11700 \t loss: 0.52266\n",
      "Epoch: 0 \t Batch: 11800 \t loss: 0.51921\n",
      "Epoch: 0 \t Batch: 11900 \t loss: 0.52173\n",
      "Epoch: 0 \t Batch: 12000 \t loss: 0.52201\n",
      "Epoch: 0 \t Batch: 12100 \t loss: 0.51878\n",
      "Epoch: 0 \t Batch: 12200 \t loss: 0.51218\n",
      "Epoch: 0 \t Batch: 12300 \t loss: 0.51085\n",
      "Epoch: 0 \t Batch: 12400 \t loss: 0.51212\n",
      "Epoch: 0 \t Batch: 12500 \t loss: 0.50738\n",
      "Epoch: 0 \t Batch: 12600 \t loss: 0.50375\n",
      "Epoch: 0 \t Batch: 12700 \t loss: 0.50474\n",
      "Epoch: 0 \t Batch: 12800 \t loss: 0.51228\n",
      "Epoch: 0 \t Batch: 12900 \t loss: 0.50682\n",
      "Epoch: 0 \t Batch: 13000 \t loss: 0.50526\n",
      "Epoch: 0 \t Batch: 13100 \t loss: 0.50223\n",
      "Epoch: 0 \t Batch: 13200 \t loss: 0.50309\n",
      "Epoch: 0 \t Batch: 13300 \t loss: 0.50355\n",
      "Epoch: 0 \t Batch: 13400 \t loss: 0.50243\n",
      "Epoch: 0 \t Batch: 13500 \t loss: 0.50085\n",
      "Epoch: 0 \t Batch: 13600 \t loss: 0.49881\n",
      "Epoch: 0 \t Batch: 13700 \t loss: 0.50277\n",
      "Epoch: 0 \t Batch: 13800 \t loss: 0.49789\n",
      "Epoch: 0 \t Batch: 13900 \t loss: 0.49590\n",
      "Epoch: 0 \t Batch: 14000 \t loss: 0.50079\n",
      "Epoch: 0 \t Batch: 14100 \t loss: 0.49688\n",
      "Epoch: 0 \t Batch: 14200 \t loss: 0.49763\n",
      "Epoch: 0 \t Batch: 14300 \t loss: 0.49664\n",
      "Epoch: 0 \t Batch: 14400 \t loss: 0.49686\n",
      "Epoch: 0 \t Batch: 14500 \t loss: 0.49181\n",
      "Epoch: 0 \t Batch: 14600 \t loss: 0.50231\n",
      "Epoch: 0 \t Batch: 14700 \t loss: 0.49373\n",
      "Epoch: 0 \t Batch: 14800 \t loss: 0.49815\n",
      "Epoch: 0 \t Batch: 14900 \t loss: 0.49265\n",
      "Epoch: 0 \t Batch: 15000 \t loss: 0.49577\n",
      "Epoch: 0 \t Batch: 15100 \t loss: 0.48985\n",
      "Epoch: 0 \t Batch: 15200 \t loss: 0.48774\n",
      "Epoch: 0 \t Batch: 15300 \t loss: 0.49294\n",
      "Epoch: 0 \t Batch: 15400 \t loss: 0.49384\n",
      "Epoch: 0 \t Batch: 15500 \t loss: 0.48514\n",
      "Epoch: 0 \t Batch: 15600 \t loss: 0.49408\n",
      "Epoch: 0 \t Batch: 15700 \t loss: 0.48951\n",
      "Epoch: 0 \t Batch: 15800 \t loss: 0.48912\n",
      "Epoch: 0 \t Batch: 15900 \t loss: 0.48312\n",
      "Epoch: 0 \t Batch: 16000 \t loss: 0.48858\n",
      "Epoch: 0 \t Batch: 16100 \t loss: 0.49180\n",
      "Epoch: 0 \t Batch: 16200 \t loss: 0.48666\n",
      "Epoch: 0 \t Batch: 16300 \t loss: 0.48921\n",
      "Epoch: 0 \t Batch: 16400 \t loss: 0.48625\n",
      "Epoch: 0 \t Batch: 16500 \t loss: 0.48864\n",
      "Epoch: 0 \t Batch: 16600 \t loss: 0.48677\n",
      "Epoch: 0 \t Batch: 16700 \t loss: 0.48355\n",
      "Epoch: 0 \t Batch: 16800 \t loss: 0.48527\n",
      "Epoch: 0 \t Batch: 16900 \t loss: 0.48416\n",
      "Epoch: 0 \t Batch: 17000 \t loss: 0.48840\n",
      "Epoch: 0 \t Batch: 17100 \t loss: 0.48266\n",
      "Epoch: 0 \t Batch: 17200 \t loss: 0.48554\n",
      "Epoch: 0 \t Batch: 17300 \t loss: 0.48265\n",
      "Epoch: 0 \t Batch: 17400 \t loss: 0.48359\n",
      "Epoch: 0 \t Batch: 17500 \t loss: 0.48575\n",
      "Epoch: 0 \t Batch: 17600 \t loss: 0.48643\n",
      "Epoch: 0 \t Batch: 17700 \t loss: 0.48243\n",
      "Epoch: 0 \t Batch: 17800 \t loss: 0.48313\n",
      "Epoch: 0 \t Batch: 17900 \t loss: 0.48423\n",
      "Epoch: 0 \t Batch: 18000 \t loss: 0.48924\n",
      "Epoch: 0 \t Batch: 18100 \t loss: 0.48180\n",
      "Epoch: 0 \t Batch: 18200 \t loss: 0.48407\n",
      "Epoch: 0 \t Batch: 18300 \t loss: 0.48206\n",
      "Epoch: 0 \t Batch: 18400 \t loss: 0.48154\n",
      "Epoch: 0 \t Batch: 18500 \t loss: 0.48017\n",
      "Epoch: 0 \t Batch: 18600 \t loss: 0.48837\n",
      "Epoch: 0 \t Batch: 18700 \t loss: 0.47661\n",
      "Epoch: 0 \t Batch: 18800 \t loss: 0.48399\n",
      "Epoch: 0 \t Batch: 18900 \t loss: 0.48452\n",
      "Epoch: 0 \t Batch: 19000 \t loss: 0.47892\n",
      "Epoch: 0 \t Batch: 19100 \t loss: 0.47972\n",
      "Epoch: 0 \t Batch: 19200 \t loss: 0.48283\n",
      "Epoch: 0 \t Batch: 19300 \t loss: 0.47942\n",
      "Epoch: 0 \t Batch: 19400 \t loss: 0.47906\n",
      "Epoch: 0 \t Batch: 19500 \t loss: 0.48133\n",
      "Epoch: 0 \t Batch: 19600 \t loss: 0.47601\n",
      "Epoch: 0 \t Batch: 19700 \t loss: 0.47781\n",
      "Epoch: 0 \t Batch: 19800 \t loss: 0.48183\n",
      "Epoch: 0 \t Batch: 19900 \t loss: 0.47628\n",
      "Epoch: 0 \t Batch: 20000 \t loss: 0.47956\n",
      "Epoch: 0 \t Batch: 20100 \t loss: 0.47870\n",
      "Epoch: 0 \t Batch: 20200 \t loss: 0.47731\n",
      "Epoch: 0 \t Batch: 20300 \t loss: 0.47374\n",
      "Epoch: 0 \t Batch: 20400 \t loss: 0.47602\n",
      "Epoch: 0 \t Batch: 20500 \t loss: 0.47387\n",
      "Epoch: 0 \t Batch: 20600 \t loss: 0.48055\n",
      "Epoch: 0 \t Batch: 20700 \t loss: 0.47705\n",
      "Epoch: 0 \t Batch: 20800 \t loss: 0.47703\n",
      "Epoch: 0 \t Batch: 20900 \t loss: 0.47782\n",
      "Epoch: 0 \t Batch: 21000 \t loss: 0.47531\n",
      "Epoch: 0 \t Batch: 21100 \t loss: 0.47560\n",
      "Epoch: 0 \t Batch: 21200 \t loss: 0.47106\n",
      "Epoch: 0 \t Batch: 21300 \t loss: 0.47227\n",
      "Epoch: 0 \t Batch: 21400 \t loss: 0.47802\n",
      "Epoch: 0 \t Batch: 21500 \t loss: 0.47623\n",
      "Epoch: 0 \t Batch: 21600 \t loss: 0.47515\n",
      "Epoch: 0 \t Batch: 21700 \t loss: 0.47707\n",
      "Epoch: 0 \t Batch: 21800 \t loss: 0.47594\n",
      "Epoch: 0 \t Batch: 21900 \t loss: 0.47474\n",
      "Epoch: 0 \t Batch: 22000 \t loss: 0.47365\n",
      "Epoch: 0 \t Batch: 22100 \t loss: 0.46877\n",
      "Epoch: 0 \t Batch: 22200 \t loss: 0.47437\n",
      "Epoch: 0 \t Batch: 22300 \t loss: 0.47314\n",
      "Epoch: 0 \t Batch: 22400 \t loss: 0.47677\n",
      "Epoch: 0 \t Batch: 22500 \t loss: 0.47429\n",
      "Epoch: 0 \t Batch: 22600 \t loss: 0.47205\n",
      "Epoch: 0 \t Batch: 22700 \t loss: 0.47341\n",
      "Epoch: 0 \t Batch: 22800 \t loss: 0.46939\n",
      "Epoch: 0 \t Batch: 22900 \t loss: 0.47383\n",
      "Epoch: 0 \t Batch: 23000 \t loss: 0.47629\n",
      "Epoch: 0 \t Batch: 23100 \t loss: 0.47264\n",
      "Epoch: 0 \t Batch: 23200 \t loss: 0.47448\n",
      "Epoch: 0 \t Batch: 23300 \t loss: 0.47176\n",
      "Epoch: 0 \t Batch: 23400 \t loss: 0.47245\n",
      "Epoch: 0 \t Batch: 23500 \t loss: 0.47404\n",
      "Epoch: 0 \t Batch: 23600 \t loss: 0.47023\n",
      "Epoch: 0 \t Batch: 23700 \t loss: 0.46816\n",
      "Epoch: 0 \t Batch: 23800 \t loss: 0.47510\n",
      "Epoch: 0 \t Batch: 23900 \t loss: 0.47276\n",
      "Epoch: 0 \t Batch: 24000 \t loss: 0.47156\n",
      "Epoch: 0 \t Batch: 24100 \t loss: 0.47207\n",
      "Epoch: 0 \t Batch: 24200 \t loss: 0.47281\n",
      "Epoch: 0 \t Batch: 24300 \t loss: 0.47177\n",
      "Epoch: 0 \t Batch: 24400 \t loss: 0.47488\n",
      "Epoch: 0 \t Batch: 24500 \t loss: 0.47315\n",
      "Epoch: 0 \t Batch: 24600 \t loss: 0.46908\n",
      "Epoch: 0 \t Batch: 24700 \t loss: 0.47354\n",
      "Epoch: 0 \t Batch: 24800 \t loss: 0.46926\n",
      "Epoch: 0 \t Batch: 24900 \t loss: 0.46758\n",
      "Epoch: 0 \t Batch: 25000 \t loss: 0.46666\n",
      "Epoch: 0 \t Batch: 25100 \t loss: 0.47463\n",
      "Epoch: 0 \t Batch: 25200 \t loss: 0.47037\n",
      "Epoch: 0 \t Batch: 25300 \t loss: 0.47210\n",
      "Epoch: 0 \t Batch: 25400 \t loss: 0.47165\n",
      "Epoch: 0 \t Batch: 25500 \t loss: 0.46829\n",
      "Epoch: 0 \t Batch: 25600 \t loss: 0.47325\n",
      "Epoch: 0 \t Batch: 25700 \t loss: 0.46666\n",
      "Epoch: 0 \t Batch: 25800 \t loss: 0.46953\n",
      "Epoch: 0 \t Batch: 25900 \t loss: 0.46805\n",
      "Epoch: 0 \t Batch: 26000 \t loss: 0.47003\n",
      "Epoch: 0 \t Batch: 26100 \t loss: 0.46514\n",
      "Epoch: 0 \t Batch: 26200 \t loss: 0.46696\n",
      "Epoch: 0 \t Batch: 26300 \t loss: 0.47129\n",
      "Epoch: 0 \t Batch: 26400 \t loss: 0.46795\n",
      "Epoch: 0 \t Batch: 26500 \t loss: 0.46317\n",
      "Epoch: 0 \t Batch: 26600 \t loss: 0.47014\n",
      "Epoch: 0 \t Batch: 26700 \t loss: 0.47208\n",
      "Epoch: 0 \t Batch: 26800 \t loss: 0.47354\n",
      "Epoch: 0 \t Batch: 26900 \t loss: 0.46886\n",
      "Epoch: 0 \t Batch: 27000 \t loss: 0.46891\n",
      "Epoch: 0 \t Batch: 27100 \t loss: 0.46757\n",
      "Epoch: 0 \t Batch: 27200 \t loss: 0.46605\n",
      "Epoch: 0 \t Batch: 27300 \t loss: 0.46307\n",
      "Epoch: 0 \t Batch: 27400 \t loss: 0.46241\n",
      "Epoch: 0 \t Batch: 27500 \t loss: 0.46920\n",
      "Epoch: 0 \t Batch: 27600 \t loss: 0.46894\n",
      "Epoch: 0 \t Batch: 27700 \t loss: 0.46795\n",
      "Epoch: 0 \t Batch: 27800 \t loss: 0.46602\n",
      "Epoch: 0 \t Batch: 27900 \t loss: 0.46396\n",
      "Epoch: 0 \t Batch: 28000 \t loss: 0.46742\n",
      "Epoch: 0 \t Batch: 28100 \t loss: 0.46944\n",
      "Epoch: 0 \t Batch: 28200 \t loss: 0.46842\n",
      "Epoch: 0 \t Batch: 28300 \t loss: 0.46555\n",
      "Epoch: 0 \t Batch: 28400 \t loss: 0.46944\n",
      "Epoch: 0 \t Batch: 28500 \t loss: 0.46647\n",
      "Epoch: 0 \t Batch: 28600 \t loss: 0.46415\n",
      "Epoch: 0 \t Batch: 28700 \t loss: 0.46756\n",
      "Epoch: 0 \t Batch: 28800 \t loss: 0.46644\n",
      "Epoch: 0 \t Batch: 28900 \t loss: 0.46616\n",
      "Epoch: 0 \t Batch: 29000 \t loss: 0.46279\n",
      "Epoch: 0 \t Batch: 29100 \t loss: 0.47021\n",
      "Epoch: 0 \t Batch: 29200 \t loss: 0.46642\n",
      "Epoch: 0 \t Batch: 29300 \t loss: 0.46180\n",
      "Epoch: 0 \t Batch: 29400 \t loss: 0.46265\n",
      "Epoch: 0 \t Batch: 29500 \t loss: 0.46775\n",
      "Epoch: 0 \t Batch: 29600 \t loss: 0.46378\n",
      "Epoch: 0 \t Batch: 29700 \t loss: 0.46704\n",
      "Epoch: 0 \t Batch: 29800 \t loss: 0.46584\n",
      "Epoch: 0 \t Batch: 29900 \t loss: 0.46614\n",
      "Epoch: 0 \t Batch: 30000 \t loss: 0.46456\n",
      "Epoch: 0 \t Batch: 30100 \t loss: 0.46171\n",
      "Epoch: 0 \t Batch: 30200 \t loss: 0.46245\n",
      "Epoch: 0 \t Batch: 30300 \t loss: 0.46433\n",
      "Epoch: 0 \t Batch: 30400 \t loss: 0.46500\n",
      "Epoch: 0 \t Batch: 30500 \t loss: 0.46065\n",
      "Epoch: 0 \t Batch: 30600 \t loss: 0.46688\n",
      "Epoch: 0 \t Batch: 30700 \t loss: 0.46244\n",
      "Epoch: 0 \t Batch: 30800 \t loss: 0.46475\n",
      "Epoch: 0 \t Batch: 30900 \t loss: 0.46425\n",
      "Epoch: 0 \t Batch: 31000 \t loss: 0.46512\n",
      "Epoch: 0 \t Batch: 31100 \t loss: 0.46320\n",
      "Epoch: 0 \t Batch: 31200 \t loss: 0.45936\n",
      "Epoch: 0 \t Batch: 31300 \t loss: 0.46304\n",
      "Epoch: 0 \t Batch: 31400 \t loss: 0.46038\n",
      "Epoch: 0 \t Batch: 31500 \t loss: 0.46180\n",
      "Epoch: 0 \t Batch: 31600 \t loss: 0.46449\n",
      "Epoch: 0 \t Batch: 31700 \t loss: 0.46331\n",
      "Epoch: 0 \t Batch: 31800 \t loss: 0.46023\n",
      "Epoch: 0 \t Batch: 31900 \t loss: 0.46462\n",
      "Epoch: 0 \t Batch: 32000 \t loss: 0.46355\n",
      "Epoch: 0 \t Batch: 32100 \t loss: 0.46135\n",
      "Epoch: 0 \t Batch: 32200 \t loss: 0.46213\n",
      "Epoch: 0 \t Batch: 32300 \t loss: 0.46285\n",
      "Epoch: 0 \t Batch: 32400 \t loss: 0.46367\n",
      "Epoch: 0 \t Batch: 32500 \t loss: 0.45455\n",
      "Epoch: 0 \t Batch: 32600 \t loss: 0.46225\n",
      "Epoch: 0 \t Batch: 32700 \t loss: 0.45873\n",
      "Epoch: 0 \t Batch: 32800 \t loss: 0.46145\n",
      "Epoch: 0 \t Batch: 32900 \t loss: 0.46145\n",
      "Epoch: 0 \t Batch: 33000 \t loss: 0.45851\n",
      "Epoch: 0 \t Batch: 33100 \t loss: 0.46203\n",
      "Epoch: 0 \t Batch: 33200 \t loss: 0.46147\n",
      "Epoch: 0 \t Batch: 33300 \t loss: 0.46211\n",
      "Epoch: 0 \t Batch: 33400 \t loss: 0.46130\n",
      "Epoch: 0 \t Batch: 33500 \t loss: 0.46448\n",
      "Epoch: 0 \t Batch: 33600 \t loss: 0.46090\n",
      "Epoch: 0 \t Batch: 33700 \t loss: 0.46149\n",
      "Epoch: 0 \t Batch: 33800 \t loss: 0.46145\n",
      "Epoch: 0 \t Batch: 33900 \t loss: 0.46064\n",
      "Epoch: 0 \t Batch: 34000 \t loss: 0.46045\n",
      "Epoch: 0 \t Batch: 34100 \t loss: 0.45877\n",
      "Epoch: 0 \t Batch: 34200 \t loss: 0.46289\n",
      "Epoch: 0 \t Batch: 34300 \t loss: 0.46160\n",
      "Epoch: 0 \t Batch: 34400 \t loss: 0.45744\n",
      "Epoch: 0 \t Batch: 34500 \t loss: 0.45753\n",
      "Epoch: 0 \t Batch: 34600 \t loss: 0.45565\n",
      "Epoch: 0 \t Batch: 34700 \t loss: 0.46247\n",
      "Epoch: 0 \t Batch: 34800 \t loss: 0.46174\n",
      "Epoch: 0 \t Batch: 34900 \t loss: 0.45648\n",
      "Epoch: 0 \t Batch: 35000 \t loss: 0.45808\n",
      "Epoch: 0 \t Batch: 35100 \t loss: 0.46042\n",
      "Epoch: 0 \t Batch: 35200 \t loss: 0.45733\n",
      "Epoch: 0 \t Batch: 35300 \t loss: 0.46122\n",
      "Epoch: 0 \t Batch: 35400 \t loss: 0.46139\n",
      "Epoch: 0 \t Batch: 35500 \t loss: 0.46116\n",
      "Epoch: 0 \t Batch: 35600 \t loss: 0.46030\n",
      "Epoch: 0 \t Batch: 35700 \t loss: 0.46328\n",
      "Epoch: 0 \t Batch: 35800 \t loss: 0.45926\n",
      "Epoch: 0 \t Batch: 35900 \t loss: 0.45802\n",
      "Epoch: 0 \t Batch: 36000 \t loss: 0.45627\n",
      "Epoch: 0 \t Batch: 36100 \t loss: 0.45663\n",
      "Epoch: 0 \t Batch: 36200 \t loss: 0.45811\n",
      "Epoch: 0 \t Batch: 36300 \t loss: 0.45695\n",
      "Epoch: 0 \t Batch: 36400 \t loss: 0.45938\n",
      "Epoch: 0 \t Batch: 36500 \t loss: 0.45582\n",
      "Epoch: 0 \t Batch: 36600 \t loss: 0.45580\n",
      "Epoch: 0 \t Batch: 36700 \t loss: 0.45937\n",
      "Epoch: 0 \t Batch: 36800 \t loss: 0.45903\n",
      "Epoch: 0 \t Batch: 36900 \t loss: 0.46013\n",
      "Epoch: 0 \t Batch: 37000 \t loss: 0.45833\n",
      "Epoch: 0 \t Batch: 37100 \t loss: 0.45892\n",
      "Epoch: 0 \t Batch: 37200 \t loss: 0.45886\n",
      "Epoch: 0 \t Batch: 37300 \t loss: 0.45859\n",
      "Epoch: 0 \t Batch: 37400 \t loss: 0.45690\n",
      "Epoch: 0 \t Batch: 37500 \t loss: 0.45799\n",
      "Epoch: 0 \t Batch: 37600 \t loss: 0.45765\n",
      "Epoch: 0 \t Batch: 37700 \t loss: 0.45393\n",
      "Epoch: 0 \t Batch: 37800 \t loss: 0.45659\n",
      "Epoch: 0 \t Batch: 37900 \t loss: 0.45869\n",
      "Epoch: 0 \t Batch: 38000 \t loss: 0.45846\n",
      "Epoch: 0 \t Batch: 38100 \t loss: 0.45525\n",
      "Epoch: 0 \t Batch: 38200 \t loss: 0.46048\n",
      "Epoch: 0 \t Batch: 38300 \t loss: 0.45850\n",
      "Epoch: 0 \t Batch: 38400 \t loss: 0.45543\n",
      "Epoch: 0 \t Batch: 38500 \t loss: 0.45657\n",
      "Epoch: 0 \t Batch: 38600 \t loss: 0.46003\n",
      "Epoch: 0 \t Batch: 38700 \t loss: 0.45581\n",
      "Epoch: 0 \t Batch: 38800 \t loss: 0.46088\n",
      "Epoch: 0 \t Batch: 38900 \t loss: 0.46248\n",
      "Epoch: 0 \t Batch: 39000 \t loss: 0.45608\n",
      "Epoch: 0 \t Batch: 39100 \t loss: 0.45417\n",
      "Epoch: 0 \t Batch: 39200 \t loss: 0.45707\n",
      "Epoch: 0 \t Batch: 39300 \t loss: 0.45887\n",
      "Epoch: 0 \t Batch: 39400 \t loss: 0.45900\n",
      "Epoch: 0 \t Batch: 39500 \t loss: 0.45419\n",
      "Epoch: 0 \t Batch: 39600 \t loss: 0.46015\n",
      "Epoch: 0 \t Batch: 39700 \t loss: 0.46037\n",
      "Epoch: 0 \t Batch: 39800 \t loss: 0.45690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vilab/anaconda3/envs/bia/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:802: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 39900 \t loss: 0.45993\n",
      "Epoch: 0 \t Batch: 40000 \t loss: 0.45828\n",
      "Epoch: 0 \t Batch: 40100 \t loss: 0.45178\n",
      "Epoch: 0 \t Batch: 40200 \t loss: 0.45736\n",
      "Epoch: 0 \t Batch: 40300 \t loss: 0.45748\n",
      "Epoch: 0 \t Batch: 40400 \t loss: 0.45734\n",
      "Epoch: 0 \t Batch: 40500 \t loss: 0.45449\n",
      "Epoch: 0 \t Batch: 40600 \t loss: 0.45700\n",
      "Epoch: 0 \t Batch: 40700 \t loss: 0.45748\n",
      "Epoch: 0 \t Batch: 40800 \t loss: 0.45408\n",
      "Epoch: 0 \t Batch: 40900 \t loss: 0.45492\n",
      "Epoch: 0 \t Batch: 41000 \t loss: 0.45386\n",
      "Epoch: 0 \t Batch: 41100 \t loss: 0.45225\n",
      "Epoch: 0 \t Batch: 41200 \t loss: 0.45089\n",
      "Epoch: 0 \t Batch: 41300 \t loss: 0.45658\n",
      "Epoch: 0 \t Batch: 41400 \t loss: 0.45635\n",
      "Epoch: 0 \t Batch: 41500 \t loss: 0.45526\n",
      "Epoch: 0 \t Batch: 41600 \t loss: 0.45453\n",
      "Epoch: 0 \t Batch: 41700 \t loss: 0.45614\n",
      "Epoch: 0 \t Batch: 41800 \t loss: 0.45899\n",
      "Epoch: 0 \t Batch: 41900 \t loss: 0.45395\n",
      "Epoch: 0 \t Batch: 42000 \t loss: 0.45589\n",
      "Epoch: 0 \t Batch: 42100 \t loss: 0.45389\n",
      "Epoch: 0 \t Batch: 42200 \t loss: 0.45681\n",
      "Epoch: 0 \t Batch: 42300 \t loss: 0.45693\n",
      "Epoch: 0 \t Batch: 42400 \t loss: 0.45466\n",
      "Epoch: 0 \t Batch: 42500 \t loss: 0.45599\n",
      "Epoch: 0 \t Batch: 42600 \t loss: 0.45288\n",
      "Epoch: 0 \t Batch: 42700 \t loss: 0.45676\n",
      "Epoch: 0 \t Batch: 42800 \t loss: 0.45347\n",
      "Epoch: 0 \t Batch: 42900 \t loss: 0.45501\n",
      "Epoch: 0 \t Batch: 43000 \t loss: 0.45239\n",
      "Epoch: 0 \t Batch: 43100 \t loss: 0.45302\n",
      "Epoch: 0 \t Batch: 43200 \t loss: 0.45238\n",
      "Epoch: 0 \t Batch: 43300 \t loss: 0.45613\n",
      "Epoch: 0 \t Batch: 43400 \t loss: 0.45364\n",
      "Epoch: 0 \t Batch: 43500 \t loss: 0.45210\n",
      "Epoch: 0 \t Batch: 43600 \t loss: 0.45238\n",
      "Epoch: 0 \t Batch: 43700 \t loss: 0.45322\n",
      "Epoch: 0 \t Batch: 43800 \t loss: 0.45514\n",
      "Epoch: 0 \t Batch: 43900 \t loss: 0.45553\n",
      "Epoch: 0 \t Batch: 44000 \t loss: 0.45492\n",
      "Epoch: 0 \t Batch: 44100 \t loss: 0.44978\n",
      "Epoch: 0 \t Batch: 44200 \t loss: 0.45776\n",
      "Epoch: 0 \t Batch: 44300 \t loss: 0.45328\n",
      "Epoch: 0 \t Batch: 44400 \t loss: 0.45603\n",
      "Epoch: 0 \t Batch: 44500 \t loss: 0.45565\n",
      "Epoch: 0 \t Batch: 44600 \t loss: 0.45011\n",
      "Epoch: 0 \t Batch: 44700 \t loss: 0.45471\n",
      "Epoch: 0 \t Batch: 44800 \t loss: 0.45106\n",
      "Epoch: 0 \t Batch: 44900 \t loss: 0.45235\n",
      "Epoch: 0 \t Batch: 45000 \t loss: 0.45469\n",
      "Epoch: 0 \t Batch: 45100 \t loss: 0.45065\n",
      "Epoch: 0 \t Batch: 45200 \t loss: 0.45590\n",
      "Epoch: 0 \t Batch: 45300 \t loss: 0.45006\n",
      "Epoch: 0 \t Batch: 45400 \t loss: 0.45635\n",
      "Epoch: 0 \t Batch: 45500 \t loss: 0.45371\n",
      "Epoch: 0 \t Batch: 45600 \t loss: 0.45224\n",
      "Epoch: 0 \t Batch: 45700 \t loss: 0.45285\n",
      "Epoch: 0 \t Batch: 45800 \t loss: 0.45560\n",
      "Epoch: 0 \t Batch: 45900 \t loss: 0.45611\n",
      "Epoch: 0 \t Batch: 46000 \t loss: 0.45285\n",
      "Epoch: 0 \t Batch: 46100 \t loss: 0.45508\n",
      "Epoch: 0 \t Batch: 46200 \t loss: 0.45159\n",
      "Epoch: 0 \t Batch: 46300 \t loss: 0.45549\n",
      "Epoch: 0 \t Batch: 46400 \t loss: 0.45147\n",
      "Epoch: 0 \t Batch: 46500 \t loss: 0.45643\n",
      "Epoch: 0 \t Batch: 46600 \t loss: 0.45368\n",
      "Epoch: 0 \t Batch: 46700 \t loss: 0.45743\n",
      "Epoch: 0 \t Batch: 46800 \t loss: 0.46100\n",
      "Epoch: 0 \t Batch: 46900 \t loss: 0.45307\n",
      "Epoch: 0 \t Batch: 47000 \t loss: 0.44809\n",
      "Epoch: 0 \t Batch: 47100 \t loss: 0.45460\n",
      "Epoch: 0 \t Batch: 47200 \t loss: 0.45289\n",
      "Epoch: 0 \t Batch: 47300 \t loss: 0.44801\n",
      "Epoch: 0 \t Batch: 47400 \t loss: 0.45185\n",
      "Epoch: 0 \t Batch: 47500 \t loss: 0.45745\n",
      "Epoch: 0 \t Batch: 47600 \t loss: 0.45331\n",
      "Epoch: 0 \t Batch: 47700 \t loss: 0.45430\n",
      "Epoch: 0 \t Batch: 47800 \t loss: 0.44829\n",
      "Epoch: 0 \t Batch: 47900 \t loss: 0.45111\n",
      "Epoch: 0 \t Batch: 48000 \t loss: 0.45070\n",
      "Epoch: 0 \t Batch: 48100 \t loss: 0.45186\n",
      "Epoch: 0 \t Batch: 48200 \t loss: 0.45640\n",
      "Epoch: 0 \t Batch: 48300 \t loss: 0.45182\n",
      "Epoch: 0 \t Batch: 48400 \t loss: 0.45220\n",
      "Epoch: 0 \t Batch: 48500 \t loss: 0.44920\n",
      "Epoch: 0 \t Batch: 48600 \t loss: 0.44891\n",
      "Epoch: 0 \t Batch: 48700 \t loss: 0.44796\n",
      "Epoch: 0 \t Batch: 48800 \t loss: 0.45079\n",
      "Epoch: 0 \t Batch: 48900 \t loss: 0.44790\n",
      "Epoch: 0 \t Batch: 49000 \t loss: 0.45473\n",
      "Epoch: 0 \t Batch: 49100 \t loss: 0.45888\n",
      "Epoch: 0 \t Batch: 49200 \t loss: 0.45420\n",
      "Epoch: 0 \t Batch: 49300 \t loss: 0.45428\n",
      "Epoch: 0 \t Batch: 49400 \t loss: 0.45081\n",
      "Epoch: 0 \t Batch: 49500 \t loss: 0.44969\n",
      "Epoch: 0 \t Batch: 49600 \t loss: 0.45464\n",
      "Epoch: 0 \t Batch: 49700 \t loss: 0.45236\n",
      "Epoch: 0 \t Batch: 49800 \t loss: 0.45100\n",
      "Epoch: 0 \t Batch: 49900 \t loss: 0.45503\n",
      "Epoch: 0 \t Batch: 50000 \t loss: 0.44882\n",
      "Epoch: 0 \t Batch: 50100 \t loss: 0.45163\n",
      "Epoch: 0 \t Batch: 50200 \t loss: 0.45677\n",
      "Epoch: 0 \t Batch: 50300 \t loss: 0.45463\n",
      "Epoch: 0 \t Batch: 50400 \t loss: 0.45193\n",
      "Epoch: 0 \t Batch: 50500 \t loss: 0.44899\n",
      "Epoch: 0 \t Batch: 50600 \t loss: 0.45116\n",
      "Epoch: 0 \t Batch: 50700 \t loss: 0.44896\n",
      "Epoch: 0 \t Batch: 50800 \t loss: 0.45284\n",
      "Epoch: 0 \t Batch: 50900 \t loss: 0.44858\n",
      "Epoch: 0 \t Batch: 51000 \t loss: 0.45171\n",
      "Epoch: 0 \t Batch: 51100 \t loss: 0.45277\n",
      "Epoch: 0 \t Batch: 51200 \t loss: 0.45510\n",
      "Epoch: 0 \t Batch: 51300 \t loss: 0.44684\n",
      "Epoch: 0 \t Batch: 51400 \t loss: 0.45427\n",
      "Epoch: 0 \t Batch: 51500 \t loss: 0.45245\n",
      "Epoch: 0 \t Batch: 51600 \t loss: 0.44929\n",
      "Epoch: 0 \t Batch: 51700 \t loss: 0.45104\n",
      "Epoch: 0 \t Batch: 51800 \t loss: 0.44817\n",
      "Epoch: 0 \t Batch: 51900 \t loss: 0.45564\n",
      "Epoch: 0 \t Batch: 52000 \t loss: 0.45294\n",
      "Epoch: 0 \t Batch: 52100 \t loss: 0.45434\n",
      "Epoch: 0 \t Batch: 52200 \t loss: 0.45121\n",
      "Epoch: 0 \t Batch: 52300 \t loss: 0.45454\n",
      "Epoch: 0 \t Batch: 52400 \t loss: 0.45124\n",
      "Epoch: 0 \t Batch: 52500 \t loss: 0.45083\n",
      "Epoch: 0 \t Batch: 52600 \t loss: 0.44943\n",
      "Epoch: 0 \t Batch: 52700 \t loss: 0.44858\n",
      "Epoch: 0 \t Batch: 52800 \t loss: 0.44992\n",
      "Epoch: 0 \t Batch: 52900 \t loss: 0.44869\n",
      "Epoch: 0 \t Batch: 53000 \t loss: 0.44791\n",
      "Epoch: 0 \t Batch: 53100 \t loss: 0.45052\n",
      "Epoch: 0 \t Batch: 53200 \t loss: 0.45380\n",
      "Epoch: 0 \t Batch: 53300 \t loss: 0.45143\n",
      "Epoch: 0 \t Batch: 53400 \t loss: 0.45076\n",
      "Epoch: 0 \t Batch: 53500 \t loss: 0.45029\n",
      "Epoch: 0 \t Batch: 53600 \t loss: 0.45045\n",
      "Epoch: 0 \t Batch: 53700 \t loss: 0.44923\n",
      "Epoch: 0 \t Batch: 53800 \t loss: 0.44817\n",
      "Epoch: 0 \t Batch: 53900 \t loss: 0.45085\n",
      "Epoch: 0 \t Batch: 54000 \t loss: 0.44690\n",
      "Epoch: 0 \t Batch: 54100 \t loss: 0.44809\n",
      "Epoch: 0 \t Batch: 54200 \t loss: 0.44778\n",
      "Epoch: 0 \t Batch: 54300 \t loss: 0.44946\n",
      "Epoch: 0 \t Batch: 54400 \t loss: 0.45246\n",
      "Epoch: 0 \t Batch: 54500 \t loss: 0.45460\n",
      "Epoch: 0 \t Batch: 54600 \t loss: 0.44880\n",
      "Epoch: 0 \t Batch: 54700 \t loss: 0.45095\n",
      "Epoch: 0 \t Batch: 54800 \t loss: 0.44677\n",
      "Epoch: 0 \t Batch: 54900 \t loss: 0.44840\n",
      "Epoch: 0 \t Batch: 55000 \t loss: 0.45203\n",
      "Epoch: 0 \t Batch: 55100 \t loss: 0.44447\n",
      "Epoch: 0 \t Batch: 55200 \t loss: 0.45009\n",
      "Epoch: 0 \t Batch: 55300 \t loss: 0.45114\n",
      "Epoch: 0 \t Batch: 55400 \t loss: 0.45046\n",
      "Epoch: 0 \t Batch: 55500 \t loss: 0.44809\n",
      "Epoch: 0 \t Batch: 55600 \t loss: 0.45334\n",
      "Epoch: 0 \t Batch: 55700 \t loss: 0.45213\n",
      "Epoch: 0 \t Batch: 55800 \t loss: 0.44603\n",
      "Epoch: 0 \t Batch: 55900 \t loss: 0.44987\n",
      "Epoch: 0 \t Batch: 56000 \t loss: 0.45156\n",
      "Epoch: 0 \t Batch: 56100 \t loss: 0.45347\n",
      "Epoch: 0 \t Batch: 56200 \t loss: 0.44854\n",
      "Epoch: 0 \t Batch: 56300 \t loss: 0.44811\n",
      "Epoch: 0 \t Batch: 56400 \t loss: 0.44260\n",
      "Epoch: 0 \t Batch: 56500 \t loss: 0.45084\n",
      "Epoch: 0 \t Batch: 56600 \t loss: 0.44876\n",
      "Epoch: 0 \t Batch: 56700 \t loss: 0.45231\n",
      "Epoch: 0 \t Batch: 56800 \t loss: 0.44970\n",
      "Epoch: 0 \t Batch: 56900 \t loss: 0.44835\n",
      "Epoch: 0 \t Batch: 57000 \t loss: 0.45019\n",
      "Epoch: 0 \t Batch: 57100 \t loss: 0.45015\n",
      "Epoch: 0 \t Batch: 57200 \t loss: 0.45182\n",
      "Epoch: 0 \t Batch: 57300 \t loss: 0.44896\n",
      "Epoch: 0 \t Batch: 57400 \t loss: 0.45170\n",
      "Epoch: 0 \t Batch: 57500 \t loss: 0.45062\n",
      "Epoch: 0 \t Batch: 57600 \t loss: 0.44641\n",
      "Epoch: 0 \t Batch: 57700 \t loss: 0.45176\n",
      "Epoch: 0 \t Batch: 57800 \t loss: 0.45332\n",
      "Epoch: 0 \t Batch: 57900 \t loss: 0.44430\n",
      "Epoch: 0 \t Batch: 58000 \t loss: 0.44761\n",
      "Epoch: 0 \t Batch: 58100 \t loss: 0.45128\n",
      "Epoch: 0 \t Batch: 58200 \t loss: 0.44951\n",
      "Epoch: 0 \t Batch: 58300 \t loss: 0.44455\n",
      "Epoch: 0 \t Batch: 58400 \t loss: 0.45054\n",
      "Epoch: 0 \t Batch: 58500 \t loss: 0.44236\n",
      "Epoch: 0 \t Batch: 58600 \t loss: 0.44709\n",
      "Epoch: 0 \t Batch: 58700 \t loss: 0.44895\n",
      "Epoch: 0 \t Batch: 58800 \t loss: 0.44730\n",
      "Epoch: 0 \t Batch: 58900 \t loss: 0.45214\n",
      "Epoch: 0 \t Batch: 59000 \t loss: 0.44712\n",
      "Epoch: 0 \t Batch: 59100 \t loss: 0.44982\n",
      "Epoch: 0 \t Batch: 59200 \t loss: 0.44791\n",
      "Epoch: 0 \t Batch: 59300 \t loss: 0.44789\n",
      "Epoch: 0 \t Batch: 59400 \t loss: 0.44718\n",
      "Epoch: 0 \t Batch: 59500 \t loss: 0.44987\n",
      "Epoch: 0 \t Batch: 59600 \t loss: 0.44836\n",
      "Epoch: 0 \t Batch: 59700 \t loss: 0.44864\n",
      "Epoch: 0 \t Batch: 59800 \t loss: 0.44789\n",
      "Epoch: 0 \t Batch: 59900 \t loss: 0.44821\n",
      "Epoch: 0 \t Batch: 60000 \t loss: 0.45031\n",
      "Epoch: 0 \t Batch: 60100 \t loss: 0.44809\n",
      "Epoch: 0 \t Batch: 60200 \t loss: 0.44830\n",
      "Epoch: 0 \t Batch: 60300 \t loss: 0.44781\n",
      "Epoch: 0 \t Batch: 60400 \t loss: 0.45152\n",
      "Epoch: 0 \t Batch: 60500 \t loss: 0.44589\n",
      "Epoch: 0 \t Batch: 60600 \t loss: 0.44662\n",
      "Epoch: 0 \t Batch: 60700 \t loss: 0.44556\n",
      "Epoch: 0 \t Batch: 60800 \t loss: 0.44594\n",
      "Epoch: 0 \t Batch: 60900 \t loss: 0.44890\n",
      "Epoch: 0 \t Batch: 61000 \t loss: 0.44797\n",
      "Epoch: 0 \t Batch: 61100 \t loss: 0.44907\n",
      "Epoch: 0 \t Batch: 61200 \t loss: 0.44596\n",
      "Epoch: 0 \t Batch: 61300 \t loss: 0.44882\n",
      "Epoch: 0 \t Batch: 61400 \t loss: 0.44658\n",
      "Epoch: 0 \t Batch: 61500 \t loss: 0.44475\n",
      "Epoch: 0 \t Batch: 61600 \t loss: 0.44673\n",
      "Epoch: 0 \t Batch: 61700 \t loss: 0.44684\n",
      "Epoch: 0 \t Batch: 61800 \t loss: 0.44531\n",
      "Epoch: 0 \t Batch: 61900 \t loss: 0.44971\n",
      "Epoch: 0 \t Batch: 62000 \t loss: 0.44442\n",
      "Epoch: 0 \t Batch: 62100 \t loss: 0.44535\n",
      "Epoch: 0 \t Batch: 62200 \t loss: 0.44752\n",
      "Epoch: 0 \t Batch: 62300 \t loss: 0.44608\n",
      "Epoch: 0 \t Batch: 62400 \t loss: 0.44455\n",
      "Epoch: 0 \t Batch: 62500 \t loss: 0.44725\n",
      "Epoch: 0 \t Batch: 62600 \t loss: 0.44997\n",
      "Epoch: 0 \t Batch: 62700 \t loss: 0.44788\n",
      "Epoch: 0 \t Batch: 62800 \t loss: 0.44438\n",
      "Epoch: 0 \t Batch: 62900 \t loss: 0.45171\n",
      "Epoch: 0 \t Batch: 63000 \t loss: 0.44415\n",
      "Epoch: 0 \t Batch: 63100 \t loss: 0.44439\n",
      "Epoch: 0 \t Batch: 63200 \t loss: 0.44666\n",
      "Epoch: 0 \t Batch: 63300 \t loss: 0.44488\n",
      "Epoch: 0 \t Batch: 63400 \t loss: 0.44982\n",
      "Epoch: 0 \t Batch: 63500 \t loss: 0.44645\n",
      "Epoch: 0 \t Batch: 63600 \t loss: 0.44654\n",
      "Epoch: 0 \t Batch: 63700 \t loss: 0.44373\n",
      "Epoch: 0 \t Batch: 63800 \t loss: 0.44509\n",
      "Epoch: 0 \t Batch: 63900 \t loss: 0.44439\n",
      "Epoch: 0 \t Batch: 64000 \t loss: 0.45082\n",
      "Epoch: 0 \t Batch: 64100 \t loss: 0.44752\n",
      "Epoch: 0 \t Batch: 64200 \t loss: 0.44642\n",
      "Epoch: 0 \t Batch: 64300 \t loss: 0.45153\n",
      "Epoch: 0 \t Batch: 64400 \t loss: 0.45071\n",
      "Epoch: 0 \t Batch: 64500 \t loss: 0.44507\n",
      "Epoch: 0 \t Batch: 64600 \t loss: 0.44871\n",
      "Epoch: 0 \t Batch: 64700 \t loss: 0.44575\n",
      "Epoch: 0 \t Batch: 64800 \t loss: 0.44940\n",
      "Epoch: 0 \t Batch: 64900 \t loss: 0.44489\n",
      "Epoch: 0 \t Batch: 65000 \t loss: 0.44604\n",
      "Epoch: 0 \t Batch: 65100 \t loss: 0.44548\n",
      "Epoch: 0 \t Batch: 65200 \t loss: 0.44453\n",
      "Epoch: 0 \t Batch: 65300 \t loss: 0.45034\n",
      "Epoch: 0 \t Batch: 65400 \t loss: 0.44618\n",
      "Epoch: 0 \t Batch: 65500 \t loss: 0.44765\n",
      "Epoch: 0 \t Batch: 65600 \t loss: 0.44642\n",
      "Epoch: 0 \t Batch: 65700 \t loss: 0.44314\n",
      "Epoch: 0 \t Batch: 65800 \t loss: 0.44640\n",
      "Epoch: 0 \t Batch: 65900 \t loss: 0.44353\n",
      "Epoch: 0 \t Batch: 66000 \t loss: 0.44698\n",
      "Epoch: 0 \t Batch: 66100 \t loss: 0.44510\n",
      "Epoch: 0 \t Batch: 66200 \t loss: 0.45161\n",
      "Epoch: 0 \t Batch: 66300 \t loss: 0.44807\n",
      "Epoch: 0 \t Batch: 66400 \t loss: 0.44497\n",
      "Epoch: 0 \t Batch: 66500 \t loss: 0.44420\n",
      "Epoch: 0 \t Batch: 66600 \t loss: 0.44237\n",
      "Epoch: 0 \t Batch: 66700 \t loss: 0.44645\n",
      "Epoch: 0 \t Batch: 66800 \t loss: 0.44519\n",
      "Epoch: 0 \t Batch: 66900 \t loss: 0.44965\n",
      "Epoch: 0 \t Batch: 67000 \t loss: 0.44532\n",
      "Epoch: 0 \t Batch: 67100 \t loss: 0.45060\n",
      "Epoch: 0 \t Batch: 67200 \t loss: 0.44935\n",
      "Epoch: 0 \t Batch: 67300 \t loss: 0.44528\n",
      "Epoch: 0 \t Batch: 67400 \t loss: 0.44776\n",
      "Epoch: 0 \t Batch: 67500 \t loss: 0.44416\n",
      "Epoch: 0 \t Batch: 67600 \t loss: 0.44516\n",
      "Epoch: 0 \t Batch: 67700 \t loss: 0.44761\n",
      "Epoch: 0 \t Batch: 67800 \t loss: 0.44226\n",
      "Epoch: 0 \t Batch: 67900 \t loss: 0.44456\n",
      "Epoch: 0 \t Batch: 68000 \t loss: 0.44717\n",
      "Epoch: 0 \t Batch: 68100 \t loss: 0.45152\n",
      "Epoch: 0 \t Batch: 68200 \t loss: 0.44202\n",
      "Epoch: 0 \t Batch: 68300 \t loss: 0.44622\n",
      "Epoch: 0 \t Batch: 68400 \t loss: 0.44440\n",
      "Epoch: 0 \t Batch: 68500 \t loss: 0.44598\n",
      "Epoch: 0 \t Batch: 68600 \t loss: 0.44674\n",
      "Epoch: 0 \t Batch: 68700 \t loss: 0.44538\n",
      "Epoch: 0 \t Batch: 68800 \t loss: 0.44423\n",
      "Epoch: 0 \t Batch: 68900 \t loss: 0.44338\n",
      "Epoch: 0 \t Batch: 69000 \t loss: 0.44564\n",
      "Epoch: 0 \t Batch: 69100 \t loss: 0.44720\n",
      "Epoch: 0 \t Batch: 69200 \t loss: 0.44543\n",
      "Epoch: 0 \t Batch: 69300 \t loss: 0.44181\n",
      "Epoch: 0 \t Batch: 69400 \t loss: 0.44287\n",
      "Epoch: 0 \t Batch: 69500 \t loss: 0.44265\n",
      "Epoch: 0 \t Batch: 69600 \t loss: 0.44159\n",
      "Epoch: 0 \t Batch: 69700 \t loss: 0.44595\n",
      "Epoch: 0 \t Batch: 69800 \t loss: 0.44715\n",
      "Epoch: 0 \t Batch: 69900 \t loss: 0.44681\n",
      "Epoch: 0 \t Batch: 70000 \t loss: 0.44556\n",
      "Epoch: 0 \t Batch: 70100 \t loss: 0.44453\n",
      "Epoch: 0 \t Batch: 70200 \t loss: 0.44511\n",
      "Epoch: 0 \t Batch: 70300 \t loss: 0.44550\n",
      "Epoch: 0 \t Batch: 70400 \t loss: 0.44331\n",
      "Epoch: 0 \t Batch: 70500 \t loss: 0.44870\n",
      "Epoch: 0 \t Batch: 70600 \t loss: 0.44198\n",
      "Epoch: 0 \t Batch: 70700 \t loss: 0.45025\n",
      "Epoch: 0 \t Batch: 70800 \t loss: 0.44580\n",
      "Epoch: 0 \t Batch: 70900 \t loss: 0.44653\n",
      "Epoch: 0 \t Batch: 71000 \t loss: 0.44474\n",
      "Epoch: 0 \t Batch: 71100 \t loss: 0.44598\n",
      "Epoch: 0 \t Batch: 71200 \t loss: 0.44589\n",
      "Epoch: 0 \t Batch: 71300 \t loss: 0.44769\n",
      "Epoch: 0 \t Batch: 71400 \t loss: 0.44233\n",
      "Epoch: 0 \t Batch: 71500 \t loss: 0.44745\n",
      "Epoch: 0 \t Batch: 71600 \t loss: 0.44958\n",
      "Epoch: 0 \t Batch: 71700 \t loss: 0.44571\n",
      "Epoch: 0 \t Batch: 71800 \t loss: 0.44717\n",
      "Epoch: 0 \t Batch: 71900 \t loss: 0.44803\n",
      "Epoch: 0 \t Batch: 72000 \t loss: 0.44644\n",
      "Epoch: 0 \t Batch: 72100 \t loss: 0.44354\n",
      "Epoch: 0 \t Batch: 72200 \t loss: 0.44510\n",
      "Epoch: 0 \t Batch: 72300 \t loss: 0.44473\n",
      "Epoch: 0 \t Batch: 72400 \t loss: 0.44203\n",
      "Epoch: 0 \t Batch: 72500 \t loss: 0.44541\n",
      "Epoch: 0 \t Batch: 72600 \t loss: 0.44706\n",
      "Epoch: 0 \t Batch: 72700 \t loss: 0.44457\n",
      "Epoch: 0 \t Batch: 72800 \t loss: 0.44839\n",
      "Epoch: 0 \t Batch: 72900 \t loss: 0.44772\n",
      "Epoch: 0 \t Batch: 73000 \t loss: 0.44560\n",
      "Epoch: 0 \t Batch: 73100 \t loss: 0.44724\n",
      "Epoch: 0 \t Batch: 73200 \t loss: 0.44548\n",
      "Epoch: 0 \t Batch: 73300 \t loss: 0.44184\n",
      "Epoch: 0 \t Batch: 73400 \t loss: 0.44540\n",
      "Epoch: 0 \t Batch: 73500 \t loss: 0.44650\n",
      "Epoch: 0 \t Batch: 73600 \t loss: 0.44607\n",
      "Epoch: 0 \t Batch: 73700 \t loss: 0.44650\n",
      "Epoch: 0 \t Batch: 73800 \t loss: 0.44343\n",
      "Epoch: 0 \t Batch: 73900 \t loss: 0.44607\n",
      "Epoch: 0 \t Batch: 74000 \t loss: 0.43939\n",
      "Epoch: 0 \t Batch: 74100 \t loss: 0.44589\n",
      "Epoch: 0 \t Batch: 74200 \t loss: 0.44226\n",
      "Epoch: 0 \t Batch: 74300 \t loss: 0.44363\n",
      "Epoch: 0 \t Batch: 74400 \t loss: 0.44408\n",
      "Epoch: 0 \t Batch: 74500 \t loss: 0.44446\n",
      "Epoch: 0 \t Batch: 74600 \t loss: 0.44242\n",
      "Epoch: 0 \t Batch: 74700 \t loss: 0.44255\n",
      "Epoch: 0 \t Batch: 74800 \t loss: 0.44351\n",
      "Epoch: 0 \t Batch: 74900 \t loss: 0.44569\n",
      "Epoch: 0 \t Batch: 75000 \t loss: 0.43918\n",
      "Epoch: 0 \t Batch: 75100 \t loss: 0.44416\n",
      "Epoch: 0 \t Batch: 75200 \t loss: 0.44352\n",
      "Epoch: 0 \t Batch: 75300 \t loss: 0.44611\n",
      "Epoch: 0 \t Batch: 75400 \t loss: 0.44374\n",
      "Epoch: 0 \t Batch: 75500 \t loss: 0.43940\n",
      "Epoch: 0 \t Batch: 75600 \t loss: 0.44722\n",
      "Epoch: 0 \t Batch: 75700 \t loss: 0.44111\n",
      "Epoch: 0 \t Batch: 75800 \t loss: 0.44021\n",
      "Epoch: 0 \t Batch: 75900 \t loss: 0.44593\n",
      "Epoch: 0 \t Batch: 76000 \t loss: 0.44504\n",
      "Epoch: 0 \t Batch: 76100 \t loss: 0.44768\n",
      "Epoch: 0 \t Batch: 76200 \t loss: 0.44342\n",
      "Epoch: 0 \t Batch: 76300 \t loss: 0.44411\n",
      "Epoch: 0 \t Batch: 76400 \t loss: 0.44571\n",
      "Epoch: 0 \t Batch: 76500 \t loss: 0.44424\n",
      "Epoch: 0 \t Batch: 76600 \t loss: 0.44569\n",
      "Epoch: 0 \t Batch: 76700 \t loss: 0.44267\n",
      "Epoch: 0 \t Batch: 76800 \t loss: 0.44230\n",
      "Epoch: 0 \t Batch: 76900 \t loss: 0.44156\n",
      "Epoch: 0 \t Batch: 77000 \t loss: 0.44497\n",
      "Epoch: 0 \t Batch: 77100 \t loss: 0.44519\n",
      "Epoch: 0 \t Batch: 77200 \t loss: 0.44846\n",
      "Epoch: 0 \t Batch: 77300 \t loss: 0.44120\n",
      "Epoch: 0 \t Batch: 77400 \t loss: 0.44261\n",
      "Epoch: 0 \t Batch: 77500 \t loss: 0.44155\n",
      "Epoch: 0 \t Batch: 77600 \t loss: 0.44748\n",
      "Epoch: 0 \t Batch: 77700 \t loss: 0.44315\n",
      "Epoch: 0 \t Batch: 77800 \t loss: 0.44461\n",
      "Epoch: 0 \t Batch: 77900 \t loss: 0.44580\n",
      "Epoch: 0 \t Batch: 78000 \t loss: 0.44175\n",
      "Epoch: 0 \t Batch: 78100 \t loss: 0.44234\n",
      "Epoch: 0 \t Batch: 78200 \t loss: 0.44146\n",
      "Epoch: 0 \t Batch: 78300 \t loss: 0.44135\n",
      "Epoch: 0 \t Batch: 78400 \t loss: 0.44035\n",
      "Epoch: 0 \t Batch: 78500 \t loss: 0.44527\n",
      "Epoch: 0 \t Batch: 78600 \t loss: 0.44120\n",
      "Epoch: 0 \t Batch: 78700 \t loss: 0.44238\n",
      "Epoch: 0 \t Batch: 78800 \t loss: 0.43989\n",
      "Epoch: 0 \t Batch: 78900 \t loss: 0.44212\n",
      "Epoch: 0 \t Batch: 79000 \t loss: 0.44253\n",
      "Epoch: 0 \t Batch: 79100 \t loss: 0.44287\n",
      "Epoch: 0 \t Batch: 79200 \t loss: 0.44583\n",
      "Epoch: 0 \t Batch: 79300 \t loss: 0.44308\n",
      "Epoch: 0 \t Batch: 79400 \t loss: 0.44215\n",
      "Epoch: 0 \t Batch: 79500 \t loss: 0.44551\n",
      "Epoch: 0 \t Batch: 79600 \t loss: 0.44289\n",
      "Epoch: 0 \t Batch: 79700 \t loss: 0.44337\n",
      "Epoch: 0 \t Batch: 79800 \t loss: 0.44558\n",
      "Epoch: 0 \t Batch: 79900 \t loss: 0.44462\n",
      "Epoch: 0 \t Batch: 80000 \t loss: 0.44392\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    running_loss = 0\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        netG.train()\n",
    "        optimG.zero_grad()\n",
    "        \n",
    "        if args.FA:\n",
    "            gauss = (torch.randn(img.size()[0], 3, img_size, img_size) * (args.sigma / 255)).to(device)\n",
    "            mask = (torch.rand_like(img) * 2 * args.rho + 1 - args.rho).to(device)\n",
    "            \n",
    "            img_dct = dct_2d(img + gauss).to(device)\n",
    "            img_idct = idct_2d(img_dct * mask)\n",
    "            img_idct = V(img_idct, requires_grad=True)\n",
    "            img = img_idct\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # adversarial translation        \n",
    "        adv = netG(img)\n",
    "        adv = torch.min(torch.max(adv, img - args.eps/255.0), img + args.eps/255.0)\n",
    "        adv = torch.clamp(adv, 0.0, 1.0)\n",
    "        \n",
    "\n",
    "        # if args.FA:\n",
    "        #     gauss = (torch.randn(img.size()[0], 3, img_size, img_size) * (args.sigma / 255)).to(device)\n",
    "        #     mask = (torch.rand_like(img) * 2 * args.rho + 1 - args.rho).to(device)\n",
    "            \n",
    "        #     img_dct = dct_2d(img + gauss).to(device)\n",
    "        #     img_idct = idct_2d(img_dct * mask)\n",
    "        #     img_idct = V(img_idct, requires_grad=True)\n",
    "        #     img = img_idct\n",
    "            \n",
    "        #     adv_dct = dct_2d(adv + gauss).to(device)\n",
    "        #     adv_idct = idct_2d(adv_dct * mask)\n",
    "        #     adv_idct = V(adv_idct, requires_grad=True)\n",
    "        #     adv = adv_idct\n",
    "        # else:\n",
    "        #     pass\n",
    "        \n",
    "        if args.RN:\n",
    "            mean = np.random.normal(0.50, 0.08) # default=(0.50, 0.08) \n",
    "            std = np.random.normal(0.75, 0.08) # default=(0.75, 0.08)\n",
    "            adv_out_slice = model(normalize(adv.clone(), mean, std))[layer_idx]\n",
    "            img_out_slice = model(normalize(img.clone(), mean, std))[layer_idx]\n",
    "        else:\n",
    "            adv_out_slice = model(default_normalize(adv.clone()))[layer_idx]\n",
    "            img_out_slice = model(default_normalize(img.clone()))[layer_idx]\n",
    "        \n",
    "        if args.DA:\n",
    "            attention = abs(torch.mean(img_out_slice, dim=1, keepdim=True)).detach()\n",
    "        else:\n",
    "            attention = torch.ones(adv_out_slice.shape).cuda()\n",
    "            \n",
    "        loss = torch.cosine_similarity((adv_out_slice*attention).reshape(adv_out_slice.shape[0], -1), \n",
    "                                       (img_out_slice*attention).reshape(img_out_slice.shape[0], -1)).mean()\n",
    "        loss.backward()\n",
    "        optimG.step()\n",
    "        \n",
    "        # Every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {0} \\t Batch: {1} \\t loss: {2:.5f}'.format(epoch, i, running_loss/100))\n",
    "            running_loss = 0\n",
    "        running_loss += abs(loss.item())\n",
    "        \n",
    "        # Every 1 epoch\n",
    "        if i % 80000 == 0 and i > 0: # 1epoch=80000batch\n",
    "            save_checkpoint_dir = 'saved_models/{}'.format(args.model_type)\n",
    "            if not os.path.exists(save_checkpoint_dir):\n",
    "                os.makedirs(save_checkpoint_dir)\n",
    "            save_path = os.path.join(save_checkpoint_dir, 'netG_{}_{}.pth'.format(save_checkpoint_suffix, epoch))\n",
    "            \n",
    "            if isinstance(netG, nn.DataParallel):\n",
    "                torch.save(netG.module.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a5ed91507c36eb248011b7d4b42840e9d47d02750f359633e20bb6b8b69add"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
