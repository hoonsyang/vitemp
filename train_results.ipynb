{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom\n",
    "from model_layer import Vgg16_all_layer, Vgg19_all_layer, Res152_all_layer, Dense169_all_layer\n",
    "from generator import GeneratorResnet\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DA=False, FA=False, RN=False, batch_size=16, epochs=1, eps=10, lr=0.0002, model_type='vgg16', train_dir='../dataset/imagenet/train')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Transferable Perturbation via Frequency Manipulation')\n",
    "parser.add_argument('--train_dir', default='../dataset/imagenet/train', help='Path for imagenet training data')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='Batch size')\n",
    "parser.add_argument('--epochs', type=int, default=1, help='Number of training epochs')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='Initial learning rate')\n",
    "parser.add_argument('--eps', type=int, default=10, help='Perturbation budget (0~255)')\n",
    "parser.add_argument('--model_type', type=str, default='vgg16', help='Victim model: vgg16, vgg19, res152, dense169')\n",
    "parser.add_argument('--RN', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Random Normalization module in training phase')\n",
    "parser.add_argument('--DA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Domain-agnostic Attention module in training phase')\n",
    "parser.add_argument('--FA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Frequency Augmentation module in training phase')\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the victim classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16(\n",
       "  (vgg): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (model): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.model_type == 'vgg16':\n",
    "    model = Vgg16_all_layer.Vgg16()\n",
    "    layer_idx = 16 # Maxpooling.3\n",
    "elif args.model_type == 'vgg19':\n",
    "    model = Vgg19_all_layer.Vgg19()\n",
    "    layer_idx = 18 # Maxpooling.3\n",
    "elif args.model_type == 'res152':\n",
    "    model = Res152_all_layer.Resnet152()\n",
    "    layer_idx = 5 # Conv3_8\n",
    "elif args.model_type == 'dense169':\n",
    "    model = Dense169_all_layer.Dense169()\n",
    "    layer_idx = 6 # Denseblock.2\n",
    "else:\n",
    "    raise Exception('Check the model_type')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generative attack model/optimizer/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1281167\n"
     ]
    }
   ],
   "source": [
    "# Model, Optimizer\n",
    "\n",
    "netG = GeneratorResnet()\n",
    "\n",
    "netG = nn.DataParallel(netG).to(device)\n",
    "# netG = netG.to(device)\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "\n",
    "if args.RN and args.DA:\n",
    "    save_checkpoint_suffix = 'BIA+RN+DA'\n",
    "elif args.RN:\n",
    "    save_checkpoint_suffix = 'BIA+RN'\n",
    "elif args.DA:\n",
    "    save_checkpoint_suffix = 'BIA+DA'\n",
    "elif args.FA:\n",
    "    save_checkpoint_suffix = 'BIA+FA'\n",
    "else:\n",
    "    save_checkpoint_suffix = 'BIA'\n",
    "\n",
    "# Data, Transform\n",
    "scale_size = 256\n",
    "img_size = 224\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(scale_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dir = args.train_dir\n",
    "train_set = datasets.ImageFolder(train_dir, data_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_size = len(train_set)\n",
    "print('Training data size:', train_size)\n",
    "\n",
    "def default_normalize(t):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - 0.485) / 0.229\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - 0.456) / 0.224\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - 0.406) / 0.225\n",
    "    return t\n",
    "\n",
    "def normalize(t, mean, std):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - mean) / std\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - mean) / std\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - mean) / std\n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t loss: 0.00000\n",
      "Epoch: 0 \t Batch: 100 \t loss: 0.60239\n",
      "Epoch: 0 \t Batch: 200 \t loss: 0.53301\n",
      "Epoch: 0 \t Batch: 300 \t loss: 0.51284\n",
      "Epoch: 0 \t Batch: 400 \t loss: 0.50395\n",
      "Epoch: 0 \t Batch: 500 \t loss: 0.47936\n",
      "Epoch: 0 \t Batch: 600 \t loss: 0.46319\n",
      "Epoch: 0 \t Batch: 700 \t loss: 0.45974\n",
      "Epoch: 0 \t Batch: 800 \t loss: 0.44782\n",
      "Epoch: 0 \t Batch: 900 \t loss: 0.45019\n",
      "Epoch: 0 \t Batch: 1000 \t loss: 0.44803\n",
      "Epoch: 0 \t Batch: 1100 \t loss: 0.44351\n",
      "Epoch: 0 \t Batch: 1200 \t loss: 0.42972\n",
      "Epoch: 0 \t Batch: 1300 \t loss: 0.43251\n",
      "Epoch: 0 \t Batch: 1400 \t loss: 0.42851\n",
      "Epoch: 0 \t Batch: 1500 \t loss: 0.42402\n",
      "Epoch: 0 \t Batch: 1600 \t loss: 0.41964\n",
      "Epoch: 0 \t Batch: 1700 \t loss: 0.41983\n",
      "Epoch: 0 \t Batch: 1800 \t loss: 0.41588\n",
      "Epoch: 0 \t Batch: 1900 \t loss: 0.41160\n",
      "Epoch: 0 \t Batch: 2000 \t loss: 0.41186\n",
      "Epoch: 0 \t Batch: 2100 \t loss: 0.41421\n",
      "Epoch: 0 \t Batch: 2200 \t loss: 0.41270\n",
      "Epoch: 0 \t Batch: 2300 \t loss: 0.40954\n",
      "Epoch: 0 \t Batch: 2400 \t loss: 0.40782\n",
      "Epoch: 0 \t Batch: 2500 \t loss: 0.40800\n",
      "Epoch: 0 \t Batch: 2600 \t loss: 0.40067\n",
      "Epoch: 0 \t Batch: 2700 \t loss: 0.40173\n",
      "Epoch: 0 \t Batch: 2800 \t loss: 0.40137\n",
      "Epoch: 0 \t Batch: 2900 \t loss: 0.40107\n",
      "Epoch: 0 \t Batch: 3000 \t loss: 0.40443\n",
      "Epoch: 0 \t Batch: 3100 \t loss: 0.39501\n",
      "Epoch: 0 \t Batch: 3200 \t loss: 0.38865\n",
      "Epoch: 0 \t Batch: 3300 \t loss: 0.39524\n",
      "Epoch: 0 \t Batch: 3400 \t loss: 0.39660\n",
      "Epoch: 0 \t Batch: 3500 \t loss: 0.38835\n",
      "Epoch: 0 \t Batch: 3600 \t loss: 0.38956\n",
      "Epoch: 0 \t Batch: 3700 \t loss: 0.39003\n",
      "Epoch: 0 \t Batch: 3800 \t loss: 0.38810\n",
      "Epoch: 0 \t Batch: 3900 \t loss: 0.38613\n",
      "Epoch: 0 \t Batch: 4000 \t loss: 0.37989\n",
      "Epoch: 0 \t Batch: 4100 \t loss: 0.38740\n",
      "Epoch: 0 \t Batch: 4200 \t loss: 0.38408\n",
      "Epoch: 0 \t Batch: 4300 \t loss: 0.38119\n",
      "Epoch: 0 \t Batch: 4400 \t loss: 0.37871\n",
      "Epoch: 0 \t Batch: 4500 \t loss: 0.38030\n",
      "Epoch: 0 \t Batch: 4600 \t loss: 0.38120\n",
      "Epoch: 0 \t Batch: 4700 \t loss: 0.37838\n",
      "Epoch: 0 \t Batch: 4800 \t loss: 0.37713\n",
      "Epoch: 0 \t Batch: 4900 \t loss: 0.37473\n",
      "Epoch: 0 \t Batch: 5000 \t loss: 0.37835\n",
      "Epoch: 0 \t Batch: 5100 \t loss: 0.37683\n",
      "Epoch: 0 \t Batch: 5200 \t loss: 0.37430\n",
      "Epoch: 0 \t Batch: 5300 \t loss: 0.37272\n",
      "Epoch: 0 \t Batch: 5400 \t loss: 0.37620\n",
      "Epoch: 0 \t Batch: 5500 \t loss: 0.36959\n",
      "Epoch: 0 \t Batch: 5600 \t loss: 0.37197\n",
      "Epoch: 0 \t Batch: 5700 \t loss: 0.37289\n",
      "Epoch: 0 \t Batch: 5800 \t loss: 0.36703\n",
      "Epoch: 0 \t Batch: 5900 \t loss: 0.37308\n",
      "Epoch: 0 \t Batch: 6000 \t loss: 0.36832\n",
      "Epoch: 0 \t Batch: 6100 \t loss: 0.37098\n",
      "Epoch: 0 \t Batch: 6200 \t loss: 0.37088\n",
      "Epoch: 0 \t Batch: 6300 \t loss: 0.37107\n",
      "Epoch: 0 \t Batch: 6400 \t loss: 0.37154\n",
      "Epoch: 0 \t Batch: 6500 \t loss: 0.36962\n",
      "Epoch: 0 \t Batch: 6600 \t loss: 0.37049\n",
      "Epoch: 0 \t Batch: 6700 \t loss: 0.36747\n",
      "Epoch: 0 \t Batch: 6800 \t loss: 0.36734\n",
      "Epoch: 0 \t Batch: 6900 \t loss: 0.36618\n",
      "Epoch: 0 \t Batch: 7000 \t loss: 0.36107\n",
      "Epoch: 0 \t Batch: 7100 \t loss: 0.36683\n",
      "Epoch: 0 \t Batch: 7200 \t loss: 0.36579\n",
      "Epoch: 0 \t Batch: 7300 \t loss: 0.36254\n",
      "Epoch: 0 \t Batch: 7400 \t loss: 0.36163\n",
      "Epoch: 0 \t Batch: 7500 \t loss: 0.36214\n",
      "Epoch: 0 \t Batch: 7600 \t loss: 0.36645\n",
      "Epoch: 0 \t Batch: 7700 \t loss: 0.37014\n",
      "Epoch: 0 \t Batch: 7800 \t loss: 0.35986\n",
      "Epoch: 0 \t Batch: 7900 \t loss: 0.36282\n",
      "Epoch: 0 \t Batch: 8000 \t loss: 0.36179\n",
      "Epoch: 0 \t Batch: 8100 \t loss: 0.36089\n",
      "Epoch: 0 \t Batch: 8200 \t loss: 0.35897\n",
      "Epoch: 0 \t Batch: 8300 \t loss: 0.36443\n",
      "Epoch: 0 \t Batch: 8400 \t loss: 0.35750\n",
      "Epoch: 0 \t Batch: 8500 \t loss: 0.35135\n",
      "Epoch: 0 \t Batch: 8600 \t loss: 0.35539\n",
      "Epoch: 0 \t Batch: 8700 \t loss: 0.36124\n",
      "Epoch: 0 \t Batch: 8800 \t loss: 0.36025\n",
      "Epoch: 0 \t Batch: 8900 \t loss: 0.35526\n",
      "Epoch: 0 \t Batch: 9000 \t loss: 0.35887\n",
      "Epoch: 0 \t Batch: 9100 \t loss: 0.35757\n",
      "Epoch: 0 \t Batch: 9200 \t loss: 0.35256\n",
      "Epoch: 0 \t Batch: 9300 \t loss: 0.35439\n",
      "Epoch: 0 \t Batch: 9400 \t loss: 0.35710\n",
      "Epoch: 0 \t Batch: 9500 \t loss: 0.36112\n",
      "Epoch: 0 \t Batch: 9600 \t loss: 0.35548\n",
      "Epoch: 0 \t Batch: 9700 \t loss: 0.35364\n",
      "Epoch: 0 \t Batch: 9800 \t loss: 0.35390\n",
      "Epoch: 0 \t Batch: 9900 \t loss: 0.36203\n",
      "Epoch: 0 \t Batch: 10000 \t loss: 0.35719\n",
      "Epoch: 0 \t Batch: 10100 \t loss: 0.35718\n",
      "Epoch: 0 \t Batch: 10200 \t loss: 0.34947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vilab/anaconda3/envs/bia/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:802: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 10300 \t loss: 0.35337\n",
      "Epoch: 0 \t Batch: 10400 \t loss: 0.35595\n",
      "Epoch: 0 \t Batch: 10500 \t loss: 0.35417\n",
      "Epoch: 0 \t Batch: 10600 \t loss: 0.35476\n",
      "Epoch: 0 \t Batch: 10700 \t loss: 0.35218\n",
      "Epoch: 0 \t Batch: 10800 \t loss: 0.35161\n",
      "Epoch: 0 \t Batch: 10900 \t loss: 0.35069\n",
      "Epoch: 0 \t Batch: 11000 \t loss: 0.35488\n",
      "Epoch: 0 \t Batch: 11100 \t loss: 0.35584\n",
      "Epoch: 0 \t Batch: 11200 \t loss: 0.35312\n",
      "Epoch: 0 \t Batch: 11300 \t loss: 0.35473\n",
      "Epoch: 0 \t Batch: 11400 \t loss: 0.35106\n",
      "Epoch: 0 \t Batch: 11500 \t loss: 0.34771\n",
      "Epoch: 0 \t Batch: 11600 \t loss: 0.35204\n",
      "Epoch: 0 \t Batch: 11700 \t loss: 0.34776\n",
      "Epoch: 0 \t Batch: 11800 \t loss: 0.35062\n",
      "Epoch: 0 \t Batch: 11900 \t loss: 0.35733\n",
      "Epoch: 0 \t Batch: 12000 \t loss: 0.35202\n",
      "Epoch: 0 \t Batch: 12100 \t loss: 0.34997\n",
      "Epoch: 0 \t Batch: 12200 \t loss: 0.34999\n",
      "Epoch: 0 \t Batch: 12300 \t loss: 0.34672\n",
      "Epoch: 0 \t Batch: 12400 \t loss: 0.35303\n",
      "Epoch: 0 \t Batch: 12500 \t loss: 0.34526\n",
      "Epoch: 0 \t Batch: 12600 \t loss: 0.35857\n",
      "Epoch: 0 \t Batch: 12700 \t loss: 0.34768\n",
      "Epoch: 0 \t Batch: 12800 \t loss: 0.34770\n",
      "Epoch: 0 \t Batch: 12900 \t loss: 0.35045\n",
      "Epoch: 0 \t Batch: 13000 \t loss: 0.34986\n",
      "Epoch: 0 \t Batch: 13100 \t loss: 0.34829\n",
      "Epoch: 0 \t Batch: 13200 \t loss: 0.34403\n",
      "Epoch: 0 \t Batch: 13300 \t loss: 0.34482\n",
      "Epoch: 0 \t Batch: 13400 \t loss: 0.35177\n",
      "Epoch: 0 \t Batch: 13500 \t loss: 0.35136\n",
      "Epoch: 0 \t Batch: 13600 \t loss: 0.35189\n",
      "Epoch: 0 \t Batch: 13700 \t loss: 0.34412\n",
      "Epoch: 0 \t Batch: 13800 \t loss: 0.34703\n",
      "Epoch: 0 \t Batch: 13900 \t loss: 0.34429\n",
      "Epoch: 0 \t Batch: 14000 \t loss: 0.34572\n",
      "Epoch: 0 \t Batch: 14100 \t loss: 0.34459\n",
      "Epoch: 0 \t Batch: 14200 \t loss: 0.34787\n",
      "Epoch: 0 \t Batch: 14300 \t loss: 0.34181\n",
      "Epoch: 0 \t Batch: 14400 \t loss: 0.34365\n",
      "Epoch: 0 \t Batch: 14500 \t loss: 0.35076\n",
      "Epoch: 0 \t Batch: 14600 \t loss: 0.34263\n",
      "Epoch: 0 \t Batch: 14700 \t loss: 0.34672\n",
      "Epoch: 0 \t Batch: 14800 \t loss: 0.34345\n",
      "Epoch: 0 \t Batch: 14900 \t loss: 0.33633\n",
      "Epoch: 0 \t Batch: 15000 \t loss: 0.34050\n",
      "Epoch: 0 \t Batch: 15100 \t loss: 0.34457\n",
      "Epoch: 0 \t Batch: 15200 \t loss: 0.34318\n",
      "Epoch: 0 \t Batch: 15300 \t loss: 0.34240\n",
      "Epoch: 0 \t Batch: 15400 \t loss: 0.33869\n",
      "Epoch: 0 \t Batch: 15500 \t loss: 0.34721\n",
      "Epoch: 0 \t Batch: 15600 \t loss: 0.33879\n",
      "Epoch: 0 \t Batch: 15700 \t loss: 0.34753\n",
      "Epoch: 0 \t Batch: 15800 \t loss: 0.34297\n",
      "Epoch: 0 \t Batch: 15900 \t loss: 0.34171\n",
      "Epoch: 0 \t Batch: 16000 \t loss: 0.34154\n",
      "Epoch: 0 \t Batch: 16100 \t loss: 0.34134\n",
      "Epoch: 0 \t Batch: 16200 \t loss: 0.34490\n",
      "Epoch: 0 \t Batch: 16300 \t loss: 0.34143\n",
      "Epoch: 0 \t Batch: 16400 \t loss: 0.34503\n",
      "Epoch: 0 \t Batch: 16500 \t loss: 0.34302\n",
      "Epoch: 0 \t Batch: 16600 \t loss: 0.33995\n",
      "Epoch: 0 \t Batch: 16700 \t loss: 0.33688\n",
      "Epoch: 0 \t Batch: 16800 \t loss: 0.34415\n",
      "Epoch: 0 \t Batch: 16900 \t loss: 0.34852\n",
      "Epoch: 0 \t Batch: 17000 \t loss: 0.33972\n",
      "Epoch: 0 \t Batch: 17100 \t loss: 0.34072\n",
      "Epoch: 0 \t Batch: 17200 \t loss: 0.33651\n",
      "Epoch: 0 \t Batch: 17300 \t loss: 0.34652\n",
      "Epoch: 0 \t Batch: 17400 \t loss: 0.34412\n",
      "Epoch: 0 \t Batch: 17500 \t loss: 0.34416\n",
      "Epoch: 0 \t Batch: 17600 \t loss: 0.34679\n",
      "Epoch: 0 \t Batch: 17700 \t loss: 0.34126\n",
      "Epoch: 0 \t Batch: 17800 \t loss: 0.33867\n",
      "Epoch: 0 \t Batch: 17900 \t loss: 0.34104\n",
      "Epoch: 0 \t Batch: 18000 \t loss: 0.34057\n",
      "Epoch: 0 \t Batch: 18100 \t loss: 0.33990\n",
      "Epoch: 0 \t Batch: 18200 \t loss: 0.34032\n",
      "Epoch: 0 \t Batch: 18300 \t loss: 0.33765\n",
      "Epoch: 0 \t Batch: 18400 \t loss: 0.34181\n",
      "Epoch: 0 \t Batch: 18500 \t loss: 0.34011\n",
      "Epoch: 0 \t Batch: 18600 \t loss: 0.33764\n",
      "Epoch: 0 \t Batch: 18700 \t loss: 0.33470\n",
      "Epoch: 0 \t Batch: 18800 \t loss: 0.33535\n",
      "Epoch: 0 \t Batch: 18900 \t loss: 0.34182\n",
      "Epoch: 0 \t Batch: 19000 \t loss: 0.33962\n",
      "Epoch: 0 \t Batch: 19100 \t loss: 0.33844\n",
      "Epoch: 0 \t Batch: 19200 \t loss: 0.33724\n",
      "Epoch: 0 \t Batch: 19300 \t loss: 0.33443\n",
      "Epoch: 0 \t Batch: 19400 \t loss: 0.33411\n",
      "Epoch: 0 \t Batch: 19500 \t loss: 0.33996\n",
      "Epoch: 0 \t Batch: 19600 \t loss: 0.33550\n",
      "Epoch: 0 \t Batch: 19700 \t loss: 0.33726\n",
      "Epoch: 0 \t Batch: 19800 \t loss: 0.33876\n",
      "Epoch: 0 \t Batch: 19900 \t loss: 0.33574\n",
      "Epoch: 0 \t Batch: 20000 \t loss: 0.34056\n",
      "Epoch: 0 \t Batch: 20100 \t loss: 0.34183\n",
      "Epoch: 0 \t Batch: 20200 \t loss: 0.33723\n",
      "Epoch: 0 \t Batch: 20300 \t loss: 0.33543\n",
      "Epoch: 0 \t Batch: 20400 \t loss: 0.33397\n",
      "Epoch: 0 \t Batch: 20500 \t loss: 0.33188\n",
      "Epoch: 0 \t Batch: 20600 \t loss: 0.33446\n",
      "Epoch: 0 \t Batch: 20700 \t loss: 0.34136\n",
      "Epoch: 0 \t Batch: 20800 \t loss: 0.33660\n",
      "Epoch: 0 \t Batch: 20900 \t loss: 0.33787\n",
      "Epoch: 0 \t Batch: 21000 \t loss: 0.33828\n",
      "Epoch: 0 \t Batch: 21100 \t loss: 0.33420\n",
      "Epoch: 0 \t Batch: 21200 \t loss: 0.33572\n",
      "Epoch: 0 \t Batch: 21300 \t loss: 0.33929\n",
      "Epoch: 0 \t Batch: 21400 \t loss: 0.33776\n",
      "Epoch: 0 \t Batch: 21500 \t loss: 0.33892\n",
      "Epoch: 0 \t Batch: 21600 \t loss: 0.33750\n",
      "Epoch: 0 \t Batch: 21700 \t loss: 0.33042\n",
      "Epoch: 0 \t Batch: 21800 \t loss: 0.33501\n",
      "Epoch: 0 \t Batch: 21900 \t loss: 0.33101\n",
      "Epoch: 0 \t Batch: 22000 \t loss: 0.33436\n",
      "Epoch: 0 \t Batch: 22100 \t loss: 0.33420\n",
      "Epoch: 0 \t Batch: 22200 \t loss: 0.33481\n",
      "Epoch: 0 \t Batch: 22300 \t loss: 0.33385\n",
      "Epoch: 0 \t Batch: 22400 \t loss: 0.33190\n",
      "Epoch: 0 \t Batch: 22500 \t loss: 0.32768\n",
      "Epoch: 0 \t Batch: 22600 \t loss: 0.33400\n",
      "Epoch: 0 \t Batch: 22700 \t loss: 0.33772\n",
      "Epoch: 0 \t Batch: 22800 \t loss: 0.32917\n",
      "Epoch: 0 \t Batch: 22900 \t loss: 0.33261\n",
      "Epoch: 0 \t Batch: 23000 \t loss: 0.33690\n",
      "Epoch: 0 \t Batch: 23100 \t loss: 0.33443\n",
      "Epoch: 0 \t Batch: 23200 \t loss: 0.33326\n",
      "Epoch: 0 \t Batch: 23300 \t loss: 0.33952\n",
      "Epoch: 0 \t Batch: 23400 \t loss: 0.33541\n",
      "Epoch: 0 \t Batch: 23500 \t loss: 0.33016\n",
      "Epoch: 0 \t Batch: 23600 \t loss: 0.33098\n",
      "Epoch: 0 \t Batch: 23700 \t loss: 0.33523\n",
      "Epoch: 0 \t Batch: 23800 \t loss: 0.33116\n",
      "Epoch: 0 \t Batch: 23900 \t loss: 0.33028\n",
      "Epoch: 0 \t Batch: 24000 \t loss: 0.33293\n",
      "Epoch: 0 \t Batch: 24100 \t loss: 0.32824\n",
      "Epoch: 0 \t Batch: 24200 \t loss: 0.33524\n",
      "Epoch: 0 \t Batch: 24300 \t loss: 0.33181\n",
      "Epoch: 0 \t Batch: 24400 \t loss: 0.33112\n",
      "Epoch: 0 \t Batch: 24500 \t loss: 0.33290\n",
      "Epoch: 0 \t Batch: 24600 \t loss: 0.33388\n",
      "Epoch: 0 \t Batch: 24700 \t loss: 0.33571\n",
      "Epoch: 0 \t Batch: 24800 \t loss: 0.32681\n",
      "Epoch: 0 \t Batch: 24900 \t loss: 0.32991\n",
      "Epoch: 0 \t Batch: 25000 \t loss: 0.33348\n",
      "Epoch: 0 \t Batch: 25100 \t loss: 0.33143\n",
      "Epoch: 0 \t Batch: 25200 \t loss: 0.33364\n",
      "Epoch: 0 \t Batch: 25300 \t loss: 0.32795\n",
      "Epoch: 0 \t Batch: 25400 \t loss: 0.33379\n",
      "Epoch: 0 \t Batch: 25500 \t loss: 0.33405\n",
      "Epoch: 0 \t Batch: 25600 \t loss: 0.33316\n",
      "Epoch: 0 \t Batch: 25700 \t loss: 0.33026\n",
      "Epoch: 0 \t Batch: 25800 \t loss: 0.32772\n",
      "Epoch: 0 \t Batch: 25900 \t loss: 0.33022\n",
      "Epoch: 0 \t Batch: 26000 \t loss: 0.33036\n",
      "Epoch: 0 \t Batch: 26100 \t loss: 0.33183\n",
      "Epoch: 0 \t Batch: 26200 \t loss: 0.33435\n",
      "Epoch: 0 \t Batch: 26300 \t loss: 0.32788\n",
      "Epoch: 0 \t Batch: 26400 \t loss: 0.33092\n",
      "Epoch: 0 \t Batch: 26500 \t loss: 0.33587\n",
      "Epoch: 0 \t Batch: 26600 \t loss: 0.32781\n",
      "Epoch: 0 \t Batch: 26700 \t loss: 0.33325\n",
      "Epoch: 0 \t Batch: 26800 \t loss: 0.33147\n",
      "Epoch: 0 \t Batch: 26900 \t loss: 0.33504\n",
      "Epoch: 0 \t Batch: 27000 \t loss: 0.33092\n",
      "Epoch: 0 \t Batch: 27100 \t loss: 0.32794\n",
      "Epoch: 0 \t Batch: 27200 \t loss: 0.33504\n",
      "Epoch: 0 \t Batch: 27300 \t loss: 0.32554\n",
      "Epoch: 0 \t Batch: 27400 \t loss: 0.32862\n",
      "Epoch: 0 \t Batch: 27500 \t loss: 0.33411\n",
      "Epoch: 0 \t Batch: 27600 \t loss: 0.33122\n",
      "Epoch: 0 \t Batch: 27700 \t loss: 0.32967\n",
      "Epoch: 0 \t Batch: 27800 \t loss: 0.32757\n",
      "Epoch: 0 \t Batch: 27900 \t loss: 0.33280\n",
      "Epoch: 0 \t Batch: 28000 \t loss: 0.32892\n",
      "Epoch: 0 \t Batch: 28100 \t loss: 0.32893\n",
      "Epoch: 0 \t Batch: 28200 \t loss: 0.32551\n",
      "Epoch: 0 \t Batch: 28300 \t loss: 0.32641\n",
      "Epoch: 0 \t Batch: 28400 \t loss: 0.33193\n",
      "Epoch: 0 \t Batch: 28500 \t loss: 0.33094\n",
      "Epoch: 0 \t Batch: 28600 \t loss: 0.33419\n",
      "Epoch: 0 \t Batch: 28700 \t loss: 0.33407\n",
      "Epoch: 0 \t Batch: 28800 \t loss: 0.32754\n",
      "Epoch: 0 \t Batch: 28900 \t loss: 0.32634\n",
      "Epoch: 0 \t Batch: 29000 \t loss: 0.33079\n",
      "Epoch: 0 \t Batch: 29100 \t loss: 0.33293\n",
      "Epoch: 0 \t Batch: 29200 \t loss: 0.33082\n",
      "Epoch: 0 \t Batch: 29300 \t loss: 0.33050\n",
      "Epoch: 0 \t Batch: 29400 \t loss: 0.33025\n",
      "Epoch: 0 \t Batch: 29500 \t loss: 0.32327\n",
      "Epoch: 0 \t Batch: 29600 \t loss: 0.32844\n",
      "Epoch: 0 \t Batch: 29700 \t loss: 0.32943\n",
      "Epoch: 0 \t Batch: 29800 \t loss: 0.32737\n",
      "Epoch: 0 \t Batch: 29900 \t loss: 0.32816\n",
      "Epoch: 0 \t Batch: 30000 \t loss: 0.32799\n",
      "Epoch: 0 \t Batch: 30100 \t loss: 0.32407\n",
      "Epoch: 0 \t Batch: 30200 \t loss: 0.33021\n",
      "Epoch: 0 \t Batch: 30300 \t loss: 0.33281\n",
      "Epoch: 0 \t Batch: 30400 \t loss: 0.33103\n",
      "Epoch: 0 \t Batch: 30500 \t loss: 0.32964\n",
      "Epoch: 0 \t Batch: 30600 \t loss: 0.32588\n",
      "Epoch: 0 \t Batch: 30700 \t loss: 0.32972\n",
      "Epoch: 0 \t Batch: 30800 \t loss: 0.32904\n",
      "Epoch: 0 \t Batch: 30900 \t loss: 0.32640\n",
      "Epoch: 0 \t Batch: 31000 \t loss: 0.33180\n",
      "Epoch: 0 \t Batch: 31100 \t loss: 0.32988\n",
      "Epoch: 0 \t Batch: 31200 \t loss: 0.32737\n",
      "Epoch: 0 \t Batch: 31300 \t loss: 0.32832\n",
      "Epoch: 0 \t Batch: 31400 \t loss: 0.32870\n",
      "Epoch: 0 \t Batch: 31500 \t loss: 0.32714\n",
      "Epoch: 0 \t Batch: 31600 \t loss: 0.33230\n",
      "Epoch: 0 \t Batch: 31700 \t loss: 0.32339\n",
      "Epoch: 0 \t Batch: 31800 \t loss: 0.32491\n",
      "Epoch: 0 \t Batch: 31900 \t loss: 0.32553\n",
      "Epoch: 0 \t Batch: 32000 \t loss: 0.32841\n",
      "Epoch: 0 \t Batch: 32100 \t loss: 0.32359\n",
      "Epoch: 0 \t Batch: 32200 \t loss: 0.32014\n",
      "Epoch: 0 \t Batch: 32300 \t loss: 0.33031\n",
      "Epoch: 0 \t Batch: 32400 \t loss: 0.32547\n",
      "Epoch: 0 \t Batch: 32500 \t loss: 0.32507\n",
      "Epoch: 0 \t Batch: 32600 \t loss: 0.32708\n",
      "Epoch: 0 \t Batch: 32700 \t loss: 0.32841\n",
      "Epoch: 0 \t Batch: 32800 \t loss: 0.32624\n",
      "Epoch: 0 \t Batch: 32900 \t loss: 0.32376\n",
      "Epoch: 0 \t Batch: 33000 \t loss: 0.32957\n",
      "Epoch: 0 \t Batch: 33100 \t loss: 0.32626\n",
      "Epoch: 0 \t Batch: 33200 \t loss: 0.32904\n",
      "Epoch: 0 \t Batch: 33300 \t loss: 0.32993\n",
      "Epoch: 0 \t Batch: 33400 \t loss: 0.32350\n",
      "Epoch: 0 \t Batch: 33500 \t loss: 0.32920\n",
      "Epoch: 0 \t Batch: 33600 \t loss: 0.32080\n",
      "Epoch: 0 \t Batch: 33700 \t loss: 0.32713\n",
      "Epoch: 0 \t Batch: 33800 \t loss: 0.32421\n",
      "Epoch: 0 \t Batch: 33900 \t loss: 0.32544\n",
      "Epoch: 0 \t Batch: 34000 \t loss: 0.32654\n",
      "Epoch: 0 \t Batch: 34100 \t loss: 0.33161\n",
      "Epoch: 0 \t Batch: 34200 \t loss: 0.32231\n",
      "Epoch: 0 \t Batch: 34300 \t loss: 0.32578\n",
      "Epoch: 0 \t Batch: 34400 \t loss: 0.32545\n",
      "Epoch: 0 \t Batch: 34500 \t loss: 0.32514\n",
      "Epoch: 0 \t Batch: 34600 \t loss: 0.32769\n",
      "Epoch: 0 \t Batch: 34700 \t loss: 0.32428\n",
      "Epoch: 0 \t Batch: 34800 \t loss: 0.32690\n",
      "Epoch: 0 \t Batch: 34900 \t loss: 0.32566\n",
      "Epoch: 0 \t Batch: 35000 \t loss: 0.32624\n",
      "Epoch: 0 \t Batch: 35100 \t loss: 0.32554\n",
      "Epoch: 0 \t Batch: 35200 \t loss: 0.32713\n",
      "Epoch: 0 \t Batch: 35300 \t loss: 0.32826\n",
      "Epoch: 0 \t Batch: 35400 \t loss: 0.32348\n",
      "Epoch: 0 \t Batch: 35500 \t loss: 0.32487\n",
      "Epoch: 0 \t Batch: 35600 \t loss: 0.32394\n",
      "Epoch: 0 \t Batch: 35700 \t loss: 0.32828\n",
      "Epoch: 0 \t Batch: 35800 \t loss: 0.32565\n",
      "Epoch: 0 \t Batch: 35900 \t loss: 0.32224\n",
      "Epoch: 0 \t Batch: 36000 \t loss: 0.32519\n",
      "Epoch: 0 \t Batch: 36100 \t loss: 0.32323\n",
      "Epoch: 0 \t Batch: 36200 \t loss: 0.32780\n",
      "Epoch: 0 \t Batch: 36300 \t loss: 0.32554\n",
      "Epoch: 0 \t Batch: 36400 \t loss: 0.32372\n",
      "Epoch: 0 \t Batch: 36500 \t loss: 0.32278\n",
      "Epoch: 0 \t Batch: 36600 \t loss: 0.32520\n",
      "Epoch: 0 \t Batch: 36700 \t loss: 0.32115\n",
      "Epoch: 0 \t Batch: 36800 \t loss: 0.32634\n",
      "Epoch: 0 \t Batch: 36900 \t loss: 0.32771\n",
      "Epoch: 0 \t Batch: 37000 \t loss: 0.32358\n",
      "Epoch: 0 \t Batch: 37100 \t loss: 0.32656\n",
      "Epoch: 0 \t Batch: 37200 \t loss: 0.32028\n",
      "Epoch: 0 \t Batch: 37300 \t loss: 0.32306\n",
      "Epoch: 0 \t Batch: 37400 \t loss: 0.32321\n",
      "Epoch: 0 \t Batch: 37500 \t loss: 0.32568\n",
      "Epoch: 0 \t Batch: 37600 \t loss: 0.32341\n",
      "Epoch: 0 \t Batch: 37700 \t loss: 0.32491\n",
      "Epoch: 0 \t Batch: 37800 \t loss: 0.32069\n",
      "Epoch: 0 \t Batch: 37900 \t loss: 0.32317\n",
      "Epoch: 0 \t Batch: 38000 \t loss: 0.32078\n",
      "Epoch: 0 \t Batch: 38100 \t loss: 0.32462\n",
      "Epoch: 0 \t Batch: 38200 \t loss: 0.31995\n",
      "Epoch: 0 \t Batch: 38300 \t loss: 0.32675\n",
      "Epoch: 0 \t Batch: 38400 \t loss: 0.31959\n",
      "Epoch: 0 \t Batch: 38500 \t loss: 0.32372\n",
      "Epoch: 0 \t Batch: 38600 \t loss: 0.32376\n",
      "Epoch: 0 \t Batch: 38700 \t loss: 0.32451\n",
      "Epoch: 0 \t Batch: 38800 \t loss: 0.32229\n",
      "Epoch: 0 \t Batch: 38900 \t loss: 0.32515\n",
      "Epoch: 0 \t Batch: 39000 \t loss: 0.32468\n",
      "Epoch: 0 \t Batch: 39100 \t loss: 0.32091\n",
      "Epoch: 0 \t Batch: 39200 \t loss: 0.32306\n",
      "Epoch: 0 \t Batch: 39300 \t loss: 0.32543\n",
      "Epoch: 0 \t Batch: 39400 \t loss: 0.32279\n",
      "Epoch: 0 \t Batch: 39500 \t loss: 0.32416\n",
      "Epoch: 0 \t Batch: 39600 \t loss: 0.32613\n",
      "Epoch: 0 \t Batch: 39700 \t loss: 0.32635\n",
      "Epoch: 0 \t Batch: 39800 \t loss: 0.32273\n",
      "Epoch: 0 \t Batch: 39900 \t loss: 0.31893\n",
      "Epoch: 0 \t Batch: 40000 \t loss: 0.32231\n",
      "Epoch: 0 \t Batch: 40100 \t loss: 0.32448\n",
      "Epoch: 0 \t Batch: 40200 \t loss: 0.32365\n",
      "Epoch: 0 \t Batch: 40300 \t loss: 0.32097\n",
      "Epoch: 0 \t Batch: 40400 \t loss: 0.32791\n",
      "Epoch: 0 \t Batch: 40500 \t loss: 0.32269\n",
      "Epoch: 0 \t Batch: 40600 \t loss: 0.32373\n",
      "Epoch: 0 \t Batch: 40700 \t loss: 0.31863\n",
      "Epoch: 0 \t Batch: 40800 \t loss: 0.32061\n",
      "Epoch: 0 \t Batch: 40900 \t loss: 0.32071\n",
      "Epoch: 0 \t Batch: 41000 \t loss: 0.32171\n",
      "Epoch: 0 \t Batch: 41100 \t loss: 0.32230\n",
      "Epoch: 0 \t Batch: 41200 \t loss: 0.32329\n",
      "Epoch: 0 \t Batch: 41300 \t loss: 0.31613\n",
      "Epoch: 0 \t Batch: 41400 \t loss: 0.31874\n",
      "Epoch: 0 \t Batch: 41500 \t loss: 0.32285\n",
      "Epoch: 0 \t Batch: 41600 \t loss: 0.32781\n",
      "Epoch: 0 \t Batch: 41700 \t loss: 0.31846\n",
      "Epoch: 0 \t Batch: 41800 \t loss: 0.31898\n",
      "Epoch: 0 \t Batch: 41900 \t loss: 0.31548\n",
      "Epoch: 0 \t Batch: 42000 \t loss: 0.32255\n",
      "Epoch: 0 \t Batch: 42100 \t loss: 0.32133\n",
      "Epoch: 0 \t Batch: 42200 \t loss: 0.32341\n",
      "Epoch: 0 \t Batch: 42300 \t loss: 0.32209\n",
      "Epoch: 0 \t Batch: 42400 \t loss: 0.32659\n",
      "Epoch: 0 \t Batch: 42500 \t loss: 0.31762\n",
      "Epoch: 0 \t Batch: 42600 \t loss: 0.32171\n",
      "Epoch: 0 \t Batch: 42700 \t loss: 0.32041\n",
      "Epoch: 0 \t Batch: 42800 \t loss: 0.32142\n",
      "Epoch: 0 \t Batch: 42900 \t loss: 0.31847\n",
      "Epoch: 0 \t Batch: 43000 \t loss: 0.32257\n",
      "Epoch: 0 \t Batch: 43100 \t loss: 0.31906\n",
      "Epoch: 0 \t Batch: 43200 \t loss: 0.32235\n",
      "Epoch: 0 \t Batch: 43300 \t loss: 0.31730\n",
      "Epoch: 0 \t Batch: 43400 \t loss: 0.32369\n",
      "Epoch: 0 \t Batch: 43500 \t loss: 0.32174\n",
      "Epoch: 0 \t Batch: 43600 \t loss: 0.32348\n",
      "Epoch: 0 \t Batch: 43700 \t loss: 0.31945\n",
      "Epoch: 0 \t Batch: 43800 \t loss: 0.31858\n",
      "Epoch: 0 \t Batch: 43900 \t loss: 0.32454\n",
      "Epoch: 0 \t Batch: 44000 \t loss: 0.31616\n",
      "Epoch: 0 \t Batch: 44100 \t loss: 0.32016\n",
      "Epoch: 0 \t Batch: 44200 \t loss: 0.32417\n",
      "Epoch: 0 \t Batch: 44300 \t loss: 0.31856\n",
      "Epoch: 0 \t Batch: 44400 \t loss: 0.31825\n",
      "Epoch: 0 \t Batch: 44500 \t loss: 0.31938\n",
      "Epoch: 0 \t Batch: 44600 \t loss: 0.31605\n",
      "Epoch: 0 \t Batch: 44700 \t loss: 0.32125\n",
      "Epoch: 0 \t Batch: 44800 \t loss: 0.32232\n",
      "Epoch: 0 \t Batch: 44900 \t loss: 0.31258\n",
      "Epoch: 0 \t Batch: 45000 \t loss: 0.31761\n",
      "Epoch: 0 \t Batch: 45100 \t loss: 0.31962\n",
      "Epoch: 0 \t Batch: 45200 \t loss: 0.31506\n",
      "Epoch: 0 \t Batch: 45300 \t loss: 0.31735\n",
      "Epoch: 0 \t Batch: 45400 \t loss: 0.32405\n",
      "Epoch: 0 \t Batch: 45500 \t loss: 0.31875\n",
      "Epoch: 0 \t Batch: 45600 \t loss: 0.32009\n",
      "Epoch: 0 \t Batch: 45700 \t loss: 0.31892\n",
      "Epoch: 0 \t Batch: 45800 \t loss: 0.31976\n",
      "Epoch: 0 \t Batch: 45900 \t loss: 0.31930\n",
      "Epoch: 0 \t Batch: 46000 \t loss: 0.31679\n",
      "Epoch: 0 \t Batch: 46100 \t loss: 0.32262\n",
      "Epoch: 0 \t Batch: 46200 \t loss: 0.31815\n",
      "Epoch: 0 \t Batch: 46300 \t loss: 0.32242\n",
      "Epoch: 0 \t Batch: 46400 \t loss: 0.31877\n",
      "Epoch: 0 \t Batch: 46500 \t loss: 0.32154\n",
      "Epoch: 0 \t Batch: 46600 \t loss: 0.32154\n",
      "Epoch: 0 \t Batch: 46700 \t loss: 0.32229\n",
      "Epoch: 0 \t Batch: 46800 \t loss: 0.32347\n",
      "Epoch: 0 \t Batch: 46900 \t loss: 0.32266\n",
      "Epoch: 0 \t Batch: 47000 \t loss: 0.31851\n",
      "Epoch: 0 \t Batch: 47100 \t loss: 0.31994\n",
      "Epoch: 0 \t Batch: 47200 \t loss: 0.31755\n",
      "Epoch: 0 \t Batch: 47300 \t loss: 0.31930\n",
      "Epoch: 0 \t Batch: 47400 \t loss: 0.31893\n",
      "Epoch: 0 \t Batch: 47500 \t loss: 0.31545\n",
      "Epoch: 0 \t Batch: 47600 \t loss: 0.31929\n",
      "Epoch: 0 \t Batch: 47700 \t loss: 0.31668\n",
      "Epoch: 0 \t Batch: 47800 \t loss: 0.31558\n",
      "Epoch: 0 \t Batch: 47900 \t loss: 0.31515\n",
      "Epoch: 0 \t Batch: 48000 \t loss: 0.31692\n",
      "Epoch: 0 \t Batch: 48100 \t loss: 0.31847\n",
      "Epoch: 0 \t Batch: 48200 \t loss: 0.31662\n",
      "Epoch: 0 \t Batch: 48300 \t loss: 0.31570\n",
      "Epoch: 0 \t Batch: 48400 \t loss: 0.31820\n",
      "Epoch: 0 \t Batch: 48500 \t loss: 0.31945\n",
      "Epoch: 0 \t Batch: 48600 \t loss: 0.32249\n",
      "Epoch: 0 \t Batch: 48700 \t loss: 0.31649\n",
      "Epoch: 0 \t Batch: 48800 \t loss: 0.31840\n",
      "Epoch: 0 \t Batch: 48900 \t loss: 0.32186\n",
      "Epoch: 0 \t Batch: 49000 \t loss: 0.31620\n",
      "Epoch: 0 \t Batch: 49100 \t loss: 0.32000\n",
      "Epoch: 0 \t Batch: 49200 \t loss: 0.31936\n",
      "Epoch: 0 \t Batch: 49300 \t loss: 0.31351\n",
      "Epoch: 0 \t Batch: 49400 \t loss: 0.31815\n",
      "Epoch: 0 \t Batch: 49500 \t loss: 0.32050\n",
      "Epoch: 0 \t Batch: 49600 \t loss: 0.31488\n",
      "Epoch: 0 \t Batch: 49700 \t loss: 0.31834\n",
      "Epoch: 0 \t Batch: 49800 \t loss: 0.31146\n",
      "Epoch: 0 \t Batch: 49900 \t loss: 0.31682\n",
      "Epoch: 0 \t Batch: 50000 \t loss: 0.32128\n",
      "Epoch: 0 \t Batch: 50100 \t loss: 0.31822\n",
      "Epoch: 0 \t Batch: 50200 \t loss: 0.32125\n",
      "Epoch: 0 \t Batch: 50300 \t loss: 0.31880\n",
      "Epoch: 0 \t Batch: 50400 \t loss: 0.31731\n",
      "Epoch: 0 \t Batch: 50500 \t loss: 0.31973\n",
      "Epoch: 0 \t Batch: 50600 \t loss: 0.31858\n",
      "Epoch: 0 \t Batch: 50700 \t loss: 0.32008\n",
      "Epoch: 0 \t Batch: 50800 \t loss: 0.32064\n",
      "Epoch: 0 \t Batch: 50900 \t loss: 0.31743\n",
      "Epoch: 0 \t Batch: 51000 \t loss: 0.31449\n",
      "Epoch: 0 \t Batch: 51100 \t loss: 0.32249\n",
      "Epoch: 0 \t Batch: 51200 \t loss: 0.31707\n",
      "Epoch: 0 \t Batch: 51300 \t loss: 0.31967\n",
      "Epoch: 0 \t Batch: 51400 \t loss: 0.31092\n",
      "Epoch: 0 \t Batch: 51500 \t loss: 0.32092\n",
      "Epoch: 0 \t Batch: 51600 \t loss: 0.31682\n",
      "Epoch: 0 \t Batch: 51700 \t loss: 0.31529\n",
      "Epoch: 0 \t Batch: 51800 \t loss: 0.31600\n",
      "Epoch: 0 \t Batch: 51900 \t loss: 0.32149\n",
      "Epoch: 0 \t Batch: 52000 \t loss: 0.31691\n",
      "Epoch: 0 \t Batch: 52100 \t loss: 0.31805\n",
      "Epoch: 0 \t Batch: 52200 \t loss: 0.31466\n",
      "Epoch: 0 \t Batch: 52300 \t loss: 0.31789\n",
      "Epoch: 0 \t Batch: 52400 \t loss: 0.31443\n",
      "Epoch: 0 \t Batch: 52500 \t loss: 0.31226\n",
      "Epoch: 0 \t Batch: 52600 \t loss: 0.31730\n",
      "Epoch: 0 \t Batch: 52700 \t loss: 0.31672\n",
      "Epoch: 0 \t Batch: 52800 \t loss: 0.31438\n",
      "Epoch: 0 \t Batch: 52900 \t loss: 0.31838\n",
      "Epoch: 0 \t Batch: 53000 \t loss: 0.31231\n",
      "Epoch: 0 \t Batch: 53100 \t loss: 0.31734\n",
      "Epoch: 0 \t Batch: 53200 \t loss: 0.31881\n",
      "Epoch: 0 \t Batch: 53300 \t loss: 0.31846\n",
      "Epoch: 0 \t Batch: 53400 \t loss: 0.32094\n",
      "Epoch: 0 \t Batch: 53500 \t loss: 0.31697\n",
      "Epoch: 0 \t Batch: 53600 \t loss: 0.31170\n",
      "Epoch: 0 \t Batch: 53700 \t loss: 0.31831\n",
      "Epoch: 0 \t Batch: 53800 \t loss: 0.31780\n",
      "Epoch: 0 \t Batch: 53900 \t loss: 0.32024\n",
      "Epoch: 0 \t Batch: 54000 \t loss: 0.31929\n",
      "Epoch: 0 \t Batch: 54100 \t loss: 0.31992\n",
      "Epoch: 0 \t Batch: 54200 \t loss: 0.32114\n",
      "Epoch: 0 \t Batch: 54300 \t loss: 0.32190\n",
      "Epoch: 0 \t Batch: 54400 \t loss: 0.31175\n",
      "Epoch: 0 \t Batch: 54500 \t loss: 0.31130\n",
      "Epoch: 0 \t Batch: 54600 \t loss: 0.31659\n",
      "Epoch: 0 \t Batch: 54700 \t loss: 0.31513\n",
      "Epoch: 0 \t Batch: 54800 \t loss: 0.31825\n",
      "Epoch: 0 \t Batch: 54900 \t loss: 0.31850\n",
      "Epoch: 0 \t Batch: 55000 \t loss: 0.31403\n",
      "Epoch: 0 \t Batch: 55100 \t loss: 0.32148\n",
      "Epoch: 0 \t Batch: 55200 \t loss: 0.31915\n",
      "Epoch: 0 \t Batch: 55300 \t loss: 0.31550\n",
      "Epoch: 0 \t Batch: 55400 \t loss: 0.31531\n",
      "Epoch: 0 \t Batch: 55500 \t loss: 0.31341\n",
      "Epoch: 0 \t Batch: 55600 \t loss: 0.31590\n",
      "Epoch: 0 \t Batch: 55700 \t loss: 0.31998\n",
      "Epoch: 0 \t Batch: 55800 \t loss: 0.31640\n",
      "Epoch: 0 \t Batch: 55900 \t loss: 0.31392\n",
      "Epoch: 0 \t Batch: 56000 \t loss: 0.31872\n",
      "Epoch: 0 \t Batch: 56100 \t loss: 0.31256\n",
      "Epoch: 0 \t Batch: 56200 \t loss: 0.31750\n",
      "Epoch: 0 \t Batch: 56300 \t loss: 0.31049\n",
      "Epoch: 0 \t Batch: 56400 \t loss: 0.31466\n",
      "Epoch: 0 \t Batch: 56500 \t loss: 0.31536\n",
      "Epoch: 0 \t Batch: 56600 \t loss: 0.31163\n",
      "Epoch: 0 \t Batch: 56700 \t loss: 0.32014\n",
      "Epoch: 0 \t Batch: 56800 \t loss: 0.31106\n",
      "Epoch: 0 \t Batch: 56900 \t loss: 0.31667\n",
      "Epoch: 0 \t Batch: 57000 \t loss: 0.31863\n",
      "Epoch: 0 \t Batch: 57100 \t loss: 0.31467\n",
      "Epoch: 0 \t Batch: 57200 \t loss: 0.31722\n",
      "Epoch: 0 \t Batch: 57300 \t loss: 0.31542\n",
      "Epoch: 0 \t Batch: 57400 \t loss: 0.31526\n",
      "Epoch: 0 \t Batch: 57500 \t loss: 0.31386\n",
      "Epoch: 0 \t Batch: 57600 \t loss: 0.31120\n",
      "Epoch: 0 \t Batch: 57700 \t loss: 0.31730\n",
      "Epoch: 0 \t Batch: 57800 \t loss: 0.31662\n",
      "Epoch: 0 \t Batch: 57900 \t loss: 0.31791\n",
      "Epoch: 0 \t Batch: 58000 \t loss: 0.31384\n",
      "Epoch: 0 \t Batch: 58100 \t loss: 0.31813\n",
      "Epoch: 0 \t Batch: 58200 \t loss: 0.31745\n",
      "Epoch: 0 \t Batch: 58300 \t loss: 0.31828\n",
      "Epoch: 0 \t Batch: 58400 \t loss: 0.31617\n",
      "Epoch: 0 \t Batch: 58500 \t loss: 0.31830\n",
      "Epoch: 0 \t Batch: 58600 \t loss: 0.31414\n",
      "Epoch: 0 \t Batch: 58700 \t loss: 0.31278\n",
      "Epoch: 0 \t Batch: 58800 \t loss: 0.31218\n",
      "Epoch: 0 \t Batch: 58900 \t loss: 0.31630\n",
      "Epoch: 0 \t Batch: 59000 \t loss: 0.31750\n",
      "Epoch: 0 \t Batch: 59100 \t loss: 0.31483\n",
      "Epoch: 0 \t Batch: 59200 \t loss: 0.31186\n",
      "Epoch: 0 \t Batch: 59300 \t loss: 0.31700\n",
      "Epoch: 0 \t Batch: 59400 \t loss: 0.31393\n",
      "Epoch: 0 \t Batch: 59500 \t loss: 0.30981\n",
      "Epoch: 0 \t Batch: 59600 \t loss: 0.31255\n",
      "Epoch: 0 \t Batch: 59700 \t loss: 0.31508\n",
      "Epoch: 0 \t Batch: 59800 \t loss: 0.31644\n",
      "Epoch: 0 \t Batch: 59900 \t loss: 0.31430\n",
      "Epoch: 0 \t Batch: 60000 \t loss: 0.31812\n",
      "Epoch: 0 \t Batch: 60100 \t loss: 0.31916\n",
      "Epoch: 0 \t Batch: 60200 \t loss: 0.31519\n",
      "Epoch: 0 \t Batch: 60300 \t loss: 0.31251\n",
      "Epoch: 0 \t Batch: 60400 \t loss: 0.31327\n",
      "Epoch: 0 \t Batch: 60500 \t loss: 0.31084\n",
      "Epoch: 0 \t Batch: 60600 \t loss: 0.31444\n",
      "Epoch: 0 \t Batch: 60700 \t loss: 0.31249\n",
      "Epoch: 0 \t Batch: 60800 \t loss: 0.31644\n",
      "Epoch: 0 \t Batch: 60900 \t loss: 0.31103\n",
      "Epoch: 0 \t Batch: 61000 \t loss: 0.31621\n",
      "Epoch: 0 \t Batch: 61100 \t loss: 0.31327\n",
      "Epoch: 0 \t Batch: 61200 \t loss: 0.31659\n",
      "Epoch: 0 \t Batch: 61300 \t loss: 0.31658\n",
      "Epoch: 0 \t Batch: 61400 \t loss: 0.31472\n",
      "Epoch: 0 \t Batch: 61500 \t loss: 0.31565\n",
      "Epoch: 0 \t Batch: 61600 \t loss: 0.31351\n",
      "Epoch: 0 \t Batch: 61700 \t loss: 0.31386\n",
      "Epoch: 0 \t Batch: 61800 \t loss: 0.31506\n",
      "Epoch: 0 \t Batch: 61900 \t loss: 0.31761\n",
      "Epoch: 0 \t Batch: 62000 \t loss: 0.30968\n",
      "Epoch: 0 \t Batch: 62100 \t loss: 0.31801\n",
      "Epoch: 0 \t Batch: 62200 \t loss: 0.31076\n",
      "Epoch: 0 \t Batch: 62300 \t loss: 0.31519\n",
      "Epoch: 0 \t Batch: 62400 \t loss: 0.31228\n",
      "Epoch: 0 \t Batch: 62500 \t loss: 0.31049\n",
      "Epoch: 0 \t Batch: 62600 \t loss: 0.31258\n",
      "Epoch: 0 \t Batch: 62700 \t loss: 0.30939\n",
      "Epoch: 0 \t Batch: 62800 \t loss: 0.32346\n",
      "Epoch: 0 \t Batch: 62900 \t loss: 0.31400\n",
      "Epoch: 0 \t Batch: 63000 \t loss: 0.31532\n",
      "Epoch: 0 \t Batch: 63100 \t loss: 0.31042\n",
      "Epoch: 0 \t Batch: 63200 \t loss: 0.31434\n",
      "Epoch: 0 \t Batch: 63300 \t loss: 0.31363\n",
      "Epoch: 0 \t Batch: 63400 \t loss: 0.31873\n",
      "Epoch: 0 \t Batch: 63500 \t loss: 0.30972\n",
      "Epoch: 0 \t Batch: 63600 \t loss: 0.31340\n",
      "Epoch: 0 \t Batch: 63700 \t loss: 0.30892\n",
      "Epoch: 0 \t Batch: 63800 \t loss: 0.31600\n",
      "Epoch: 0 \t Batch: 63900 \t loss: 0.30480\n",
      "Epoch: 0 \t Batch: 64000 \t loss: 0.31206\n",
      "Epoch: 0 \t Batch: 64100 \t loss: 0.30652\n",
      "Epoch: 0 \t Batch: 64200 \t loss: 0.30749\n",
      "Epoch: 0 \t Batch: 64300 \t loss: 0.30618\n",
      "Epoch: 0 \t Batch: 64400 \t loss: 0.30320\n",
      "Epoch: 0 \t Batch: 64500 \t loss: 0.30169\n",
      "Epoch: 0 \t Batch: 64600 \t loss: 0.30009\n",
      "Epoch: 0 \t Batch: 64700 \t loss: 0.30318\n",
      "Epoch: 0 \t Batch: 64800 \t loss: 0.30063\n",
      "Epoch: 0 \t Batch: 64900 \t loss: 0.30177\n",
      "Epoch: 0 \t Batch: 65000 \t loss: 0.29569\n",
      "Epoch: 0 \t Batch: 65100 \t loss: 0.29791\n",
      "Epoch: 0 \t Batch: 65200 \t loss: 0.29824\n",
      "Epoch: 0 \t Batch: 65300 \t loss: 0.30129\n",
      "Epoch: 0 \t Batch: 65400 \t loss: 0.29602\n",
      "Epoch: 0 \t Batch: 65500 \t loss: 0.29928\n",
      "Epoch: 0 \t Batch: 65600 \t loss: 0.29987\n",
      "Epoch: 0 \t Batch: 65700 \t loss: 0.29479\n",
      "Epoch: 0 \t Batch: 65800 \t loss: 0.29515\n",
      "Epoch: 0 \t Batch: 65900 \t loss: 0.29956\n",
      "Epoch: 0 \t Batch: 66000 \t loss: 0.29651\n",
      "Epoch: 0 \t Batch: 66100 \t loss: 0.29530\n",
      "Epoch: 0 \t Batch: 66200 \t loss: 0.29336\n",
      "Epoch: 0 \t Batch: 66300 \t loss: 0.29308\n",
      "Epoch: 0 \t Batch: 66400 \t loss: 0.29494\n",
      "Epoch: 0 \t Batch: 66500 \t loss: 0.29566\n",
      "Epoch: 0 \t Batch: 66600 \t loss: 0.29423\n",
      "Epoch: 0 \t Batch: 66700 \t loss: 0.29427\n",
      "Epoch: 0 \t Batch: 66800 \t loss: 0.29732\n",
      "Epoch: 0 \t Batch: 66900 \t loss: 0.29286\n",
      "Epoch: 0 \t Batch: 67000 \t loss: 0.29387\n",
      "Epoch: 0 \t Batch: 67100 \t loss: 0.29524\n",
      "Epoch: 0 \t Batch: 67200 \t loss: 0.29854\n",
      "Epoch: 0 \t Batch: 67300 \t loss: 0.29168\n",
      "Epoch: 0 \t Batch: 67400 \t loss: 0.29126\n",
      "Epoch: 0 \t Batch: 67500 \t loss: 0.28830\n",
      "Epoch: 0 \t Batch: 67600 \t loss: 0.29272\n",
      "Epoch: 0 \t Batch: 67700 \t loss: 0.29090\n",
      "Epoch: 0 \t Batch: 67800 \t loss: 0.29483\n",
      "Epoch: 0 \t Batch: 67900 \t loss: 0.29494\n",
      "Epoch: 0 \t Batch: 68000 \t loss: 0.28919\n",
      "Epoch: 0 \t Batch: 68100 \t loss: 0.28898\n",
      "Epoch: 0 \t Batch: 68200 \t loss: 0.28615\n",
      "Epoch: 0 \t Batch: 68300 \t loss: 0.29203\n",
      "Epoch: 0 \t Batch: 68400 \t loss: 0.28783\n",
      "Epoch: 0 \t Batch: 68500 \t loss: 0.29419\n",
      "Epoch: 0 \t Batch: 68600 \t loss: 0.29036\n",
      "Epoch: 0 \t Batch: 68700 \t loss: 0.28886\n",
      "Epoch: 0 \t Batch: 68800 \t loss: 0.29239\n",
      "Epoch: 0 \t Batch: 68900 \t loss: 0.29044\n",
      "Epoch: 0 \t Batch: 69000 \t loss: 0.29449\n",
      "Epoch: 0 \t Batch: 69100 \t loss: 0.29172\n",
      "Epoch: 0 \t Batch: 69200 \t loss: 0.28681\n",
      "Epoch: 0 \t Batch: 69300 \t loss: 0.28401\n",
      "Epoch: 0 \t Batch: 69400 \t loss: 0.28740\n",
      "Epoch: 0 \t Batch: 69500 \t loss: 0.29027\n",
      "Epoch: 0 \t Batch: 69600 \t loss: 0.28986\n",
      "Epoch: 0 \t Batch: 69700 \t loss: 0.28902\n",
      "Epoch: 0 \t Batch: 69800 \t loss: 0.28592\n",
      "Epoch: 0 \t Batch: 69900 \t loss: 0.28791\n",
      "Epoch: 0 \t Batch: 70000 \t loss: 0.29399\n",
      "Epoch: 0 \t Batch: 70100 \t loss: 0.28691\n",
      "Epoch: 0 \t Batch: 70200 \t loss: 0.28859\n",
      "Epoch: 0 \t Batch: 70300 \t loss: 0.28759\n",
      "Epoch: 0 \t Batch: 70400 \t loss: 0.28757\n",
      "Epoch: 0 \t Batch: 70500 \t loss: 0.28893\n",
      "Epoch: 0 \t Batch: 70600 \t loss: 0.29135\n",
      "Epoch: 0 \t Batch: 70700 \t loss: 0.28979\n",
      "Epoch: 0 \t Batch: 70800 \t loss: 0.28904\n",
      "Epoch: 0 \t Batch: 70900 \t loss: 0.29188\n",
      "Epoch: 0 \t Batch: 71000 \t loss: 0.28436\n",
      "Epoch: 0 \t Batch: 71100 \t loss: 0.28500\n",
      "Epoch: 0 \t Batch: 71200 \t loss: 0.28176\n",
      "Epoch: 0 \t Batch: 71300 \t loss: 0.28804\n",
      "Epoch: 0 \t Batch: 71400 \t loss: 0.28721\n",
      "Epoch: 0 \t Batch: 71500 \t loss: 0.28795\n",
      "Epoch: 0 \t Batch: 71600 \t loss: 0.28598\n",
      "Epoch: 0 \t Batch: 71700 \t loss: 0.28619\n",
      "Epoch: 0 \t Batch: 71800 \t loss: 0.28903\n",
      "Epoch: 0 \t Batch: 71900 \t loss: 0.28729\n",
      "Epoch: 0 \t Batch: 72000 \t loss: 0.28209\n",
      "Epoch: 0 \t Batch: 72100 \t loss: 0.29041\n",
      "Epoch: 0 \t Batch: 72200 \t loss: 0.28743\n",
      "Epoch: 0 \t Batch: 72300 \t loss: 0.28981\n",
      "Epoch: 0 \t Batch: 72400 \t loss: 0.29006\n",
      "Epoch: 0 \t Batch: 72500 \t loss: 0.29058\n",
      "Epoch: 0 \t Batch: 72600 \t loss: 0.28484\n",
      "Epoch: 0 \t Batch: 72700 \t loss: 0.28370\n",
      "Epoch: 0 \t Batch: 72800 \t loss: 0.29041\n",
      "Epoch: 0 \t Batch: 72900 \t loss: 0.28321\n",
      "Epoch: 0 \t Batch: 73000 \t loss: 0.28290\n",
      "Epoch: 0 \t Batch: 73100 \t loss: 0.28916\n",
      "Epoch: 0 \t Batch: 73200 \t loss: 0.28671\n",
      "Epoch: 0 \t Batch: 73300 \t loss: 0.28715\n",
      "Epoch: 0 \t Batch: 73400 \t loss: 0.28720\n",
      "Epoch: 0 \t Batch: 73500 \t loss: 0.28799\n",
      "Epoch: 0 \t Batch: 73600 \t loss: 0.29149\n",
      "Epoch: 0 \t Batch: 73700 \t loss: 0.28140\n",
      "Epoch: 0 \t Batch: 73800 \t loss: 0.28768\n",
      "Epoch: 0 \t Batch: 73900 \t loss: 0.28612\n",
      "Epoch: 0 \t Batch: 74000 \t loss: 0.29197\n",
      "Epoch: 0 \t Batch: 74100 \t loss: 0.28697\n",
      "Epoch: 0 \t Batch: 74200 \t loss: 0.28724\n",
      "Epoch: 0 \t Batch: 74300 \t loss: 0.28627\n",
      "Epoch: 0 \t Batch: 74400 \t loss: 0.28486\n",
      "Epoch: 0 \t Batch: 74500 \t loss: 0.28509\n",
      "Epoch: 0 \t Batch: 74600 \t loss: 0.29059\n",
      "Epoch: 0 \t Batch: 74700 \t loss: 0.28314\n",
      "Epoch: 0 \t Batch: 74800 \t loss: 0.28708\n",
      "Epoch: 0 \t Batch: 74900 \t loss: 0.28552\n",
      "Epoch: 0 \t Batch: 75000 \t loss: 0.28388\n",
      "Epoch: 0 \t Batch: 75100 \t loss: 0.28278\n",
      "Epoch: 0 \t Batch: 75200 \t loss: 0.28718\n",
      "Epoch: 0 \t Batch: 75300 \t loss: 0.28683\n",
      "Epoch: 0 \t Batch: 75400 \t loss: 0.28090\n",
      "Epoch: 0 \t Batch: 75500 \t loss: 0.28325\n",
      "Epoch: 0 \t Batch: 75600 \t loss: 0.28305\n",
      "Epoch: 0 \t Batch: 75700 \t loss: 0.28344\n",
      "Epoch: 0 \t Batch: 75800 \t loss: 0.28508\n",
      "Epoch: 0 \t Batch: 75900 \t loss: 0.28217\n",
      "Epoch: 0 \t Batch: 76000 \t loss: 0.28266\n",
      "Epoch: 0 \t Batch: 76100 \t loss: 0.28752\n",
      "Epoch: 0 \t Batch: 76200 \t loss: 0.28061\n",
      "Epoch: 0 \t Batch: 76300 \t loss: 0.28382\n",
      "Epoch: 0 \t Batch: 76400 \t loss: 0.28417\n",
      "Epoch: 0 \t Batch: 76500 \t loss: 0.28137\n",
      "Epoch: 0 \t Batch: 76600 \t loss: 0.28747\n",
      "Epoch: 0 \t Batch: 76700 \t loss: 0.28398\n",
      "Epoch: 0 \t Batch: 76800 \t loss: 0.28246\n",
      "Epoch: 0 \t Batch: 76900 \t loss: 0.28696\n",
      "Epoch: 0 \t Batch: 77000 \t loss: 0.28761\n",
      "Epoch: 0 \t Batch: 77100 \t loss: 0.28205\n",
      "Epoch: 0 \t Batch: 77200 \t loss: 0.28442\n",
      "Epoch: 0 \t Batch: 77300 \t loss: 0.28418\n",
      "Epoch: 0 \t Batch: 77400 \t loss: 0.28709\n",
      "Epoch: 0 \t Batch: 77500 \t loss: 0.28229\n",
      "Epoch: 0 \t Batch: 77600 \t loss: 0.28451\n",
      "Epoch: 0 \t Batch: 77700 \t loss: 0.28533\n",
      "Epoch: 0 \t Batch: 77800 \t loss: 0.28426\n",
      "Epoch: 0 \t Batch: 77900 \t loss: 0.28647\n",
      "Epoch: 0 \t Batch: 78000 \t loss: 0.28270\n",
      "Epoch: 0 \t Batch: 78100 \t loss: 0.28605\n",
      "Epoch: 0 \t Batch: 78200 \t loss: 0.28382\n",
      "Epoch: 0 \t Batch: 78300 \t loss: 0.28109\n",
      "Epoch: 0 \t Batch: 78400 \t loss: 0.28546\n",
      "Epoch: 0 \t Batch: 78500 \t loss: 0.28630\n",
      "Epoch: 0 \t Batch: 78600 \t loss: 0.28412\n",
      "Epoch: 0 \t Batch: 78700 \t loss: 0.28441\n",
      "Epoch: 0 \t Batch: 78800 \t loss: 0.27936\n",
      "Epoch: 0 \t Batch: 78900 \t loss: 0.28649\n",
      "Epoch: 0 \t Batch: 79000 \t loss: 0.28195\n",
      "Epoch: 0 \t Batch: 79100 \t loss: 0.28472\n",
      "Epoch: 0 \t Batch: 79200 \t loss: 0.27874\n",
      "Epoch: 0 \t Batch: 79300 \t loss: 0.28254\n",
      "Epoch: 0 \t Batch: 79400 \t loss: 0.28593\n",
      "Epoch: 0 \t Batch: 79500 \t loss: 0.28462\n",
      "Epoch: 0 \t Batch: 79600 \t loss: 0.28749\n",
      "Epoch: 0 \t Batch: 79700 \t loss: 0.28311\n",
      "Epoch: 0 \t Batch: 79800 \t loss: 0.28560\n",
      "Epoch: 0 \t Batch: 79900 \t loss: 0.28399\n",
      "Epoch: 0 \t Batch: 80000 \t loss: 0.28456\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    running_loss = 0\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        netG.train()\n",
    "        optimG.zero_grad()\n",
    "        adv = netG(img)\n",
    "        \n",
    "        # projection\n",
    "        adv = torch.min(torch.max(adv, img - args.eps/255.0), img + args.eps/255.0)\n",
    "        adv = torch.clamp(adv, 0.0, 1.0)\n",
    "        \n",
    "        if args.RN:\n",
    "            mean = np.random.normal(0.50, 0.08)\n",
    "            std = np.random.normal(0.75, 0.08)\n",
    "            adv_out_slice = model(normalize(adv.clone(), mean, std))[layer_idx]\n",
    "            img_out_slice = model(normalize(img.clone(), mean, std))[layer_idx]\n",
    "        else:\n",
    "            adv_out_slice = model(default_normalize(adv.clone()))[layer_idx]\n",
    "            img_out_slice = model(default_normalize(img.clone()))[layer_idx]\n",
    "        \n",
    "        if args.FA:\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if args.DA:\n",
    "            attention = abs(torch.mean(img_out_slice, dim=1, keepdim=True)).detach()\n",
    "        else:\n",
    "            attention = torch.ones(adv_out_slice.shape).cuda()\n",
    "            \n",
    "        loss = torch.cosine_similarity((adv_out_slice*attention).reshape(adv_out_slice.shape[0], -1), \n",
    "                                       (img_out_slice*attention).reshape(img_out_slice.shape[0], -1)).mean()\n",
    "        loss.backward()\n",
    "        optimG.step()\n",
    "        \n",
    "        # Every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {0} \\t Batch: {1} \\t loss: {2:.5f}'.format(epoch, i, running_loss/100))\n",
    "            running_loss = 0\n",
    "        running_loss += abs(loss.item())\n",
    "        \n",
    "        # Every 1 epoch\n",
    "        if i % 80000 == 0 and i > 0:\n",
    "            save_checkpoint_dir = 'saved_models/{}'.format(args.model_type)\n",
    "            if not os.path.exists(save_checkpoint_dir):\n",
    "                os.makedirs(save_checkpoint_dir)\n",
    "            save_path = os.path.join(save_checkpoint_dir, 'netG_{}_{}.pth'.format(save_checkpoint_suffix, epoch))\n",
    "            torch.save(netG.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a5ed91507c36eb248011b7d4b42840e9d47d02750f359633e20bb6b8b69add"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
