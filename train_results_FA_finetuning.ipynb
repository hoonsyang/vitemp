{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# custom\n",
    "from model_layer import Vgg16_all_layer, Vgg19_all_layer, Res152_all_layer, Dense169_all_layer\n",
    "from generator import GeneratorResnet\n",
    "from dct import *\n",
    "# from utils import *\n",
    "from loader_checkpoint import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DA=False, FA=False, RN=False, epochs=0, eps=10, model_type='vgg16')\n"
     ]
    }
   ],
   "source": [
    "parser0 = argparse.ArgumentParser(description='Transferable Perturbation via Frequency Manipulation')\n",
    "parser0.add_argument('--epochs', type=int, default=0, help='Model checkpoint epoch number')\n",
    "parser0.add_argument('--eps', type=int, default=10, help='Perturbation budget (0~255)')\n",
    "parser0.add_argument('--model_type', type=str, default='vgg16', help='Victim model: vgg16, vgg19, res152, dense169')\n",
    "parser0.add_argument('--RN', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Random Normalization module in training phase')\n",
    "parser0.add_argument('--DA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Domain-agnostic Attention module in training phase')\n",
    "parser0.add_argument('--FA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Frequency Augmentation module in training phase')\n",
    "args0 = parser0.parse_args(args=[])\n",
    "print(args0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DA=False, FA=False, RN=False, batch_size=16, epochs=1, eps=10, lr=2e-05, model_type='vgg16', rho=0.5, sigma=16.0, train_dir='../dataset/imagenet/train')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Transferable Perturbation via Frequency Manipulation')\n",
    "parser.add_argument('--train_dir', default='../dataset/imagenet/train', help='Path for imagenet training data')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='Batch size')\n",
    "parser.add_argument('--epochs', type=int, default=1, help='Number of training epochs')\n",
    "parser.add_argument('--lr', type=float, default=0.00002, help='Initial learning rate') # default=0.0002\n",
    "parser.add_argument('--eps', type=int, default=10, help='Perturbation budget (0~255)')\n",
    "parser.add_argument('--model_type', type=str, default='vgg16', help='Victim model: vgg16, vgg19, res152, dense169')\n",
    "parser.add_argument('--RN', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Random Normalization module in training phase')\n",
    "parser.add_argument('--DA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Domain-agnostic Attention module in training phase')\n",
    "parser.add_argument('--FA', type=lambda x: (str(x).lower() == 'true'), default=False, help='If true, activating the Frequency Augmentation module in training phase')\n",
    "parser.add_argument(\"--rho\", type=float, default=0.5, help=\"Tuning factor\")\n",
    "parser.add_argument(\"--sigma\", type=float, default=16.0, help=\"Std of random noise\")\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_seed(seed):\n",
    "#     random.seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# setup_seed(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the victim classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg16(\n",
       "  (vgg): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (model): ModuleList(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.model_type == 'vgg16':\n",
    "    model = Vgg16_all_layer.Vgg16()\n",
    "    layer_idx = 16 # Maxpooling.3\n",
    "elif args.model_type == 'vgg19':\n",
    "    model = Vgg19_all_layer.Vgg19()\n",
    "    layer_idx = 18 # Maxpooling.3\n",
    "elif args.model_type == 'res152':\n",
    "    model = Res152_all_layer.Resnet152()\n",
    "    layer_idx = 5 # Conv3_8\n",
    "elif args.model_type == 'dense169':\n",
    "    model = Dense169_all_layer.Dense169()\n",
    "    layer_idx = 6 # Denseblock.2\n",
    "else:\n",
    "    raise Exception('Check the model_type')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the generative attack model/optimizer/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substitute Model: vgg16 \t RN: False \t DA: False \t FA: False \t Saving instance: 0\n",
      "Training data size: 1281167\n"
     ]
    }
   ],
   "source": [
    "### Model, Optimizer\n",
    "### From scratch\n",
    "# netG = GeneratorResnet()\n",
    "# netG = nn.DataParallel(netG, device_ids=[0,1,2,3]) # multi-GPU\n",
    "# netG = netG.to(device)\n",
    "### Load the pretrained generator\n",
    "netG = load_gan(args0, 'imagenet')\n",
    "netG = netG.to(device)\n",
    "\n",
    "optimG = optim.Adam(netG.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "if args.RN and args.DA:\n",
    "    save_checkpoint_suffix = 'BIA+RN+DA'\n",
    "elif args.RN:\n",
    "    save_checkpoint_suffix = 'BIA+RN'\n",
    "elif args.DA:\n",
    "    save_checkpoint_suffix = 'BIA+DA'\n",
    "elif args.FA:\n",
    "    save_checkpoint_suffix = 'BIA+FA'\n",
    "else:\n",
    "    save_checkpoint_suffix = 'BIA'\n",
    "\n",
    "# Data, Transform\n",
    "scale_size = 256\n",
    "img_size = 224\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(scale_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dir = args.train_dir\n",
    "train_set = datasets.ImageFolder(train_dir, data_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "train_size = len(train_set)\n",
    "print('Training data size:', train_size)\n",
    "\n",
    "def default_normalize(t):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - 0.485) / 0.229\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - 0.456) / 0.224\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - 0.406) / 0.225\n",
    "    return t\n",
    "\n",
    "def normalize(t, mean, std):\n",
    "    t[:, 0, :, :] = (t[:, 0, :, :] - mean) / std\n",
    "    t[:, 1, :, :] = (t[:, 1, :, :] - mean) / std\n",
    "    t[:, 2, :, :] = (t[:, 2, :, :] - mean) / std\n",
    "    return t"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 0 \t loss: 0.00000\n",
      "Epoch: 0 \t Batch: 100 \t loss: 0.28076\n",
      "Epoch: 0 \t Batch: 200 \t loss: 0.28268\n",
      "Epoch: 0 \t Batch: 300 \t loss: 0.28281\n",
      "Epoch: 0 \t Batch: 400 \t loss: 0.27900\n",
      "Epoch: 0 \t Batch: 500 \t loss: 0.28137\n",
      "Epoch: 0 \t Batch: 600 \t loss: 0.27571\n",
      "Epoch: 0 \t Batch: 700 \t loss: 0.28105\n",
      "Epoch: 0 \t Batch: 800 \t loss: 0.28567\n",
      "Epoch: 0 \t Batch: 900 \t loss: 0.27712\n",
      "Epoch: 0 \t Batch: 1000 \t loss: 0.27653\n",
      "Epoch: 0 \t Batch: 1100 \t loss: 0.27905\n",
      "Epoch: 0 \t Batch: 1200 \t loss: 0.27885\n",
      "Epoch: 0 \t Batch: 1300 \t loss: 0.27521\n",
      "Epoch: 0 \t Batch: 1400 \t loss: 0.27886\n",
      "Epoch: 0 \t Batch: 1500 \t loss: 0.28284\n",
      "Epoch: 0 \t Batch: 1600 \t loss: 0.28159\n",
      "Epoch: 0 \t Batch: 1700 \t loss: 0.27314\n",
      "Epoch: 0 \t Batch: 1800 \t loss: 0.27573\n",
      "Epoch: 0 \t Batch: 1900 \t loss: 0.28012\n",
      "Epoch: 0 \t Batch: 2000 \t loss: 0.27757\n",
      "Epoch: 0 \t Batch: 2100 \t loss: 0.28232\n",
      "Epoch: 0 \t Batch: 2200 \t loss: 0.27258\n",
      "Epoch: 0 \t Batch: 2300 \t loss: 0.28375\n",
      "Epoch: 0 \t Batch: 2400 \t loss: 0.27731\n",
      "Epoch: 0 \t Batch: 2500 \t loss: 0.27710\n",
      "Epoch: 0 \t Batch: 2600 \t loss: 0.27850\n",
      "Epoch: 0 \t Batch: 2700 \t loss: 0.27651\n",
      "Epoch: 0 \t Batch: 2800 \t loss: 0.27721\n",
      "Epoch: 0 \t Batch: 2900 \t loss: 0.27540\n",
      "Epoch: 0 \t Batch: 3000 \t loss: 0.28222\n",
      "Epoch: 0 \t Batch: 3100 \t loss: 0.28205\n",
      "Epoch: 0 \t Batch: 3200 \t loss: 0.27872\n",
      "Epoch: 0 \t Batch: 3300 \t loss: 0.27778\n",
      "Epoch: 0 \t Batch: 3400 \t loss: 0.27483\n",
      "Epoch: 0 \t Batch: 3500 \t loss: 0.28027\n",
      "Epoch: 0 \t Batch: 3600 \t loss: 0.27446\n",
      "Epoch: 0 \t Batch: 3700 \t loss: 0.27881\n",
      "Epoch: 0 \t Batch: 3800 \t loss: 0.27985\n",
      "Epoch: 0 \t Batch: 3900 \t loss: 0.27078\n",
      "Epoch: 0 \t Batch: 4000 \t loss: 0.27897\n",
      "Epoch: 0 \t Batch: 4100 \t loss: 0.27560\n",
      "Epoch: 0 \t Batch: 4200 \t loss: 0.28431\n",
      "Epoch: 0 \t Batch: 4300 \t loss: 0.27969\n",
      "Epoch: 0 \t Batch: 4400 \t loss: 0.28006\n",
      "Epoch: 0 \t Batch: 4500 \t loss: 0.27453\n",
      "Epoch: 0 \t Batch: 4600 \t loss: 0.27767\n",
      "Epoch: 0 \t Batch: 4700 \t loss: 0.27889\n",
      "Epoch: 0 \t Batch: 4800 \t loss: 0.27205\n",
      "Epoch: 0 \t Batch: 4900 \t loss: 0.27461\n",
      "Epoch: 0 \t Batch: 5000 \t loss: 0.27640\n",
      "Epoch: 0 \t Batch: 5100 \t loss: 0.27629\n",
      "Epoch: 0 \t Batch: 5200 \t loss: 0.27724\n",
      "Epoch: 0 \t Batch: 5300 \t loss: 0.27590\n",
      "Epoch: 0 \t Batch: 5400 \t loss: 0.27375\n",
      "Epoch: 0 \t Batch: 5500 \t loss: 0.27794\n",
      "Epoch: 0 \t Batch: 5600 \t loss: 0.27980\n",
      "Epoch: 0 \t Batch: 5700 \t loss: 0.28078\n",
      "Epoch: 0 \t Batch: 5800 \t loss: 0.27249\n",
      "Epoch: 0 \t Batch: 5900 \t loss: 0.27957\n",
      "Epoch: 0 \t Batch: 6000 \t loss: 0.28013\n",
      "Epoch: 0 \t Batch: 6100 \t loss: 0.27563\n",
      "Epoch: 0 \t Batch: 6200 \t loss: 0.27545\n",
      "Epoch: 0 \t Batch: 6300 \t loss: 0.28109\n",
      "Epoch: 0 \t Batch: 6400 \t loss: 0.27137\n",
      "Epoch: 0 \t Batch: 6500 \t loss: 0.27936\n",
      "Epoch: 0 \t Batch: 6600 \t loss: 0.27488\n",
      "Epoch: 0 \t Batch: 6700 \t loss: 0.27595\n",
      "Epoch: 0 \t Batch: 6800 \t loss: 0.28114\n",
      "Epoch: 0 \t Batch: 6900 \t loss: 0.27671\n",
      "Epoch: 0 \t Batch: 7000 \t loss: 0.27618\n",
      "Epoch: 0 \t Batch: 7100 \t loss: 0.27586\n",
      "Epoch: 0 \t Batch: 7200 \t loss: 0.27688\n",
      "Epoch: 0 \t Batch: 7300 \t loss: 0.27977\n",
      "Epoch: 0 \t Batch: 7400 \t loss: 0.27483\n",
      "Epoch: 0 \t Batch: 7500 \t loss: 0.28166\n",
      "Epoch: 0 \t Batch: 7600 \t loss: 0.28457\n",
      "Epoch: 0 \t Batch: 7700 \t loss: 0.27359\n",
      "Epoch: 0 \t Batch: 7800 \t loss: 0.27258\n",
      "Epoch: 0 \t Batch: 7900 \t loss: 0.27828\n",
      "Epoch: 0 \t Batch: 8000 \t loss: 0.27897\n",
      "Epoch: 0 \t Batch: 8100 \t loss: 0.27718\n",
      "Epoch: 0 \t Batch: 8200 \t loss: 0.27468\n",
      "Epoch: 0 \t Batch: 8300 \t loss: 0.27589\n",
      "Epoch: 0 \t Batch: 8400 \t loss: 0.28178\n",
      "Epoch: 0 \t Batch: 8500 \t loss: 0.27918\n",
      "Epoch: 0 \t Batch: 8600 \t loss: 0.28057\n",
      "Epoch: 0 \t Batch: 8700 \t loss: 0.28168\n",
      "Epoch: 0 \t Batch: 8800 \t loss: 0.27602\n",
      "Epoch: 0 \t Batch: 8900 \t loss: 0.27529\n",
      "Epoch: 0 \t Batch: 9000 \t loss: 0.27437\n",
      "Epoch: 0 \t Batch: 9100 \t loss: 0.27525\n",
      "Epoch: 0 \t Batch: 9200 \t loss: 0.27551\n",
      "Epoch: 0 \t Batch: 9300 \t loss: 0.27752\n",
      "Epoch: 0 \t Batch: 9400 \t loss: 0.27688\n",
      "Epoch: 0 \t Batch: 9500 \t loss: 0.27724\n",
      "Epoch: 0 \t Batch: 9600 \t loss: 0.27440\n",
      "Epoch: 0 \t Batch: 9700 \t loss: 0.27461\n",
      "Epoch: 0 \t Batch: 9800 \t loss: 0.27416\n",
      "Epoch: 0 \t Batch: 9900 \t loss: 0.27346\n",
      "Epoch: 0 \t Batch: 10000 \t loss: 0.27602\n",
      "Epoch: 0 \t Batch: 10100 \t loss: 0.27459\n",
      "Epoch: 0 \t Batch: 10200 \t loss: 0.27896\n",
      "Epoch: 0 \t Batch: 10300 \t loss: 0.27484\n",
      "Epoch: 0 \t Batch: 10400 \t loss: 0.27563\n",
      "Epoch: 0 \t Batch: 10500 \t loss: 0.27834\n",
      "Epoch: 0 \t Batch: 10600 \t loss: 0.27874\n",
      "Epoch: 0 \t Batch: 10700 \t loss: 0.27057\n",
      "Epoch: 0 \t Batch: 10800 \t loss: 0.27692\n",
      "Epoch: 0 \t Batch: 10900 \t loss: 0.27697\n",
      "Epoch: 0 \t Batch: 11000 \t loss: 0.27447\n",
      "Epoch: 0 \t Batch: 11100 \t loss: 0.27564\n",
      "Epoch: 0 \t Batch: 11200 \t loss: 0.27931\n",
      "Epoch: 0 \t Batch: 11300 \t loss: 0.27457\n",
      "Epoch: 0 \t Batch: 11400 \t loss: 0.27769\n",
      "Epoch: 0 \t Batch: 11500 \t loss: 0.27989\n",
      "Epoch: 0 \t Batch: 11600 \t loss: 0.27716\n",
      "Epoch: 0 \t Batch: 11700 \t loss: 0.27585\n",
      "Epoch: 0 \t Batch: 11800 \t loss: 0.27585\n",
      "Epoch: 0 \t Batch: 11900 \t loss: 0.27716\n",
      "Epoch: 0 \t Batch: 12000 \t loss: 0.27288\n",
      "Epoch: 0 \t Batch: 12100 \t loss: 0.27680\n",
      "Epoch: 0 \t Batch: 12200 \t loss: 0.27559\n",
      "Epoch: 0 \t Batch: 12300 \t loss: 0.27324\n",
      "Epoch: 0 \t Batch: 12400 \t loss: 0.27831\n",
      "Epoch: 0 \t Batch: 12500 \t loss: 0.27716\n",
      "Epoch: 0 \t Batch: 12600 \t loss: 0.27461\n",
      "Epoch: 0 \t Batch: 12700 \t loss: 0.27760\n",
      "Epoch: 0 \t Batch: 12800 \t loss: 0.27652\n",
      "Epoch: 0 \t Batch: 12900 \t loss: 0.27521\n",
      "Epoch: 0 \t Batch: 13000 \t loss: 0.27880\n",
      "Epoch: 0 \t Batch: 13100 \t loss: 0.27760\n",
      "Epoch: 0 \t Batch: 13200 \t loss: 0.27599\n",
      "Epoch: 0 \t Batch: 13300 \t loss: 0.27752\n",
      "Epoch: 0 \t Batch: 13400 \t loss: 0.27350\n",
      "Epoch: 0 \t Batch: 13500 \t loss: 0.27690\n",
      "Epoch: 0 \t Batch: 13600 \t loss: 0.27790\n",
      "Epoch: 0 \t Batch: 13700 \t loss: 0.27932\n",
      "Epoch: 0 \t Batch: 13800 \t loss: 0.27356\n",
      "Epoch: 0 \t Batch: 13900 \t loss: 0.27281\n",
      "Epoch: 0 \t Batch: 14000 \t loss: 0.27710\n",
      "Epoch: 0 \t Batch: 14100 \t loss: 0.28025\n",
      "Epoch: 0 \t Batch: 14200 \t loss: 0.27878\n",
      "Epoch: 0 \t Batch: 14300 \t loss: 0.27505\n",
      "Epoch: 0 \t Batch: 14400 \t loss: 0.27204\n",
      "Epoch: 0 \t Batch: 14500 \t loss: 0.27505\n",
      "Epoch: 0 \t Batch: 14600 \t loss: 0.27494\n",
      "Epoch: 0 \t Batch: 14700 \t loss: 0.27935\n",
      "Epoch: 0 \t Batch: 14800 \t loss: 0.28147\n",
      "Epoch: 0 \t Batch: 14900 \t loss: 0.27483\n",
      "Epoch: 0 \t Batch: 15000 \t loss: 0.27430\n",
      "Epoch: 0 \t Batch: 15100 \t loss: 0.27728\n",
      "Epoch: 0 \t Batch: 15200 \t loss: 0.27230\n",
      "Epoch: 0 \t Batch: 15300 \t loss: 0.27474\n",
      "Epoch: 0 \t Batch: 15400 \t loss: 0.27907\n",
      "Epoch: 0 \t Batch: 15500 \t loss: 0.27576\n",
      "Epoch: 0 \t Batch: 15600 \t loss: 0.27823\n",
      "Epoch: 0 \t Batch: 15700 \t loss: 0.27403\n",
      "Epoch: 0 \t Batch: 15800 \t loss: 0.27197\n",
      "Epoch: 0 \t Batch: 15900 \t loss: 0.27893\n",
      "Epoch: 0 \t Batch: 16000 \t loss: 0.27589\n",
      "Epoch: 0 \t Batch: 16100 \t loss: 0.27647\n",
      "Epoch: 0 \t Batch: 16200 \t loss: 0.27731\n",
      "Epoch: 0 \t Batch: 16300 \t loss: 0.27590\n",
      "Epoch: 0 \t Batch: 16400 \t loss: 0.27353\n",
      "Epoch: 0 \t Batch: 16500 \t loss: 0.27239\n",
      "Epoch: 0 \t Batch: 16600 \t loss: 0.27540\n",
      "Epoch: 0 \t Batch: 16700 \t loss: 0.27572\n",
      "Epoch: 0 \t Batch: 16800 \t loss: 0.27452\n",
      "Epoch: 0 \t Batch: 16900 \t loss: 0.27875\n",
      "Epoch: 0 \t Batch: 17000 \t loss: 0.27816\n",
      "Epoch: 0 \t Batch: 17100 \t loss: 0.27343\n",
      "Epoch: 0 \t Batch: 17200 \t loss: 0.27595\n",
      "Epoch: 0 \t Batch: 17300 \t loss: 0.27216\n",
      "Epoch: 0 \t Batch: 17400 \t loss: 0.27870\n",
      "Epoch: 0 \t Batch: 17500 \t loss: 0.27636\n",
      "Epoch: 0 \t Batch: 17600 \t loss: 0.27436\n",
      "Epoch: 0 \t Batch: 17700 \t loss: 0.27217\n",
      "Epoch: 0 \t Batch: 17800 \t loss: 0.27942\n",
      "Epoch: 0 \t Batch: 17900 \t loss: 0.27626\n",
      "Epoch: 0 \t Batch: 18000 \t loss: 0.27366\n",
      "Epoch: 0 \t Batch: 18100 \t loss: 0.27592\n",
      "Epoch: 0 \t Batch: 18200 \t loss: 0.28102\n",
      "Epoch: 0 \t Batch: 18300 \t loss: 0.27477\n",
      "Epoch: 0 \t Batch: 18400 \t loss: 0.27441\n",
      "Epoch: 0 \t Batch: 18500 \t loss: 0.27432\n",
      "Epoch: 0 \t Batch: 18600 \t loss: 0.27101\n",
      "Epoch: 0 \t Batch: 18700 \t loss: 0.28150\n",
      "Epoch: 0 \t Batch: 18800 \t loss: 0.27412\n",
      "Epoch: 0 \t Batch: 18900 \t loss: 0.27578\n",
      "Epoch: 0 \t Batch: 19000 \t loss: 0.27188\n",
      "Epoch: 0 \t Batch: 19100 \t loss: 0.27351\n",
      "Epoch: 0 \t Batch: 19200 \t loss: 0.27823\n",
      "Epoch: 0 \t Batch: 19300 \t loss: 0.27680\n",
      "Epoch: 0 \t Batch: 19400 \t loss: 0.27484\n",
      "Epoch: 0 \t Batch: 19500 \t loss: 0.27784\n",
      "Epoch: 0 \t Batch: 19600 \t loss: 0.27562\n",
      "Epoch: 0 \t Batch: 19700 \t loss: 0.27569\n",
      "Epoch: 0 \t Batch: 19800 \t loss: 0.27979\n",
      "Epoch: 0 \t Batch: 19900 \t loss: 0.27786\n",
      "Epoch: 0 \t Batch: 20000 \t loss: 0.27529\n",
      "Epoch: 0 \t Batch: 20100 \t loss: 0.27145\n",
      "Epoch: 0 \t Batch: 20200 \t loss: 0.27611\n",
      "Epoch: 0 \t Batch: 20300 \t loss: 0.27985\n",
      "Epoch: 0 \t Batch: 20400 \t loss: 0.27410\n",
      "Epoch: 0 \t Batch: 20500 \t loss: 0.27790\n",
      "Epoch: 0 \t Batch: 20600 \t loss: 0.27649\n",
      "Epoch: 0 \t Batch: 20700 \t loss: 0.27733\n",
      "Epoch: 0 \t Batch: 20800 \t loss: 0.27538\n",
      "Epoch: 0 \t Batch: 20900 \t loss: 0.27686\n",
      "Epoch: 0 \t Batch: 21000 \t loss: 0.27205\n",
      "Epoch: 0 \t Batch: 21100 \t loss: 0.27532\n",
      "Epoch: 0 \t Batch: 21200 \t loss: 0.27824\n",
      "Epoch: 0 \t Batch: 21300 \t loss: 0.27410\n",
      "Epoch: 0 \t Batch: 21400 \t loss: 0.27505\n",
      "Epoch: 0 \t Batch: 21500 \t loss: 0.27525\n",
      "Epoch: 0 \t Batch: 21600 \t loss: 0.27772\n",
      "Epoch: 0 \t Batch: 21700 \t loss: 0.27353\n",
      "Epoch: 0 \t Batch: 21800 \t loss: 0.27511\n",
      "Epoch: 0 \t Batch: 21900 \t loss: 0.27526\n",
      "Epoch: 0 \t Batch: 22000 \t loss: 0.27793\n",
      "Epoch: 0 \t Batch: 22100 \t loss: 0.27751\n",
      "Epoch: 0 \t Batch: 22200 \t loss: 0.27762\n",
      "Epoch: 0 \t Batch: 22300 \t loss: 0.27618\n",
      "Epoch: 0 \t Batch: 22400 \t loss: 0.27331\n",
      "Epoch: 0 \t Batch: 22500 \t loss: 0.27879\n",
      "Epoch: 0 \t Batch: 22600 \t loss: 0.27286\n",
      "Epoch: 0 \t Batch: 22700 \t loss: 0.27326\n",
      "Epoch: 0 \t Batch: 22800 \t loss: 0.27577\n",
      "Epoch: 0 \t Batch: 22900 \t loss: 0.27186\n",
      "Epoch: 0 \t Batch: 23000 \t loss: 0.27494\n",
      "Epoch: 0 \t Batch: 23100 \t loss: 0.27783\n",
      "Epoch: 0 \t Batch: 23200 \t loss: 0.27611\n",
      "Epoch: 0 \t Batch: 23300 \t loss: 0.27523\n",
      "Epoch: 0 \t Batch: 23400 \t loss: 0.27640\n",
      "Epoch: 0 \t Batch: 23500 \t loss: 0.27507\n",
      "Epoch: 0 \t Batch: 23600 \t loss: 0.27856\n",
      "Epoch: 0 \t Batch: 23700 \t loss: 0.27551\n",
      "Epoch: 0 \t Batch: 23800 \t loss: 0.27669\n",
      "Epoch: 0 \t Batch: 23900 \t loss: 0.27485\n",
      "Epoch: 0 \t Batch: 24000 \t loss: 0.27218\n",
      "Epoch: 0 \t Batch: 24100 \t loss: 0.27819\n",
      "Epoch: 0 \t Batch: 24200 \t loss: 0.27826\n",
      "Epoch: 0 \t Batch: 24300 \t loss: 0.27359\n",
      "Epoch: 0 \t Batch: 24400 \t loss: 0.28258\n",
      "Epoch: 0 \t Batch: 24500 \t loss: 0.28098\n",
      "Epoch: 0 \t Batch: 24600 \t loss: 0.27705\n",
      "Epoch: 0 \t Batch: 24700 \t loss: 0.27610\n",
      "Epoch: 0 \t Batch: 24800 \t loss: 0.27134\n",
      "Epoch: 0 \t Batch: 24900 \t loss: 0.27644\n",
      "Epoch: 0 \t Batch: 25000 \t loss: 0.27245\n",
      "Epoch: 0 \t Batch: 25100 \t loss: 0.28026\n",
      "Epoch: 0 \t Batch: 25200 \t loss: 0.27410\n",
      "Epoch: 0 \t Batch: 25300 \t loss: 0.27754\n",
      "Epoch: 0 \t Batch: 25400 \t loss: 0.27769\n",
      "Epoch: 0 \t Batch: 25500 \t loss: 0.27584\n",
      "Epoch: 0 \t Batch: 25600 \t loss: 0.27046\n",
      "Epoch: 0 \t Batch: 25700 \t loss: 0.27123\n",
      "Epoch: 0 \t Batch: 25800 \t loss: 0.27950\n",
      "Epoch: 0 \t Batch: 25900 \t loss: 0.27546\n",
      "Epoch: 0 \t Batch: 26000 \t loss: 0.27252\n",
      "Epoch: 0 \t Batch: 26100 \t loss: 0.27566\n",
      "Epoch: 0 \t Batch: 26200 \t loss: 0.27574\n",
      "Epoch: 0 \t Batch: 26300 \t loss: 0.27705\n",
      "Epoch: 0 \t Batch: 26400 \t loss: 0.27822\n",
      "Epoch: 0 \t Batch: 26500 \t loss: 0.27676\n",
      "Epoch: 0 \t Batch: 26600 \t loss: 0.27367\n",
      "Epoch: 0 \t Batch: 26700 \t loss: 0.27089\n",
      "Epoch: 0 \t Batch: 26800 \t loss: 0.27670\n",
      "Epoch: 0 \t Batch: 26900 \t loss: 0.27617\n",
      "Epoch: 0 \t Batch: 27000 \t loss: 0.27094\n",
      "Epoch: 0 \t Batch: 27100 \t loss: 0.27710\n",
      "Epoch: 0 \t Batch: 27200 \t loss: 0.27643\n",
      "Epoch: 0 \t Batch: 27300 \t loss: 0.27593\n",
      "Epoch: 0 \t Batch: 27400 \t loss: 0.27727\n",
      "Epoch: 0 \t Batch: 27500 \t loss: 0.27384\n",
      "Epoch: 0 \t Batch: 27600 \t loss: 0.27598\n",
      "Epoch: 0 \t Batch: 27700 \t loss: 0.27453\n",
      "Epoch: 0 \t Batch: 27800 \t loss: 0.27548\n",
      "Epoch: 0 \t Batch: 27900 \t loss: 0.27791\n",
      "Epoch: 0 \t Batch: 28000 \t loss: 0.27541\n",
      "Epoch: 0 \t Batch: 28100 \t loss: 0.27560\n",
      "Epoch: 0 \t Batch: 28200 \t loss: 0.27496\n",
      "Epoch: 0 \t Batch: 28300 \t loss: 0.27421\n",
      "Epoch: 0 \t Batch: 28400 \t loss: 0.27664\n",
      "Epoch: 0 \t Batch: 28500 \t loss: 0.27642\n",
      "Epoch: 0 \t Batch: 28600 \t loss: 0.27576\n",
      "Epoch: 0 \t Batch: 28700 \t loss: 0.27422\n",
      "Epoch: 0 \t Batch: 28800 \t loss: 0.27637\n",
      "Epoch: 0 \t Batch: 28900 \t loss: 0.27163\n",
      "Epoch: 0 \t Batch: 29000 \t loss: 0.27753\n",
      "Epoch: 0 \t Batch: 29100 \t loss: 0.27702\n",
      "Epoch: 0 \t Batch: 29200 \t loss: 0.27863\n",
      "Epoch: 0 \t Batch: 29300 \t loss: 0.27612\n",
      "Epoch: 0 \t Batch: 29400 \t loss: 0.27386\n",
      "Epoch: 0 \t Batch: 29500 \t loss: 0.27721\n",
      "Epoch: 0 \t Batch: 29600 \t loss: 0.27840\n",
      "Epoch: 0 \t Batch: 29700 \t loss: 0.27311\n",
      "Epoch: 0 \t Batch: 29800 \t loss: 0.28006\n",
      "Epoch: 0 \t Batch: 29900 \t loss: 0.27858\n",
      "Epoch: 0 \t Batch: 30000 \t loss: 0.27511\n",
      "Epoch: 0 \t Batch: 30100 \t loss: 0.27919\n",
      "Epoch: 0 \t Batch: 30200 \t loss: 0.26897\n",
      "Epoch: 0 \t Batch: 30300 \t loss: 0.27789\n",
      "Epoch: 0 \t Batch: 30400 \t loss: 0.27501\n",
      "Epoch: 0 \t Batch: 30500 \t loss: 0.27305\n",
      "Epoch: 0 \t Batch: 30600 \t loss: 0.27647\n",
      "Epoch: 0 \t Batch: 30700 \t loss: 0.27603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vilab/anaconda3/envs/bia/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:802: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Batch: 30800 \t loss: 0.27536\n",
      "Epoch: 0 \t Batch: 30900 \t loss: 0.27637\n",
      "Epoch: 0 \t Batch: 31000 \t loss: 0.28062\n",
      "Epoch: 0 \t Batch: 31100 \t loss: 0.27816\n",
      "Epoch: 0 \t Batch: 31200 \t loss: 0.27442\n",
      "Epoch: 0 \t Batch: 31300 \t loss: 0.27131\n",
      "Epoch: 0 \t Batch: 31400 \t loss: 0.27432\n",
      "Epoch: 0 \t Batch: 31500 \t loss: 0.28092\n",
      "Epoch: 0 \t Batch: 31600 \t loss: 0.27632\n",
      "Epoch: 0 \t Batch: 31700 \t loss: 0.27738\n",
      "Epoch: 0 \t Batch: 31800 \t loss: 0.27638\n",
      "Epoch: 0 \t Batch: 31900 \t loss: 0.27723\n",
      "Epoch: 0 \t Batch: 32000 \t loss: 0.27075\n",
      "Epoch: 0 \t Batch: 32100 \t loss: 0.27228\n",
      "Epoch: 0 \t Batch: 32200 \t loss: 0.27750\n",
      "Epoch: 0 \t Batch: 32300 \t loss: 0.27433\n",
      "Epoch: 0 \t Batch: 32400 \t loss: 0.27886\n",
      "Epoch: 0 \t Batch: 32500 \t loss: 0.27588\n",
      "Epoch: 0 \t Batch: 32600 \t loss: 0.27773\n",
      "Epoch: 0 \t Batch: 32700 \t loss: 0.27589\n",
      "Epoch: 0 \t Batch: 32800 \t loss: 0.27408\n",
      "Epoch: 0 \t Batch: 32900 \t loss: 0.27622\n",
      "Epoch: 0 \t Batch: 33000 \t loss: 0.27397\n",
      "Epoch: 0 \t Batch: 33100 \t loss: 0.27290\n",
      "Epoch: 0 \t Batch: 33200 \t loss: 0.26922\n",
      "Epoch: 0 \t Batch: 33300 \t loss: 0.27279\n",
      "Epoch: 0 \t Batch: 33400 \t loss: 0.27707\n",
      "Epoch: 0 \t Batch: 33500 \t loss: 0.27673\n",
      "Epoch: 0 \t Batch: 33600 \t loss: 0.27593\n",
      "Epoch: 0 \t Batch: 33700 \t loss: 0.27164\n",
      "Epoch: 0 \t Batch: 33800 \t loss: 0.27535\n",
      "Epoch: 0 \t Batch: 33900 \t loss: 0.27630\n",
      "Epoch: 0 \t Batch: 34000 \t loss: 0.27782\n",
      "Epoch: 0 \t Batch: 34100 \t loss: 0.27655\n",
      "Epoch: 0 \t Batch: 34200 \t loss: 0.27352\n",
      "Epoch: 0 \t Batch: 34300 \t loss: 0.27191\n",
      "Epoch: 0 \t Batch: 34400 \t loss: 0.27504\n",
      "Epoch: 0 \t Batch: 34500 \t loss: 0.27770\n",
      "Epoch: 0 \t Batch: 34600 \t loss: 0.27680\n",
      "Epoch: 0 \t Batch: 34700 \t loss: 0.27325\n",
      "Epoch: 0 \t Batch: 34800 \t loss: 0.27626\n",
      "Epoch: 0 \t Batch: 34900 \t loss: 0.27257\n",
      "Epoch: 0 \t Batch: 35000 \t loss: 0.27848\n",
      "Epoch: 0 \t Batch: 35100 \t loss: 0.27246\n",
      "Epoch: 0 \t Batch: 35200 \t loss: 0.27256\n",
      "Epoch: 0 \t Batch: 35300 \t loss: 0.27802\n",
      "Epoch: 0 \t Batch: 35400 \t loss: 0.27452\n",
      "Epoch: 0 \t Batch: 35500 \t loss: 0.27928\n",
      "Epoch: 0 \t Batch: 35600 \t loss: 0.27635\n",
      "Epoch: 0 \t Batch: 35700 \t loss: 0.27561\n",
      "Epoch: 0 \t Batch: 35800 \t loss: 0.27316\n",
      "Epoch: 0 \t Batch: 35900 \t loss: 0.27743\n",
      "Epoch: 0 \t Batch: 36000 \t loss: 0.27779\n",
      "Epoch: 0 \t Batch: 36100 \t loss: 0.27535\n",
      "Epoch: 0 \t Batch: 36200 \t loss: 0.27664\n",
      "Epoch: 0 \t Batch: 36300 \t loss: 0.27472\n",
      "Epoch: 0 \t Batch: 36400 \t loss: 0.27625\n",
      "Epoch: 0 \t Batch: 36500 \t loss: 0.27393\n",
      "Epoch: 0 \t Batch: 36600 \t loss: 0.27588\n",
      "Epoch: 0 \t Batch: 36700 \t loss: 0.27298\n",
      "Epoch: 0 \t Batch: 36800 \t loss: 0.27767\n",
      "Epoch: 0 \t Batch: 36900 \t loss: 0.27009\n",
      "Epoch: 0 \t Batch: 37000 \t loss: 0.27654\n",
      "Epoch: 0 \t Batch: 37100 \t loss: 0.27657\n",
      "Epoch: 0 \t Batch: 37200 \t loss: 0.27558\n",
      "Epoch: 0 \t Batch: 37300 \t loss: 0.27690\n",
      "Epoch: 0 \t Batch: 37400 \t loss: 0.27219\n",
      "Epoch: 0 \t Batch: 37500 \t loss: 0.27468\n",
      "Epoch: 0 \t Batch: 37600 \t loss: 0.27227\n",
      "Epoch: 0 \t Batch: 37700 \t loss: 0.27092\n",
      "Epoch: 0 \t Batch: 37800 \t loss: 0.27839\n",
      "Epoch: 0 \t Batch: 37900 \t loss: 0.27615\n",
      "Epoch: 0 \t Batch: 38000 \t loss: 0.27421\n",
      "Epoch: 0 \t Batch: 38100 \t loss: 0.27697\n",
      "Epoch: 0 \t Batch: 38200 \t loss: 0.27838\n",
      "Epoch: 0 \t Batch: 38300 \t loss: 0.28042\n",
      "Epoch: 0 \t Batch: 38400 \t loss: 0.27546\n",
      "Epoch: 0 \t Batch: 38500 \t loss: 0.27848\n",
      "Epoch: 0 \t Batch: 38600 \t loss: 0.26877\n",
      "Epoch: 0 \t Batch: 38700 \t loss: 0.27433\n",
      "Epoch: 0 \t Batch: 38800 \t loss: 0.27907\n",
      "Epoch: 0 \t Batch: 38900 \t loss: 0.27821\n",
      "Epoch: 0 \t Batch: 39000 \t loss: 0.27182\n",
      "Epoch: 0 \t Batch: 39100 \t loss: 0.27052\n",
      "Epoch: 0 \t Batch: 39200 \t loss: 0.27311\n",
      "Epoch: 0 \t Batch: 39300 \t loss: 0.27651\n",
      "Epoch: 0 \t Batch: 39400 \t loss: 0.27651\n",
      "Epoch: 0 \t Batch: 39500 \t loss: 0.27482\n",
      "Epoch: 0 \t Batch: 39600 \t loss: 0.27832\n",
      "Epoch: 0 \t Batch: 39700 \t loss: 0.27800\n",
      "Epoch: 0 \t Batch: 39800 \t loss: 0.27020\n",
      "Epoch: 0 \t Batch: 39900 \t loss: 0.27723\n",
      "Epoch: 0 \t Batch: 40000 \t loss: 0.27210\n",
      "Epoch: 0 \t Batch: 40100 \t loss: 0.27208\n",
      "Epoch: 0 \t Batch: 40200 \t loss: 0.27579\n",
      "Epoch: 0 \t Batch: 40300 \t loss: 0.27328\n",
      "Epoch: 0 \t Batch: 40400 \t loss: 0.26933\n",
      "Epoch: 0 \t Batch: 40500 \t loss: 0.27335\n",
      "Epoch: 0 \t Batch: 40600 \t loss: 0.27304\n",
      "Epoch: 0 \t Batch: 40700 \t loss: 0.27635\n",
      "Epoch: 0 \t Batch: 40800 \t loss: 0.27887\n",
      "Epoch: 0 \t Batch: 40900 \t loss: 0.27160\n",
      "Epoch: 0 \t Batch: 41000 \t loss: 0.27410\n",
      "Epoch: 0 \t Batch: 41100 \t loss: 0.27484\n",
      "Epoch: 0 \t Batch: 41200 \t loss: 0.27130\n",
      "Epoch: 0 \t Batch: 41300 \t loss: 0.27452\n",
      "Epoch: 0 \t Batch: 41400 \t loss: 0.27321\n",
      "Epoch: 0 \t Batch: 41500 \t loss: 0.27444\n",
      "Epoch: 0 \t Batch: 41600 \t loss: 0.27855\n",
      "Epoch: 0 \t Batch: 41700 \t loss: 0.27361\n",
      "Epoch: 0 \t Batch: 41800 \t loss: 0.27764\n",
      "Epoch: 0 \t Batch: 41900 \t loss: 0.27139\n",
      "Epoch: 0 \t Batch: 42000 \t loss: 0.27655\n",
      "Epoch: 0 \t Batch: 42100 \t loss: 0.27559\n",
      "Epoch: 0 \t Batch: 42200 \t loss: 0.27783\n",
      "Epoch: 0 \t Batch: 42300 \t loss: 0.27111\n",
      "Epoch: 0 \t Batch: 42400 \t loss: 0.27962\n",
      "Epoch: 0 \t Batch: 42500 \t loss: 0.27401\n",
      "Epoch: 0 \t Batch: 42600 \t loss: 0.27305\n",
      "Epoch: 0 \t Batch: 42700 \t loss: 0.27517\n",
      "Epoch: 0 \t Batch: 42800 \t loss: 0.27406\n",
      "Epoch: 0 \t Batch: 42900 \t loss: 0.27482\n",
      "Epoch: 0 \t Batch: 43000 \t loss: 0.27912\n",
      "Epoch: 0 \t Batch: 43100 \t loss: 0.27435\n",
      "Epoch: 0 \t Batch: 43200 \t loss: 0.27330\n",
      "Epoch: 0 \t Batch: 43300 \t loss: 0.27818\n",
      "Epoch: 0 \t Batch: 43400 \t loss: 0.27574\n",
      "Epoch: 0 \t Batch: 43500 \t loss: 0.27149\n",
      "Epoch: 0 \t Batch: 43600 \t loss: 0.27411\n",
      "Epoch: 0 \t Batch: 43700 \t loss: 0.27964\n",
      "Epoch: 0 \t Batch: 43800 \t loss: 0.27571\n",
      "Epoch: 0 \t Batch: 43900 \t loss: 0.27659\n",
      "Epoch: 0 \t Batch: 44000 \t loss: 0.27184\n",
      "Epoch: 0 \t Batch: 44100 \t loss: 0.27473\n",
      "Epoch: 0 \t Batch: 44200 \t loss: 0.27304\n",
      "Epoch: 0 \t Batch: 44300 \t loss: 0.27234\n",
      "Epoch: 0 \t Batch: 44400 \t loss: 0.27550\n",
      "Epoch: 0 \t Batch: 44500 \t loss: 0.27592\n",
      "Epoch: 0 \t Batch: 44600 \t loss: 0.27845\n",
      "Epoch: 0 \t Batch: 44700 \t loss: 0.27810\n",
      "Epoch: 0 \t Batch: 44800 \t loss: 0.27211\n",
      "Epoch: 0 \t Batch: 44900 \t loss: 0.27513\n",
      "Epoch: 0 \t Batch: 45000 \t loss: 0.27796\n",
      "Epoch: 0 \t Batch: 45100 \t loss: 0.27018\n",
      "Epoch: 0 \t Batch: 45200 \t loss: 0.27434\n",
      "Epoch: 0 \t Batch: 45300 \t loss: 0.27359\n",
      "Epoch: 0 \t Batch: 45400 \t loss: 0.27792\n",
      "Epoch: 0 \t Batch: 45500 \t loss: 0.27273\n",
      "Epoch: 0 \t Batch: 45600 \t loss: 0.27416\n",
      "Epoch: 0 \t Batch: 45700 \t loss: 0.27255\n",
      "Epoch: 0 \t Batch: 45800 \t loss: 0.27349\n",
      "Epoch: 0 \t Batch: 45900 \t loss: 0.27375\n",
      "Epoch: 0 \t Batch: 46000 \t loss: 0.27619\n",
      "Epoch: 0 \t Batch: 46100 \t loss: 0.27669\n",
      "Epoch: 0 \t Batch: 46200 \t loss: 0.27520\n",
      "Epoch: 0 \t Batch: 46300 \t loss: 0.27347\n",
      "Epoch: 0 \t Batch: 46400 \t loss: 0.27427\n",
      "Epoch: 0 \t Batch: 46500 \t loss: 0.27052\n",
      "Epoch: 0 \t Batch: 46600 \t loss: 0.27159\n",
      "Epoch: 0 \t Batch: 46700 \t loss: 0.27680\n",
      "Epoch: 0 \t Batch: 46800 \t loss: 0.27695\n",
      "Epoch: 0 \t Batch: 46900 \t loss: 0.27361\n",
      "Epoch: 0 \t Batch: 47000 \t loss: 0.27404\n",
      "Epoch: 0 \t Batch: 47100 \t loss: 0.27140\n",
      "Epoch: 0 \t Batch: 47200 \t loss: 0.28048\n",
      "Epoch: 0 \t Batch: 47300 \t loss: 0.27753\n",
      "Epoch: 0 \t Batch: 47400 \t loss: 0.27545\n",
      "Epoch: 0 \t Batch: 47500 \t loss: 0.27014\n",
      "Epoch: 0 \t Batch: 47600 \t loss: 0.27152\n",
      "Epoch: 0 \t Batch: 47700 \t loss: 0.27266\n",
      "Epoch: 0 \t Batch: 47800 \t loss: 0.27321\n",
      "Epoch: 0 \t Batch: 47900 \t loss: 0.27761\n",
      "Epoch: 0 \t Batch: 48000 \t loss: 0.27043\n",
      "Epoch: 0 \t Batch: 48100 \t loss: 0.27429\n",
      "Epoch: 0 \t Batch: 48200 \t loss: 0.27653\n",
      "Epoch: 0 \t Batch: 48300 \t loss: 0.27018\n",
      "Epoch: 0 \t Batch: 48400 \t loss: 0.27775\n",
      "Epoch: 0 \t Batch: 48500 \t loss: 0.27290\n",
      "Epoch: 0 \t Batch: 48600 \t loss: 0.27762\n",
      "Epoch: 0 \t Batch: 48700 \t loss: 0.27478\n",
      "Epoch: 0 \t Batch: 48800 \t loss: 0.27473\n",
      "Epoch: 0 \t Batch: 48900 \t loss: 0.27452\n",
      "Epoch: 0 \t Batch: 49000 \t loss: 0.27359\n",
      "Epoch: 0 \t Batch: 49100 \t loss: 0.27929\n",
      "Epoch: 0 \t Batch: 49200 \t loss: 0.27702\n",
      "Epoch: 0 \t Batch: 49300 \t loss: 0.27654\n",
      "Epoch: 0 \t Batch: 49400 \t loss: 0.27253\n",
      "Epoch: 0 \t Batch: 49500 \t loss: 0.27516\n",
      "Epoch: 0 \t Batch: 49600 \t loss: 0.27386\n",
      "Epoch: 0 \t Batch: 49700 \t loss: 0.27567\n",
      "Epoch: 0 \t Batch: 49800 \t loss: 0.27173\n",
      "Epoch: 0 \t Batch: 49900 \t loss: 0.27397\n",
      "Epoch: 0 \t Batch: 50000 \t loss: 0.27421\n",
      "Epoch: 0 \t Batch: 50100 \t loss: 0.27265\n",
      "Epoch: 0 \t Batch: 50200 \t loss: 0.27346\n",
      "Epoch: 0 \t Batch: 50300 \t loss: 0.27371\n",
      "Epoch: 0 \t Batch: 50400 \t loss: 0.27888\n",
      "Epoch: 0 \t Batch: 50500 \t loss: 0.27326\n",
      "Epoch: 0 \t Batch: 50600 \t loss: 0.27318\n",
      "Epoch: 0 \t Batch: 50700 \t loss: 0.27664\n",
      "Epoch: 0 \t Batch: 50800 \t loss: 0.27163\n",
      "Epoch: 0 \t Batch: 50900 \t loss: 0.27594\n",
      "Epoch: 0 \t Batch: 51000 \t loss: 0.26968\n",
      "Epoch: 0 \t Batch: 51100 \t loss: 0.27563\n",
      "Epoch: 0 \t Batch: 51200 \t loss: 0.27407\n",
      "Epoch: 0 \t Batch: 51300 \t loss: 0.27339\n",
      "Epoch: 0 \t Batch: 51400 \t loss: 0.27410\n",
      "Epoch: 0 \t Batch: 51500 \t loss: 0.27702\n",
      "Epoch: 0 \t Batch: 51600 \t loss: 0.27771\n",
      "Epoch: 0 \t Batch: 51700 \t loss: 0.27272\n",
      "Epoch: 0 \t Batch: 51800 \t loss: 0.27639\n",
      "Epoch: 0 \t Batch: 51900 \t loss: 0.27323\n",
      "Epoch: 0 \t Batch: 52000 \t loss: 0.27242\n",
      "Epoch: 0 \t Batch: 52100 \t loss: 0.27272\n",
      "Epoch: 0 \t Batch: 52200 \t loss: 0.27414\n",
      "Epoch: 0 \t Batch: 52300 \t loss: 0.27426\n",
      "Epoch: 0 \t Batch: 52400 \t loss: 0.26996\n",
      "Epoch: 0 \t Batch: 52500 \t loss: 0.27164\n",
      "Epoch: 0 \t Batch: 52600 \t loss: 0.27540\n",
      "Epoch: 0 \t Batch: 52700 \t loss: 0.27062\n",
      "Epoch: 0 \t Batch: 52800 \t loss: 0.27715\n",
      "Epoch: 0 \t Batch: 52900 \t loss: 0.27634\n",
      "Epoch: 0 \t Batch: 53000 \t loss: 0.27578\n",
      "Epoch: 0 \t Batch: 53100 \t loss: 0.27279\n",
      "Epoch: 0 \t Batch: 53200 \t loss: 0.27410\n",
      "Epoch: 0 \t Batch: 53300 \t loss: 0.27038\n",
      "Epoch: 0 \t Batch: 53400 \t loss: 0.27307\n",
      "Epoch: 0 \t Batch: 53500 \t loss: 0.27627\n",
      "Epoch: 0 \t Batch: 53600 \t loss: 0.27609\n",
      "Epoch: 0 \t Batch: 53700 \t loss: 0.27004\n",
      "Epoch: 0 \t Batch: 53800 \t loss: 0.27425\n",
      "Epoch: 0 \t Batch: 53900 \t loss: 0.27640\n",
      "Epoch: 0 \t Batch: 54000 \t loss: 0.27538\n",
      "Epoch: 0 \t Batch: 54100 \t loss: 0.27990\n",
      "Epoch: 0 \t Batch: 54200 \t loss: 0.26492\n",
      "Epoch: 0 \t Batch: 54300 \t loss: 0.27155\n",
      "Epoch: 0 \t Batch: 54400 \t loss: 0.27208\n",
      "Epoch: 0 \t Batch: 54500 \t loss: 0.27458\n",
      "Epoch: 0 \t Batch: 54600 \t loss: 0.27770\n",
      "Epoch: 0 \t Batch: 54700 \t loss: 0.27203\n",
      "Epoch: 0 \t Batch: 54800 \t loss: 0.27228\n",
      "Epoch: 0 \t Batch: 54900 \t loss: 0.28021\n",
      "Epoch: 0 \t Batch: 55000 \t loss: 0.27237\n",
      "Epoch: 0 \t Batch: 55100 \t loss: 0.27676\n",
      "Epoch: 0 \t Batch: 55200 \t loss: 0.27123\n",
      "Epoch: 0 \t Batch: 55300 \t loss: 0.27726\n",
      "Epoch: 0 \t Batch: 55400 \t loss: 0.27010\n",
      "Epoch: 0 \t Batch: 55500 \t loss: 0.27701\n",
      "Epoch: 0 \t Batch: 55600 \t loss: 0.27212\n",
      "Epoch: 0 \t Batch: 55700 \t loss: 0.27679\n",
      "Epoch: 0 \t Batch: 55800 \t loss: 0.27343\n",
      "Epoch: 0 \t Batch: 55900 \t loss: 0.26973\n",
      "Epoch: 0 \t Batch: 56000 \t loss: 0.27529\n",
      "Epoch: 0 \t Batch: 56100 \t loss: 0.27538\n",
      "Epoch: 0 \t Batch: 56200 \t loss: 0.27792\n",
      "Epoch: 0 \t Batch: 56300 \t loss: 0.27792\n",
      "Epoch: 0 \t Batch: 56400 \t loss: 0.27519\n",
      "Epoch: 0 \t Batch: 56500 \t loss: 0.27544\n",
      "Epoch: 0 \t Batch: 56600 \t loss: 0.26569\n",
      "Epoch: 0 \t Batch: 56700 \t loss: 0.27411\n",
      "Epoch: 0 \t Batch: 56800 \t loss: 0.27584\n",
      "Epoch: 0 \t Batch: 56900 \t loss: 0.27908\n",
      "Epoch: 0 \t Batch: 57000 \t loss: 0.27703\n",
      "Epoch: 0 \t Batch: 57100 \t loss: 0.27593\n",
      "Epoch: 0 \t Batch: 57200 \t loss: 0.27242\n",
      "Epoch: 0 \t Batch: 57300 \t loss: 0.27508\n",
      "Epoch: 0 \t Batch: 57400 \t loss: 0.27566\n",
      "Epoch: 0 \t Batch: 57500 \t loss: 0.27573\n",
      "Epoch: 0 \t Batch: 57600 \t loss: 0.27128\n",
      "Epoch: 0 \t Batch: 57700 \t loss: 0.27367\n",
      "Epoch: 0 \t Batch: 57800 \t loss: 0.27860\n",
      "Epoch: 0 \t Batch: 57900 \t loss: 0.27386\n",
      "Epoch: 0 \t Batch: 58000 \t loss: 0.27625\n",
      "Epoch: 0 \t Batch: 58100 \t loss: 0.27665\n",
      "Epoch: 0 \t Batch: 58200 \t loss: 0.27497\n",
      "Epoch: 0 \t Batch: 58300 \t loss: 0.27822\n",
      "Epoch: 0 \t Batch: 58400 \t loss: 0.26948\n",
      "Epoch: 0 \t Batch: 58500 \t loss: 0.27453\n",
      "Epoch: 0 \t Batch: 58600 \t loss: 0.27446\n",
      "Epoch: 0 \t Batch: 58700 \t loss: 0.27679\n",
      "Epoch: 0 \t Batch: 58800 \t loss: 0.27381\n",
      "Epoch: 0 \t Batch: 58900 \t loss: 0.27048\n",
      "Epoch: 0 \t Batch: 59000 \t loss: 0.27492\n",
      "Epoch: 0 \t Batch: 59100 \t loss: 0.27472\n",
      "Epoch: 0 \t Batch: 59200 \t loss: 0.27089\n",
      "Epoch: 0 \t Batch: 59300 \t loss: 0.27341\n",
      "Epoch: 0 \t Batch: 59400 \t loss: 0.27321\n",
      "Epoch: 0 \t Batch: 59500 \t loss: 0.27562\n",
      "Epoch: 0 \t Batch: 59600 \t loss: 0.27375\n",
      "Epoch: 0 \t Batch: 59700 \t loss: 0.27469\n",
      "Epoch: 0 \t Batch: 59800 \t loss: 0.27348\n",
      "Epoch: 0 \t Batch: 59900 \t loss: 0.27324\n",
      "Epoch: 0 \t Batch: 60000 \t loss: 0.27129\n",
      "Epoch: 0 \t Batch: 60100 \t loss: 0.27821\n",
      "Epoch: 0 \t Batch: 60200 \t loss: 0.27687\n",
      "Epoch: 0 \t Batch: 60300 \t loss: 0.27607\n",
      "Epoch: 0 \t Batch: 60400 \t loss: 0.27318\n",
      "Epoch: 0 \t Batch: 60500 \t loss: 0.27574\n",
      "Epoch: 0 \t Batch: 60600 \t loss: 0.27545\n",
      "Epoch: 0 \t Batch: 60700 \t loss: 0.27704\n",
      "Epoch: 0 \t Batch: 60800 \t loss: 0.27357\n",
      "Epoch: 0 \t Batch: 60900 \t loss: 0.27435\n",
      "Epoch: 0 \t Batch: 61000 \t loss: 0.27648\n",
      "Epoch: 0 \t Batch: 61100 \t loss: 0.27484\n",
      "Epoch: 0 \t Batch: 61200 \t loss: 0.27647\n",
      "Epoch: 0 \t Batch: 61300 \t loss: 0.27420\n",
      "Epoch: 0 \t Batch: 61400 \t loss: 0.27116\n",
      "Epoch: 0 \t Batch: 61500 \t loss: 0.27426\n",
      "Epoch: 0 \t Batch: 61600 \t loss: 0.27668\n",
      "Epoch: 0 \t Batch: 61700 \t loss: 0.27741\n",
      "Epoch: 0 \t Batch: 61800 \t loss: 0.27185\n",
      "Epoch: 0 \t Batch: 61900 \t loss: 0.27278\n",
      "Epoch: 0 \t Batch: 62000 \t loss: 0.27367\n",
      "Epoch: 0 \t Batch: 62100 \t loss: 0.27159\n",
      "Epoch: 0 \t Batch: 62200 \t loss: 0.26859\n",
      "Epoch: 0 \t Batch: 62300 \t loss: 0.27179\n",
      "Epoch: 0 \t Batch: 62400 \t loss: 0.27378\n",
      "Epoch: 0 \t Batch: 62500 \t loss: 0.27362\n",
      "Epoch: 0 \t Batch: 62600 \t loss: 0.27149\n",
      "Epoch: 0 \t Batch: 62700 \t loss: 0.27185\n",
      "Epoch: 0 \t Batch: 62800 \t loss: 0.27083\n",
      "Epoch: 0 \t Batch: 62900 \t loss: 0.27694\n",
      "Epoch: 0 \t Batch: 63000 \t loss: 0.27516\n",
      "Epoch: 0 \t Batch: 63100 \t loss: 0.27139\n",
      "Epoch: 0 \t Batch: 63200 \t loss: 0.26981\n",
      "Epoch: 0 \t Batch: 63300 \t loss: 0.27565\n",
      "Epoch: 0 \t Batch: 63400 \t loss: 0.27170\n",
      "Epoch: 0 \t Batch: 63500 \t loss: 0.27131\n",
      "Epoch: 0 \t Batch: 63600 \t loss: 0.27314\n",
      "Epoch: 0 \t Batch: 63700 \t loss: 0.27809\n",
      "Epoch: 0 \t Batch: 63800 \t loss: 0.27505\n",
      "Epoch: 0 \t Batch: 63900 \t loss: 0.27437\n",
      "Epoch: 0 \t Batch: 64000 \t loss: 0.27363\n",
      "Epoch: 0 \t Batch: 64100 \t loss: 0.27520\n",
      "Epoch: 0 \t Batch: 64200 \t loss: 0.27239\n",
      "Epoch: 0 \t Batch: 64300 \t loss: 0.27107\n",
      "Epoch: 0 \t Batch: 64400 \t loss: 0.27136\n",
      "Epoch: 0 \t Batch: 64500 \t loss: 0.27024\n",
      "Epoch: 0 \t Batch: 64600 \t loss: 0.27445\n",
      "Epoch: 0 \t Batch: 64700 \t loss: 0.27620\n",
      "Epoch: 0 \t Batch: 64800 \t loss: 0.27473\n",
      "Epoch: 0 \t Batch: 64900 \t loss: 0.28232\n",
      "Epoch: 0 \t Batch: 65000 \t loss: 0.27562\n",
      "Epoch: 0 \t Batch: 65100 \t loss: 0.27435\n",
      "Epoch: 0 \t Batch: 65200 \t loss: 0.27342\n",
      "Epoch: 0 \t Batch: 65300 \t loss: 0.27229\n",
      "Epoch: 0 \t Batch: 65400 \t loss: 0.27253\n",
      "Epoch: 0 \t Batch: 65500 \t loss: 0.27034\n",
      "Epoch: 0 \t Batch: 65600 \t loss: 0.27152\n",
      "Epoch: 0 \t Batch: 65700 \t loss: 0.27533\n",
      "Epoch: 0 \t Batch: 65800 \t loss: 0.27211\n",
      "Epoch: 0 \t Batch: 65900 \t loss: 0.27195\n",
      "Epoch: 0 \t Batch: 66000 \t loss: 0.27272\n",
      "Epoch: 0 \t Batch: 66100 \t loss: 0.27680\n",
      "Epoch: 0 \t Batch: 66200 \t loss: 0.27626\n",
      "Epoch: 0 \t Batch: 66300 \t loss: 0.27054\n",
      "Epoch: 0 \t Batch: 66400 \t loss: 0.27689\n",
      "Epoch: 0 \t Batch: 66500 \t loss: 0.27155\n",
      "Epoch: 0 \t Batch: 66600 \t loss: 0.27501\n",
      "Epoch: 0 \t Batch: 66700 \t loss: 0.27703\n",
      "Epoch: 0 \t Batch: 66800 \t loss: 0.27494\n",
      "Epoch: 0 \t Batch: 66900 \t loss: 0.27434\n",
      "Epoch: 0 \t Batch: 67000 \t loss: 0.27969\n",
      "Epoch: 0 \t Batch: 67100 \t loss: 0.27833\n",
      "Epoch: 0 \t Batch: 67200 \t loss: 0.27602\n",
      "Epoch: 0 \t Batch: 67300 \t loss: 0.27489\n",
      "Epoch: 0 \t Batch: 67400 \t loss: 0.27541\n",
      "Epoch: 0 \t Batch: 67500 \t loss: 0.27273\n",
      "Epoch: 0 \t Batch: 67600 \t loss: 0.27334\n",
      "Epoch: 0 \t Batch: 67700 \t loss: 0.27768\n",
      "Epoch: 0 \t Batch: 67800 \t loss: 0.27188\n",
      "Epoch: 0 \t Batch: 67900 \t loss: 0.26862\n",
      "Epoch: 0 \t Batch: 68000 \t loss: 0.27181\n",
      "Epoch: 0 \t Batch: 68100 \t loss: 0.27433\n",
      "Epoch: 0 \t Batch: 68200 \t loss: 0.27273\n",
      "Epoch: 0 \t Batch: 68300 \t loss: 0.27271\n",
      "Epoch: 0 \t Batch: 68400 \t loss: 0.27271\n",
      "Epoch: 0 \t Batch: 68500 \t loss: 0.27385\n",
      "Epoch: 0 \t Batch: 68600 \t loss: 0.27492\n",
      "Epoch: 0 \t Batch: 68700 \t loss: 0.27698\n",
      "Epoch: 0 \t Batch: 68800 \t loss: 0.27165\n",
      "Epoch: 0 \t Batch: 68900 \t loss: 0.27579\n",
      "Epoch: 0 \t Batch: 69000 \t loss: 0.27237\n",
      "Epoch: 0 \t Batch: 69100 \t loss: 0.27355\n",
      "Epoch: 0 \t Batch: 69200 \t loss: 0.27823\n",
      "Epoch: 0 \t Batch: 69300 \t loss: 0.27533\n",
      "Epoch: 0 \t Batch: 69400 \t loss: 0.26703\n",
      "Epoch: 0 \t Batch: 69500 \t loss: 0.27544\n",
      "Epoch: 0 \t Batch: 69600 \t loss: 0.26836\n",
      "Epoch: 0 \t Batch: 69700 \t loss: 0.27025\n",
      "Epoch: 0 \t Batch: 69800 \t loss: 0.27113\n",
      "Epoch: 0 \t Batch: 69900 \t loss: 0.27760\n",
      "Epoch: 0 \t Batch: 70000 \t loss: 0.27195\n",
      "Epoch: 0 \t Batch: 70100 \t loss: 0.27833\n",
      "Epoch: 0 \t Batch: 70200 \t loss: 0.27531\n",
      "Epoch: 0 \t Batch: 70300 \t loss: 0.27766\n",
      "Epoch: 0 \t Batch: 70400 \t loss: 0.27528\n",
      "Epoch: 0 \t Batch: 70500 \t loss: 0.27282\n",
      "Epoch: 0 \t Batch: 70600 \t loss: 0.27448\n",
      "Epoch: 0 \t Batch: 70700 \t loss: 0.27369\n",
      "Epoch: 0 \t Batch: 70800 \t loss: 0.27298\n",
      "Epoch: 0 \t Batch: 70900 \t loss: 0.27303\n",
      "Epoch: 0 \t Batch: 71000 \t loss: 0.26660\n",
      "Epoch: 0 \t Batch: 71100 \t loss: 0.27361\n",
      "Epoch: 0 \t Batch: 71200 \t loss: 0.27448\n",
      "Epoch: 0 \t Batch: 71300 \t loss: 0.27047\n",
      "Epoch: 0 \t Batch: 71400 \t loss: 0.27274\n",
      "Epoch: 0 \t Batch: 71500 \t loss: 0.27639\n",
      "Epoch: 0 \t Batch: 71600 \t loss: 0.27521\n",
      "Epoch: 0 \t Batch: 71700 \t loss: 0.27058\n",
      "Epoch: 0 \t Batch: 71800 \t loss: 0.27635\n",
      "Epoch: 0 \t Batch: 71900 \t loss: 0.27701\n",
      "Epoch: 0 \t Batch: 72000 \t loss: 0.27953\n",
      "Epoch: 0 \t Batch: 72100 \t loss: 0.27120\n",
      "Epoch: 0 \t Batch: 72200 \t loss: 0.27865\n",
      "Epoch: 0 \t Batch: 72300 \t loss: 0.27462\n",
      "Epoch: 0 \t Batch: 72400 \t loss: 0.27834\n",
      "Epoch: 0 \t Batch: 72500 \t loss: 0.27304\n",
      "Epoch: 0 \t Batch: 72600 \t loss: 0.27341\n",
      "Epoch: 0 \t Batch: 72700 \t loss: 0.27658\n",
      "Epoch: 0 \t Batch: 72800 \t loss: 0.27133\n",
      "Epoch: 0 \t Batch: 72900 \t loss: 0.27151\n",
      "Epoch: 0 \t Batch: 73000 \t loss: 0.27298\n",
      "Epoch: 0 \t Batch: 73100 \t loss: 0.27424\n",
      "Epoch: 0 \t Batch: 73200 \t loss: 0.27265\n",
      "Epoch: 0 \t Batch: 73300 \t loss: 0.27365\n",
      "Epoch: 0 \t Batch: 73400 \t loss: 0.27504\n",
      "Epoch: 0 \t Batch: 73500 \t loss: 0.27433\n",
      "Epoch: 0 \t Batch: 73600 \t loss: 0.27213\n",
      "Epoch: 0 \t Batch: 73700 \t loss: 0.27296\n",
      "Epoch: 0 \t Batch: 73800 \t loss: 0.27256\n",
      "Epoch: 0 \t Batch: 73900 \t loss: 0.27290\n",
      "Epoch: 0 \t Batch: 74000 \t loss: 0.27258\n",
      "Epoch: 0 \t Batch: 74100 \t loss: 0.27626\n",
      "Epoch: 0 \t Batch: 74200 \t loss: 0.27308\n",
      "Epoch: 0 \t Batch: 74300 \t loss: 0.27377\n",
      "Epoch: 0 \t Batch: 74400 \t loss: 0.27052\n",
      "Epoch: 0 \t Batch: 74500 \t loss: 0.27287\n",
      "Epoch: 0 \t Batch: 74600 \t loss: 0.27402\n",
      "Epoch: 0 \t Batch: 74700 \t loss: 0.27403\n",
      "Epoch: 0 \t Batch: 74800 \t loss: 0.27308\n",
      "Epoch: 0 \t Batch: 74900 \t loss: 0.27157\n",
      "Epoch: 0 \t Batch: 75000 \t loss: 0.27446\n",
      "Epoch: 0 \t Batch: 75100 \t loss: 0.27393\n",
      "Epoch: 0 \t Batch: 75200 \t loss: 0.27807\n",
      "Epoch: 0 \t Batch: 75300 \t loss: 0.26807\n",
      "Epoch: 0 \t Batch: 75400 \t loss: 0.27382\n",
      "Epoch: 0 \t Batch: 75500 \t loss: 0.27652\n",
      "Epoch: 0 \t Batch: 75600 \t loss: 0.27048\n",
      "Epoch: 0 \t Batch: 75700 \t loss: 0.27326\n",
      "Epoch: 0 \t Batch: 75800 \t loss: 0.26998\n",
      "Epoch: 0 \t Batch: 75900 \t loss: 0.27949\n",
      "Epoch: 0 \t Batch: 76000 \t loss: 0.27530\n",
      "Epoch: 0 \t Batch: 76100 \t loss: 0.27269\n",
      "Epoch: 0 \t Batch: 76200 \t loss: 0.26976\n",
      "Epoch: 0 \t Batch: 76300 \t loss: 0.27586\n",
      "Epoch: 0 \t Batch: 76400 \t loss: 0.27665\n",
      "Epoch: 0 \t Batch: 76500 \t loss: 0.27530\n",
      "Epoch: 0 \t Batch: 76600 \t loss: 0.27255\n",
      "Epoch: 0 \t Batch: 76700 \t loss: 0.27515\n",
      "Epoch: 0 \t Batch: 76800 \t loss: 0.27554\n",
      "Epoch: 0 \t Batch: 76900 \t loss: 0.27333\n",
      "Epoch: 0 \t Batch: 77000 \t loss: 0.27669\n",
      "Epoch: 0 \t Batch: 77100 \t loss: 0.27125\n",
      "Epoch: 0 \t Batch: 77200 \t loss: 0.27531\n",
      "Epoch: 0 \t Batch: 77300 \t loss: 0.27746\n",
      "Epoch: 0 \t Batch: 77400 \t loss: 0.27477\n",
      "Epoch: 0 \t Batch: 77500 \t loss: 0.27350\n",
      "Epoch: 0 \t Batch: 77600 \t loss: 0.27516\n",
      "Epoch: 0 \t Batch: 77700 \t loss: 0.26985\n",
      "Epoch: 0 \t Batch: 77800 \t loss: 0.27024\n",
      "Epoch: 0 \t Batch: 77900 \t loss: 0.27536\n",
      "Epoch: 0 \t Batch: 78000 \t loss: 0.27661\n",
      "Epoch: 0 \t Batch: 78100 \t loss: 0.27124\n",
      "Epoch: 0 \t Batch: 78200 \t loss: 0.27648\n",
      "Epoch: 0 \t Batch: 78300 \t loss: 0.27563\n",
      "Epoch: 0 \t Batch: 78400 \t loss: 0.27179\n",
      "Epoch: 0 \t Batch: 78500 \t loss: 0.27819\n",
      "Epoch: 0 \t Batch: 78600 \t loss: 0.27076\n",
      "Epoch: 0 \t Batch: 78700 \t loss: 0.27307\n",
      "Epoch: 0 \t Batch: 78800 \t loss: 0.27604\n",
      "Epoch: 0 \t Batch: 78900 \t loss: 0.27102\n",
      "Epoch: 0 \t Batch: 79000 \t loss: 0.27366\n",
      "Epoch: 0 \t Batch: 79100 \t loss: 0.27397\n",
      "Epoch: 0 \t Batch: 79200 \t loss: 0.27304\n",
      "Epoch: 0 \t Batch: 79300 \t loss: 0.27188\n",
      "Epoch: 0 \t Batch: 79400 \t loss: 0.27029\n",
      "Epoch: 0 \t Batch: 79500 \t loss: 0.27480\n",
      "Epoch: 0 \t Batch: 79600 \t loss: 0.27574\n",
      "Epoch: 0 \t Batch: 79700 \t loss: 0.27142\n",
      "Epoch: 0 \t Batch: 79800 \t loss: 0.27104\n",
      "Epoch: 0 \t Batch: 79900 \t loss: 0.27201\n",
      "Epoch: 0 \t Batch: 80000 \t loss: 0.27603\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    running_loss = 0\n",
    "    for i, (img, _) in enumerate(train_loader):\n",
    "        img = img.to(device)\n",
    "        netG.train()\n",
    "        optimG.zero_grad()\n",
    "        \n",
    "        if args.FA:\n",
    "            gauss = (torch.randn(img.size()[0], 3, img_size, img_size) * (args.sigma / 255)).to(device)\n",
    "            mask = (torch.rand_like(img) * 2 * args.rho + 1 - args.rho).to(device)\n",
    "            \n",
    "            img_dct = dct_2d(img + gauss).to(device)\n",
    "            img_idct = idct_2d(img_dct * mask)\n",
    "            img_idct = V(img_idct, requires_grad=True)\n",
    "            img = img_idct\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # adversarial translation        \n",
    "        adv = netG(img)\n",
    "        adv = torch.min(torch.max(adv, img - args.eps/255.0), img + args.eps/255.0)\n",
    "        adv = torch.clamp(adv, 0.0, 1.0)\n",
    "        \n",
    "\n",
    "        # if args.FA:\n",
    "        #     gauss = (torch.randn(img.size()[0], 3, img_size, img_size) * (args.sigma / 255)).to(device)\n",
    "        #     mask = (torch.rand_like(img) * 2 * args.rho + 1 - args.rho).to(device)\n",
    "            \n",
    "        #     img_dct = dct_2d(img + gauss).to(device)\n",
    "        #     img_idct = idct_2d(img_dct * mask)\n",
    "        #     img_idct = V(img_idct, requires_grad=True)\n",
    "        #     img = img_idct\n",
    "            \n",
    "        #     adv_dct = dct_2d(adv + gauss).to(device)\n",
    "        #     adv_idct = idct_2d(adv_dct * mask)\n",
    "        #     adv_idct = V(adv_idct, requires_grad=True)\n",
    "        #     adv = adv_idct\n",
    "        # else:\n",
    "        #     pass\n",
    "        \n",
    "        if args.RN:\n",
    "            mean = np.random.normal(0.50, 0.08) # default=(0.50, 0.08) \n",
    "            std = np.random.normal(0.75, 0.08) # default=(0.75, 0.08)\n",
    "            adv_out_slice = model(normalize(adv.clone(), mean, std))[layer_idx]\n",
    "            img_out_slice = model(normalize(img.clone(), mean, std))[layer_idx]\n",
    "        else:\n",
    "            adv_out_slice = model(default_normalize(adv.clone()))[layer_idx]\n",
    "            img_out_slice = model(default_normalize(img.clone()))[layer_idx]\n",
    "        \n",
    "        if args.DA:\n",
    "            attention = abs(torch.mean(img_out_slice, dim=1, keepdim=True)).detach()\n",
    "        else:\n",
    "            attention = torch.ones(adv_out_slice.shape).cuda()\n",
    "            \n",
    "        loss = torch.cosine_similarity((adv_out_slice*attention).reshape(adv_out_slice.shape[0], -1), \n",
    "                                       (img_out_slice*attention).reshape(img_out_slice.shape[0], -1)).mean()\n",
    "        loss.backward()\n",
    "        optimG.step()\n",
    "        \n",
    "        # Every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {0} \\t Batch: {1} \\t loss: {2:.5f}'.format(epoch, i, running_loss/100))\n",
    "            running_loss = 0\n",
    "        running_loss += abs(loss.item())\n",
    "        \n",
    "        # Every 1 epoch\n",
    "        if i % 80000 == 0 and i > 0: # 1epoch=80000batch\n",
    "            save_checkpoint_dir = 'saved_models/{}'.format(args.model_type)\n",
    "            if not os.path.exists(save_checkpoint_dir):\n",
    "                os.makedirs(save_checkpoint_dir)\n",
    "            save_path = os.path.join(save_checkpoint_dir, 'netG_{}_{}.pth'.format(save_checkpoint_suffix, epoch))\n",
    "            \n",
    "            if isinstance(netG, nn.DataParallel):\n",
    "                torch.save(netG.module.state_dict(), save_path)\n",
    "            else:\n",
    "                torch.save(netG.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a5ed91507c36eb248011b7d4b42840e9d47d02750f359633e20bb6b8b69add"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
